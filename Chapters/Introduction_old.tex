\chapter{Introduction}
\justifying
\setlength{\parindent}{0em}
\textbf{The primary goal of this PhD is to create a method for generating explanations of a deep neural network’s (DNN's) classification that are both human-understandable (interpretable) and accurate (faithful) with respect to how  the  network reached  the classification}

\section{Problem motivation}

Machine learning (ML) is notably impacting several aspects of society, with applications ranging from content filtering and recommendations on social networks and e-commerce websites to protein folding, autonomous vehicle control, financial markets and cancer diagnostics~\cite{forthcoming}. The ease of application of a number of ML techniques comes from each system's ability to perform specific decision-making tasks without requiring explicit human instructions by learning from a large amount of data. ML techniques have existed since the 1960s, but it wasn't until recently (circa 2010) that the proliferation of the integration of ML systems into wider society became noteworthy. This is mainly due to the the increase of available information, as well as other developments, including hardware improvements and new optimisation algorithms.   


Powerful ML models trained on large data sets can achieve great performance, and on some cognitive tasks record results on par with humans. Many tasks, previously thought to be so computationally demanding as to be unattainable, such as image detection and recognition~\cite{HeZRS16}, strategic game planning~\cite{SilverHMGSDSAPL16} and natural language processing~\cite{DengHK13}, have seen rapid development~\cite{LeCunBH15}. However, the knowledge encoded within such ML systems is not humanly understandable and that makes it hard to trust the decisions they make — understandably so. Currently most ML system are deployed and expected to behave as intended if they show a high degree of predictive accuracy on a test set. However, in some cases, the high predictive accuracy of  ML systems can be the result of erroneous exploration of artefacts in the data (i.e.\ the presence of systematic bias in data that the system bases its decision upon) rather than the result of correctly identified parts of the input that \textit{should} lead to the decision ~\cite{leek2010tackling, SzegedyZSBEGF13, corr, taylor2006methods}. AI systems that exploit such spurious correlations from the input to make their decisions are referred to as ``Clever Hans'' Predictors~\footnote{Clever Hans was a horse that was believed to be able to count and was declared a scientific marvel in the years around the 1900s. Later it turned out that the horse did not learn how to count, but was rather deriving the answer from the questioner's reactions~\cite{lapuschkin2019unmasking}}. Moreover, since accuracy is calculated on test data, there is no guarantee that once deployed on real-world data it will remain high. Hence the lack of transparent decision making in ML systems not only limits their trustworthiness, but also poses problems to their acceptance and applicability, due to concerns over poor performance in safety critical applications, potentially leading to complex enquiries regarding accountability (i.e.\ who or what is at fault) and industrial liability (i.e.\ who or what is liable for costly mistakes made by an algorithm)~\cite{NguyenHHTK14, abs, kucharski2016study, WolfMG17}. The problem of non transparency of ML systems is even more pressing in life-critical situations where it is essential to robustly verify system's decision making process (e.g. in areas including medicine and the criminal justice system, etc.)~\cite{CaruanaLGKSE15, BojarskiYCCFJM17}.


For one to develop new methods of machine learning that are deemed trusted and accepted by the user, one must have clear criteria of when one has succeeded in creating a \textit{trustworthy} method. However, trust and acceptance are concepts that are hard to quantify and formalise~\cite{doshi2017towards, Lipton18} and as such, they cannot guide the development of safe ML algorithms. Hence, the research in this area is concerned with achieving human-understandable (\textit{interpretable}) explanations that are reliable (\textit{faithful}) with respect to how the classifier makes its decisions. Achieving these properties is a crucial stepping stone for achieving trust~\cite{Lipton18}. The research area that addresses this problem is called explainable artificial intelligence (XAI).


In recent years there has been growing recognition beyond academia of the need to develop explainable machine learning methods. In an attempt to prevent the problems that could occur from the integration and the rising importance of uninterpretable algorithms in a wide range of areas, the European Union (EU) introduced a right to explanation in the General Data Protection Regulation Act (GDPR)~\cite{GoodmanF17} in May 2018. While the legal requirement for explanations is recent and only in specific geographic areas (the EU), the term explainable artificial intelligence was coined in 2004~\cite{LentFM04}, whereas the problem of creating explanations in AI can be traced back to the mid-1970s~\cite{moore1988explanation}. 

The need for explainable artificial intelligence goes beyond justifying the decisions made by ML systems to a human observer for achieving trust and, in some areas, legal compliance (as argued above). Explainable techniques can also be used \textit{before} the system is deployed in the real-world to identify vulnerabilities and flaws, and correct them without incurring any risk. Using explanations in this way\textit{enhances control} over the system's behaviour. In a similar sense, 
explanations can also be used to guide the learning phases of the AI algorithm. Work by Zeiler has showed that using explanations during the learning phase to guide modifications in the algorithm can \textit{improve the classifier} by increasing it's accuracy and reducing the training time~\cite{ZeilerF14}. The final need for explanations is \textit{discovery}. In recent years techniques used for interpreting ML models have been able to extract new insights from complex chemical, physical and biological areas where AI systems have been used~\cite{khan2001classification, schutt2017quantum}. 

To achieve the higher aims of enhanced control, improvement, and discovery, systems should not only be interpretable but also maintain a high degree of performance in order to remain applicable. Deep neural networks (DNNs) offer some of the greatest learning capabilities and have far surpassed traditional ML methods in challenging areas like audio recognition~\cite{ChiuSWPNCKWRGJL18}, image classification~\cite{KrizvskySH17} and natural language processing~\cite{McCannBXS17}. DNNs are also one of the most complex ML algorithms in the sense that they have a high number of parameters that they use to make their decisions. This makes them also one of the most uninterpretable types of algorithms, which is why it is necessary to create methods that generate faithful and interpretable explanations for deep neural networks. 



% Current state-of-the-art methods have a trade-off between those two desiderata. Methods that generate faithful explanations capture the contribution of every individual constituent unit from the input. Individual unit represents a pixel in an image recognition task, a signal spectral power for a particular time and frequency bin in an speech recognition task and a multi-dimensional vector (representing a word) in a natural language processing task. However those units are not humanly interpretable, as they are not grouped into meaningful features which are more humanly-interpretable. They also don't give insight into \textit{how} parts of the input interact for the model to reach its decision. On the other hand, the methods that generate interpretable explanations identify input features that have contributed to a given prediction and present them as an explanation, but they approximate the function that the neural network uses and that approximation may not accurately reflect how the network has reached a given classification, hence it is not faithful. 


% Rephrasing the problem definition at the beginning of the document, the aim of this PhD is to create a method for interpretation of deep neural networks that combines the two desiderata - faithfulness and interpretability. To achieve interpretability the method should group related units from the input into features and show their contribution, as well as expose how different part of the input are used. To attain faithfulness the method should also not approximate the function with which the deep neural network makes classifications and view the classifier as a black box, but make use of the neural networks structure, weight and activations. 

The next chapter (Chapter 2) of this report explains the key concepts, structure and learning in artificial neural networks. Next, Chapter 3 provides greater insight into the various approaches of interpretability of deep neural networks, where the each specified method is evaluated and compared to the others. Chapter 4 focuses on a framework for generating rich and faithful explanations for DNN, which restates the problem at the beginning of the document in more detail, divides the problem into sub-problems and offers a proposed solution for those problems. Next, Chapter 5 explores the first part of the framework in further detail.

