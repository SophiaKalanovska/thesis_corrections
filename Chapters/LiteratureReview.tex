\chapter{Literature Review}
\label{chap:lit}
\section{Introduction}

Building upon the foundational understanding of artificial neural networks' capabilities an structure discussed in the previous Chapter~\ref{chap:background}, this chapter transitions into a critical examination of deep neural networks explainability. While these networks exhibit high adaptability and robustness, often surpassing traditional algorithms in complex scenarios~\cite{HeZRS16, SilverHMGSDSAPL16, DengHK13, LeCunBH15}, their intrinsic decision-making processes remain largely obscure. This opacity presents a significant challenge, especially in applications with far-reaching consequences, necessitating a deeper exploration into the explainability of these models. As neural networks become more involved in critical areas like healthcare, finance, and autonomous systems, the need to comprehend their decision-making processes intensifies. Understanding what the model has learned is pivotal not just for model validation and trust-building but also for identifying potential biases and ensuring ethical AI practices. 

The literature review chapter covers the general directions of research in explainable deep neural networks (XAI). It delves into various methodologies and frameworks that have been developed to make these complex models more interpretable. Further, the review explores the different metrics and benchmarks used to evaluate the effectiveness of XAI methods, as well as the ongoing open questions and challenges in the field.

\section{Evolution of Explainable AI and Categorisation of Explainability Methods}

The evolution of Explainable AI (XAI) as a field is vividly captured by the increasing number of scholarly publications over recent years, as illustrated in Figure \ref{Fig:XAI}. This trend underscores the growing academic and industry interest in making AI systems more transparent and understandable, paralleling the rapid advancements in artificial intelligence and machine learning.

The initial stages of XAI can be traced back to simpler AI models, where explainability was often a byproduct of the model's simplicity and transparency. Early AI systems, such as rule-based systems~\cite{hayes1985rule} and linear models~\cite{searle1997linear}, were relatively easy to interpret. More recently, the research focus has shifted towards more complex models like neural networks and deep learning, which provided better performance at many tasks~\cite{de2010combining,varadi2022alphafold,kuutti2020survey,cavalcante2016computational,huang2020artificial} but at the cost of reduced interpretability.

This transition marked a pivotal moment in the evolution of XAI. The complexity of deep learning models, characterised by their deep architectures and non-linear transformations, made it challenging to understand their decision-making processes. This complexity barrier sparked a surge in XAI research, aiming to demystify these ``black box'' models~\cite{VandewieleJOTH16, BastaniKB17a, ZhangWZ18a, NguyenYC16, SabourFH17, LinsleySES19, ShiXXCLLG21, SimonyanVZ13, SpringenbergDBR14, Ribeiro0G16, LundbergL17, ElenbergDFK17, Ribeiro0G18, ShrikumarGK17, SundararajanTY17, SmilkovTKVW17, ChattopadhyaySH18, bach2015pixel}.

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.9\textwidth]{Figures/documents_bar_graph.png}
\end{center}
\caption{Evolution of the number of total publications whose title, abstract and/or keywords refer to the field of XAI during the last years. Data retrieved from Scopus (November 20th, 2023)~\cite{Scopus} by using the search terms: XAI, Interpretable Artificial Intelligence and Explainable Artificial Intelligence.}
\label{Fig:XAI}
\end{figure} 

Previous research has categorised explainable AI models using various criteria that relied on factors like structure, scope, dependence, and dataset~\cite{IbrahimS23}. Structure wise, AI systems can be explainable by nature (\ie intrinsic) or explainable by interpretability methods after the black-box model is already trained (\ie post-hoc)~\cite{DuLH20, abs-1901-04592}. Intrinsic explainability is inherent in the model's architecture, making it transparent and interpretable from the outset. In contrast, post-hoc explainability involves techniques applied to already trained, opaque models to elucidate their decision-making process. Regarding scope, XAI models can either focus on individual data points for specific explanations (local) or assess overall network behaviour (global)~\cite{LiCSBGQWGZXC22, ArrietaRSBTBGGM20, DuLH20}. Local models are crucial for understanding and justifying individual decisions, whereas global models offer a broader perspective on the network's decision patterns.
The dependency of XAI models also varies. Some are designed for specific AI systems (model-specific), offering detailed insights tailored to particular network architectures. Others are more versatile, applicable across various networks (model-agnostic), though they may sacrifice some specificity in their explanations~\cite{ArrietaRSBTBGGM20, DuLH20}. When categorised by the dataset, explainability models could be separated by the intput data they require, such as images, text, and tabular data~\cite{ArrietaRSBTBGGM20}.


This chapter's literature review adopts a similar structure. The aim of this literature review is not to present the whole of the explainable artificial intelligence literature, but to provide a focused overview of the methods that are either specifically designed or applicable for solving the problem of interpretability in deep neural networks. It explores the nuances of intrinsic and post-hoc explainability within the context of deep learning, highlighting how each approach contributes to understanding complex neural architectures. The review also examines the local and global scopes of explainable artificial neural networks, analysing how these perspectives offer insights into individual predictions and overall network behaviour. By focusing on these aspects, the chapter provides a comprehensive review of the current research in explainable deep learning methods, delving into the various metrics for evaluating it, and concluding with an analysis of future prospects and inherent limitations.

\section{Intrinsically Interpretable Models}

Intrinsic interpretability in neural networks focuses on the inherent transparency of their architecture. Such models are intentionally designed for simplicity and clarity, ensuring that the process of transforming input data into outputs is easily understandable. This approach is in stark contrast to more complex, opaque `black-box' models where the decision-making mechanism are far from obvious. The significance of intrinsic interpretability becomes especially pronounced in sensitive areas, where comprehending the rationale behind decisions is just as vital as the decisions themselves.

To illustrate, the most basic models are naturally endowed with global interpretability. Consider logistic and linear regression models, which are grounded in linear equations. These equations establish a direct, transparent connection between input variables and the output. This linear relationship allows for a readily comprehensible view of how alterations in input features influence predictions. Decision Trees provide another example. They function by segmenting the input space based on input feature values, constructing a tree-like set of decision rules. The journey from the tree's root to a leaf symbolises the decision-making path, offering a clear and traceable route to the final decision. Similarly, K-Nearest Neighbours (KNN) adheres to a straightforward principle. It assigns classifications to new instances based on the predominant class among its `k' nearest neighbours in the feature space, drawing its interpretability from the intuitive concept of similarity to known data points. Rule-based Learners, operating on explicit if-then rule sets, also exemplify this clarity, as each rule directly maps specific conditions to outcomes.

\subsection{Intrinsic Globally Interpretable Models}
\label{sec:Intrinsic}

Building upon these foundational models, intrinsic interpretability extends beyond inherently clear-cut systems. One can enhance a model's interpretability by integrating constraints within the network (discussed in Section~\ref{sec:Constraints}). This approach moulds the network into an inherently interpretable form. Additionally, interpretability can be achieved by employing simpler, understandable models like decision trees, rule-based models, or linear models to approximate the functionality of a complex system (as explored in Section~\ref{sec:extraction}). This strategy bridges the gap between high performance and transparency, harnessing the strengths of both simple and complex models in a cohesive framework.

\subsubsection{Adding Interpretability Constraints}
\label{sec:Constraints}

This approach is rooted in the integration of specific constraints into the neural network's architecture. These constraints are designed not just to direct the learning process, but also to ensure that the model's internal mechanisms remains comprehensible. This is a significant departure from traditional deep learning models, where the focus is predominantly on optimising performance, often at the expense of transparency. In this subsection, we delve deeper into the application of interpretability constraints in neural networks, highlighting the work of Zhang~\textit{et al.}~\cite{ZhangWZ18a} and the development of Capsule Networks (CapsNet)~\cite{SabourFH17} as key examples.


Zhang~\textit{et al.}'s~\cite{ZhangWZ18a} work proposes a method to transform traditional convolutional neural networks into interpretable convolutional neural networks. Interpretable convolutional neural networks have each filter in a deep convolutional layer represent a specific object part, which is automatically assigned during training. This is unlike traditional CNNs where a filter might represent a mixture of patterns~\cite{NguyenYC16} and the training process doesn't impose the patterns learned. The method is versatile and can be applied to various types of CNNs with different structures. It also uses the same training data as ordinary CNNs without requiring any annotations of object parts or textures for supervision. The method achieves its objective by introducing a special loss function, which pushes the filters towards representing an object part. A masking operation is also used to aid in filtering out noisy activations. This results in disentangled representations, resulting in interpretable CNN filters' activations that could detect semantically meaningful natural objects. The location of filter activation is also more stable and consistent than in traditional CNNs, meaning when a filter is trained to recognise a specific part, it activates in the same or very similar locations across different images when that part is present. However, there was a slight decrease in discrimination power, which the researchers aimed to keep within a small range.

A different approach introduced a novel type of neural network architecture known as a Capsule Network (CapsNet)~\cite{SabourFH17}. It combines a group of neurons, and a vector, called an activity vector, which represents various properties of a specific type of entity such as an object or an object part. The length of this activity vector indicates the probability of the entity's presence, and its orientation represents initial parameters like pose, deformation, texture, etc. Capsules use a dynamic routing process to determine where their outputs should be sent in the network hierarchy. This offers more interpretability than standard CNNs and avoids the exponential inefficiencies in representing spatial hierarchies, common in CNNs. The network's architecture also allows for effective segmentation and recognition of overlapping or complex objects, which is shown on the MNIST dataset~\cite{deng2012mnist}, especially excelling at recognising highly overlapping digits. Similar to generative models, CapsNets also tend to account for all aspects of an image. Therefore, the varied backgrounds in the CIFAR-10 dataset~\cite{cifar} challenged the model, contributing to poorer performance compared to datasets with less complex backgrounds.


The integration of interpretability constraints within neural network models, as exemplified by the approaches of Zhang ~\textit{et al.}'s~\cite{ZhangWZ18a} work and the Capsule Network~\cite{SabourFH17}, reveals a fundamental trade-off between interpretability and prediction accuracy. While these methods enhance the clarity and understanding of network decisions by making each filter or capsule represent specific, identifiable object parts, this increase in transparency often comes at the cost of reduced discriminative power. Interpretable models like the interpretable CNN and CapsNet excel in providing meaningful, disentangled representations and handling complex visual tasks with greater human-like understanding. However, their performance, in terms of raw prediction accuracy, may not always match that of less interpretable, traditional neural network architecture.

\subsubsection{Model Extraction}
\label{sec:extraction}

Model extraction offers an alternative route to achieving intrinsically interpretable models. This method involves simplifying complex, often non-transparent models into more understandable forms without major performance trade-offs. By distilling a complex model like a deep neural network into more interpretable structures such as decision trees, rule-based systems, or linear models, model extraction maintains prediction accuracy while enhancing interpretability. This makes it easier for users to trust and comprehend the model's decisions.

One notable example is GENESIM~\cite{VandewieleJOTH16}, an algorithm that converts an ensemble of decision trees into a single decision tree. This new tree is not only more interpretable but also maintains sometimes even improves, accuracy compared to traditional decision tree induction techniques. GENESIM stands out for its balance of low model complexity and high interpretability, matching the performance of ensemble techniques.

Bastani~\textit{et al.}~\cite{BastaniKB17a} introduced a method to create an interpretable model, like a decision tree, from a complex, opaque model. Their algorithm employs active learning to build an interpretable model from a complex model. It actively samples numerous training data points, labelled using the complex model, to ensure that the interpretable model doesn't overfit to a limited initial dataset. Their method outperforms the CART decision tree learning algorithm~\cite{BreimanFOS84} in approximating complex models and has practical applications in understanding biased features and comparing models trained on the same data.

Another significant approach is the Classification Accuracy Reduction (CAR) method~\cite{VandewieleJOTH16}. It focuses on making deep convolutional neural networks (CNNs) more interpretable without significantly compromising accuracy. CAR employs a greedy structural compression scheme that prunes less impactful filters, resulting in smaller, more interpretable CNNs. Despite fewer filters, these networks retain a diverse filter range and maintain nearly original accuracy levels. The importance of each image category to each CNN filter is also quantified. Tested on various datasets and CNN architectures like LeNet~\cite{LeCunBDHHHJ89}, AlexNet~\cite{KrizhevskySH12}, and ResNet-50~\cite{HeZRS15}, CAR shows its effectiveness in reducing filters without losing functional diversity or accuracy.

Model extraction methods present a compelling approach for creating interpretable models from complex, opaque systems. Techniques like GENESIM~\cite{VandewieleJOTH16}, Bastani et al.'s method~\cite{BastaniKB17a}, and the CAR approach~\cite{VandewieleJOTH16} effectively simplify models such as deep neural networks into more transparent structures like decision trees or reduced CNNs. These methods aim to maintain the accuracy of the original models while significantly improving their interpretability. This increased transparency facilitates user trust and understanding, crucial in fields where decision-making processes need to be transparent and justifiable.

However, there are inherent drawbacks to model extraction. A critical aspect to consider in model extraction techniques is that even when these simplified models match the accuracy of their original counterparts, they may not provide a true representation of what the original model has learned. This discrepancy arises because accuracy alone does not capture the entire learning process of a complex model. The original models, especially deep neural networks, often capture intricate, multi-dimensional patterns and relationships within the data. When these models are distilled into simpler forms, such as decision trees or pruned neural networks, the subtleties and complexities of these relationships can be lost or overly simplified. Additionally, while these techniques aim to maintain the original model's performance, there can be instances where the extracted model's accuracy is slightly compromised, especially in scenarios involving highly intricate data patterns.


\subsection{Intrinsic Locally Interpretable Models}

Unlike globally interpretable models which aim to provide a broad understanding of a model's overall functioning, intrinsic locally interpretable models ensure that learning methods focus on specific parts of the input or learn concrete patterns. The two types of methods deployed in this category are \emph{attention based} methods and \emph{loss based} methods.


\subsubsection{Attention-based explainability}

This approach relies on the concept of \textit{attention mechanisms} primarily used in neural networks. In essence, these mechanisms allow the model to focus on specific parts of the input data when making a decision, much like how humans pay more attention to certain aspects of a problem when solving it. In the context of neural networks, especially those used for natural language processing and image recognition, attention mechanisms can provide insights into which parts of the input data (such as words in a sentence or areas in an image) the model considers most important for a particular prediction. This localised focus helps in understanding the model's decision-making process on a case-by-case basis. For example, in text analysis, an attention-based model can highlight specific words or phrases that were pivotal in determining the sentiment of a sentence. Similarly, in image recognition tasks, such models can illustrate which regions of an image were most influential in classifying the image. This granularity not only aids in interpreting individual predictions but also in identifying potential biases or errors in the model's learning process.

Bahdanau et al.'s work~\cite{BahdanauCB14} exemplifies this by introducing an attention-based model that autonomously describes image content. It visualises how the model learns to focus on salient objects while generating corresponding words. The paper addresses the challenge of combining the computer vision task of object detection with the natural language processing task of generating coherent sentences. The model uses convolutional neural networks (CNNs) to encode images into feature vectors and recurrent neural networks (RNNs) to decode these vectors into natural language sentences.

Another domain where this type of explainability is used is fine-grained classification. Fine-grained classification involves recognising subordinate-level categories, such as different types of birds or dog breeds, which is challenging due to subtle and local differences among categories. The problem is further compounded by variances in pose, scale, and rotation. Xiao~\textit{et al.}'s~\cite{XiaoXYZPZ15} work proposes applying visual attention to fine-grained classification tasks using deep convolutional neural networks. The model integrates bottom-up attention (\ie attention shaped by current features) for proposing candidate patches, object-level top-down attention (\ie attention shaped by prior knowledge) for selecting relevant patches, and part-level top-down attention (\ie directed by knowledge about which parts of an object are most informative for the classification task) for localising discriminative parts (see Figure~\ref{Fig:loss-based-grained}). The approach improved fine-grained classification accuracy significantly, particularly under weak supervision settings.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/Xiau.png}
	\end{center}
	\caption{Complete classification pipeline of the Xiao~\textit{et al.}'s~\cite{XiaoXYZPZ15} method. The photo shown is from~\cite{XiaoXYZPZ15}}
	\label{Fig:loss-based-grained}
\end{figure} 

The works of Bahdanau~\textit{et al.}\cite{BahdanauCB14} and Xiao~\textit{et al.}\cite{XiaoXYZPZ15} exemplify the practical application and effectiveness of attention-based models. Bahdanau ~\textit{et al.'s }\cite{BahdanauCB14} model, which combines object detection in images with sentence generation, showcases the model's ability to focus on relevant objects and articulate them in natural language. Xiao~\textit{et al.}~\cite{XiaoXYZPZ15} approach to fine-grained classification shows how visual attention can enhance accuracy in identifying subtle differences among categories, particularly in challenging scenarios with minimal supervision. However, integrating attention mechanisms into a model's architecture does alter the way the model performs inference and can change the learned function, but it does not fundamentally overhaul the entire inference process or the core structure of the model. This does mean that this types of methods, similarly to Section~\ref{sec:extraction} do not provide a true representation of what the original model has learned. 


\subsubsection{Loss-based explainability}

This class of methods focuses on adjusting the loss function (\ie the core metric that measures the difference between the predicted and actual outcomes) to guide the network towards learning features that are more interpretable to humans or align with preselected aspects deemed important. In traditional machine learning models, the loss function primarily drives the accuracy and performance of the model, often at the cost of interpretability. However, in loss-based explainability, the loss function is intentionally designed or modified to incorporate interpretability constraints. These constraints can be in the form of regularization terms that penalize the model for learning complex, non-interpretable patterns, or they can explicitly encourage the model to focus on specific, predefined features that are easier for humans to understand.

Unlike previous attention mechanisms, Loss-based attention~\cite{ShiXXCLLG21} does not add attention layers to CNN. The authors introduce a mechanism that uses the same parameters to learn both patch weights and image predictions simultaneously, linking the attention mechanism with the loss function.  The proposed loss-based attention mechanism is designed to focus on significant patches during training, preserving the spatial relationship of these patches and avoiding reliance on additional annotations (see Figure~\ref{Fig:loss-based}). The proposed deep architectures remove max-pooling or stride operations in convolutional layers as it boosts the accuracy of patch identification.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/Loss-based.png}
	\end{center}
	\caption{Images highlighting the areas found relevant by the loss-based attention. The photos shown are samples from~\cite{ShiXXCLLG21}}
	\label{Fig:loss-based}
\end{figure} 



Global-and local attention (GALA) was integrated with neural networks like ResNet-50 supervised by ClickMe maps to encourage the selection of visual features favoured by humans~\cite{LinsleySES19}. ClickMe.ai is an online game for large-scale data acquisition. It involved participants playing with convolutional neural network partners to recognise images from the ILSVRC12 challenge. Players selected image parts informative for recognising the category, creating top-down attention maps (\ie attention shaped by prior knowledge). ClickMe maps were used to supervise GALA modules, introducing an additional loss to the training process. Models supervised with ClickMe maps demonstrated improved object categorisation accuracy and predicted ClickMe maps more effectively (see Figure~\ref{Fig:GALA}). This led to improved interpretability as ClickMe supervision led to features that were more local and consistent with human-selected features.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/GALA.pdf}
	\end{center}
	\caption{Images showing the areas found relevant by GALA both supervised by ClickMe maps and without. The photos shown are samples from~\cite{LinsleySES19}}
	\label{Fig:GALA}
\end{figure} 

\subsection{Discussion: Intrinsically Interpretable Models}

The methods in the category of intrinsically interpretable models focus primarily on either learning an interpretable version of the model, modifying the existing model to enhance its interpretability or changing the learning process to produce a more interpretable model. These types of approaches offer a compromise, allowing the use of accurate, complex models while still gaining some level of interpretability. It makes it possible to apply AI in critical domains without completely sacrificing the model's predictive power. They are generally easier to implement and require less computational power than complex models. Their simplicity makes it easier to communicate findings and understand the learned function by the model.


This approach is crucial in fields where the ability to understand and trust AI decisions is as important as the decisions themselves. However, it's important to acknowledge the trade-offs involved, particularly in terms of the accuracy of the model. The introduction of interpretability often impacts the model's accuracy. For instance, when modifying convolutional neural networks to enhance interpretability by having each filter represent a distinct object part, there's often a slight compromise in the model's ability to discriminate effectively. This phenomenon is also evident in scenarios where a new model is designed to be inherently interpretable, such as with decision trees. In these cases, to preserve a level of accuracy comparable to the original model, the decision tree may incorporate highly detailed and complex information. As a result, despite the model's intrinsically interpretable nature, the complexity of the information it encodes can still render it challenging to comprehend easily. Conversely, if the learned model is overly simplistic, it can overlook complex interactions in the data. The challenge lies in finding a balance between interpretability and accuracy. Too much emphasis on one can significantly diminish the other. Ongoing research aims to mitigate this issue, seeking methods that retain high accuracy while providing clear insights into the model's decision-making process.

 
Given the challenges associated with intrinsically interpretable models, recent literature is increasingly concerned with interpreting models post-training~\cite{MarkusKR21}. These approaches involves applying techniques to already trained models to extract insights and understand their decision-making processes. They provide insights into complex models like deep neural networks without altering the underlying model architecture and need for retraining.

\section{Post-hoc Explanations}
\label{sec:post-hoc}

This section shifts focus to post-hoc explanations in machine learning, representing a departure from intrinsic interpretability. Unlike the direct integration of transparency into a model's architecture, post-hoc explanations aim to clarify the decision-making processes of models that have already been trained. These methods arise in response to the limitations and compromises found in intrinsically interpretable models, especially when applied to complex systems.

The objective of post-hoc explanations is to retain the performance and pattern recognition abilities of complex models while revealing the mechanisms behind their decisions. This goal is pursued without altering the structural integrity of the models, thereby preserving their accuracy and intricacy. Exploration in this section includes an examination of various methodologies and tools used in post-hoc explanations, focusing on their role in improving the interpretability of complex models. The effectiveness of these techniques across different application scenarios, along with their limitations and the challenges they face, is assessed.


\subsection{Post-hoc Global Explanations}
\label{section:postglobal}

Post-hoc global explanations attempt to map the overall logic and patterns that a model uses to make its decisions across a wide range of inputs. This involves distilling the complex, often multi-dimensional decision processes of a model into more comprehensible, overarching principles or patterns. The methods employed here typically aim to identify general trends, rules, or dependencies that are indicative of the model's behaviour on a macro level.

\subsubsection{Activation Maximisation Methods}

A key method for providing a global explanation involves identifying the preferred inputs for neurons at specific layers, commonly framed within the activation maximisation (AM) framework as outlined by Simonyan~\textit{et al.}~\cite{SimonyanVZ13}. This work involves generating inputs that, when fed into the model, yield high scores for a target category. These scores are determined by the model’s classification layer for each input, with the goal of finding an input that not only maximises the class score but also retains a degree of normal appearance. This is achieved through the incorporation of L2 regularisation. The technique leverages back-propagation, typically used for training neural networks to adjust internal parameters, but with a slight modification. In this scenario, the network's weights remain fixed post-training, and the process starts with a blank input. Iterative adjustments are made until the input is identified that maximises the class score while appearing normal (see Figure~\ref{Fig:AM}). This technique effectively reveals the network's internal representation of a class, derived from its training, and can be applied to neurons at any neural network level.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/MA.pdf}
	\end{center}
	\caption{This images are showing the class appearance, learnt by a ConvNet, trained on ILSVRC-2013. The photos shown are samples from~\cite{SimonyanVZ13}}
	\label{Fig:AM}
\end{figure} 

However, the apparent simplicity of this approach hides the challenges it faces, particularly in generating interpretable inputs. The optimisation process can often produce unrealistic and noisy explanations. Without adequate regularisation, this process might yield results that activate neurons but remain unrecognisable. Overcoming this challenge involves constraining the optimisation process with natural priors, ensuring synthetic inputs resemble natural ones more closely. Researchers have proposed various hand-crafted priors like total variation norm and Gaussian blur~\cite{NguyenDYBC16}. More robust regularisation can be achieved using natural image priors from generative models like Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs)~\cite{NguyenDYBC16}. In Nguyen~\textit{et al.}'s paper~\cite{NguyenDYBC16}, the results (see Figure~\ref{Fig:VAE}) underscore the effectiveness of using generative model priors in enhancing visualisation quality when generating images. These visualisations provide deep insights into CNN processing, especially when trained on datasets like ImageNet~\cite{deng2009imagenet}.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/VAE.png}
	\end{center}
	\caption{Images synthesised from scratch to highly activate output neurons in the CaffeNet deep
neural network. The photos shown are samples from~\cite{NguyenDYBC16}}
	\label{Fig:VAE}
\end{figure} 
Using traditional activation maximisation techniques shows only one type of feature that a neuron detects, research by Nguyen~\textit{et al.}~\cite{NguyenYC16} shows the multiple facets detected by a neuron. For instance, a neuron linked to the concept of a ``grocery store'' may be activated by diverse visual cues, ranging from rows of produce to images of storefronts (see Figure~\ref{Fig:VAE_2}). This nuanced understanding of neuronal functionality marks a significant departure from earlier methods, which often produced images with unnatural colour distributions and incoherent, repetitive fragments, failing to capture the diverse stimuli each neuron can respond to. The paper delves into two principal deep visualisation techniques: activation maximisation and code inversion. Activation maximisation is focused on identifying an image that maximally activates a particular neuron, thereby shedding light on the specific features that the neuron is sensitive to like in the work by Simonyan et al~\textit{et al.}~\cite{SimonyanVZ13}. Code inversion, in contrast, involves creating an image that yields an activation vector similar to that produced by a specific real image, thereby unravelling the image-specific information encoded by the DNN at a certain layer. To address the issue of fragmented and inconsistent image outputs seen in previous techniques, Nguyen~\textit{et al.} introduce a novel centre-biased regularisation approach. This method strategically biases the image optimisation process to favour the formation of a single, central object in the generated image. The result of this adjustment is a more coherent and structurally integrated visual output, enhancing the overall interpretability and coherence of the images produced through these deep visualisation processes.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/Nyuyen.png}
	\end{center}
	\caption{A multifaceted visualisation of example neuron feature detectors from the eight layer. The photos shown are samples from~\cite{NguyenYC16}}
	\label{Fig:VAE_2}
\end{figure} 

Work by Zhou~\textit{et al.}~\cite{ZhouKLOT14} further shows that when CNNs are trained to classify scenes, they inherently develop object detectors within their structure. This is significant because these object detectors form without explicit training or supervision on objects. This finding implies that a single network can perform multiple tasks like scene recognition and object localisation in one forward pass, without being explicitly taught about objects.

\subsubsection{Sequential Data Methods}

Other approaches aim to understand the performance and limitations of LSTMs, a type of RNN that has gained popularity due to its success in various machine learning tasks involving sequential data. Karpathy~\textit{et al.}~\cite{KarpathyJL15} use character-level language models to analyse what most strongly activate individual units in this layer. These investigations reveal that certain units within RNNs can effectively grasp intricate linguistic features, such as syntax and semantics. LSTMs that can even track long-range dependencies like line lengths, quotes, and brackets. In work by Kádár~\textit{et al.}~\cite{KadarCA17}, the authors use a word-level language model to examine the linguistic attributes encoded by the individual hidden units of a Recurrent Neural Network (RNN). Visual analysis from this work reveals that select units primarily respond to distinct semantic types. Other units, however, are adept at identifying specific syntactic categories or fulfilling particular dependency roles. Additionally, a notable observation is that certain hidden units are capable of maintaining their activation levels into subsequent temporal stages. This functionality sheds light on the RNN's proficiency in understand long-term dependencies and multifaceted linguistic elements. 

Work by Peters and colleagues introduces ELMo~\cite{PetersNIGCLZ18}, a new type of word representation that models both complex characteristics of word use (like syntax and semantics) and how these uses vary across linguistic contexts. It shows that similar to CNNs, Recurrent Neural Networks (RNNs) are capable of learning hierarchical representations by inspecting different hidden layers. The research also shows that different layers of deep bidirectional RNNs (biRNNs) encode distinct types of information. For example, lower layers of a deep LSTM model were found to be more effective at tasks like part-of-speech (POS) tagging, indicating their ability to capture syntactic information. On the other hand, higher layers were better at encoding semantic information, consistent with machine translation encoders.


\subsubsection{Concept-Based Global Explanations}

Testing with Concept Activation Vectors (TCAV), bridges the gap between model feature representations and human-understandable concepts. Proposed by Kim et al., TCAV introduces Concept Activation Vectors (CAVs) to quantify the influence of predefined human concepts on a model's predictions~\cite{KimWGCWVS18}. CAVs are derived by training a linear classifier to distinguish activations of neural network layers associated with a specific concept from random examples. This representation allows the use of directional derivatives to compute a metric of sensitivity, quantifying the importance of each concept in decision-making. Unlike traditional attribution methods, TCAV operates at the conceptual level, aligning with human intuition and enabling domain experts to evaluate models without requiring extensive technical knowledge.

This method employs randomisation-based significance testing to ensure robustness against spurious correlations. TCAV has been validated across applications such as diabetic retinopathy diagnosis and image classification, revealing model biases and enhancing user trust. It effectively complements methods like saliency maps~\cite{SpringenbergDBR14} and Grad-CAM~\cite{SelvarajuCDVPB20} (discussed in Section~\ref{cam} \& ~\ref{decov}) by providing insights into a model's global decision-making patterns through user-defined concepts.

\subsection{Post-hoc Local Explanations}

Shifting from the overarching perspectives of global explanations, this subsection delves into the realm of post-hoc local explanations in machine learning. Contrasting with global explanations, local explanations focus on providing insights into specific individual predictions made by a model, offering a detailed understanding of the decision-making process on a specific instance.

Local explanations are particularly crucial in scenarios where the reasoning behind a single prediction needs to be understood and justified. This level of granularity is essential in fields like medicine, where understanding the rationale behind a specific diagnosis can be as critical as the diagnosis itself, or in financial services, where individual loan approval or rejection decisions must be transparent.


\subsubsection{Local Approximation Based Explanation}

Local approximation based explanation methods operate on the fundamental premise that the predictions made by a machine learning model for inputs in the vicinity of a specific query instance can be effectively represented using a simpler, interpretable model. This approach does not necessitate the interpretable, or ``white-box,'' model to demonstrate accurate predictions across the entire input space of the original, more complex ``black-box'' model. Instead, its primary requirement is to closely approximate the behaviour of the black-box model within a constrained, localised region surrounding the initial input of interest. In this context, the local approximation is achieved by constructing a white-box model that mimics the decision-making process of the black-box model for a subset of data points that are in close proximity to the query instance. This subset is often selected based on similarity measures or other criteria that ensure their relevance to the query instance. Once the local white-box model is trained and its performance in approximating the black-box model within the defined neighbourhood is validated, the next step involves dissecting the white-box model to understand its decision-making process. This is typically achieved by analysing the model's parameters or decision rules, which are inherently more interpretable than those of the original model.

Ribeiro~\textit{et al.}'s work on LIME (Local Interpretable Model-Agnostic Explanation)~\cite{Ribeiro0G16} is an algorithm designed to explain the predictions of any classifier in an interpretable and faithful manner. It approximates a complex model locally with an interpretable model, providing insights into individual predictions. Alongside LIME, the paper proposes SP-LIME, a method to select a set of representative instances and explanations in a non-redundant way, thereby helping users understand and trust the model as a whole. The technique starts by perturbing the input, which can either be done with a human-in-the-loop or autonomously. In the former case, the method starts by asking the user to perturb the input in a way that makes sense for them (\eg  letting the user remove a contiguous region of pixels in an image (\textit{super pixel}) or important words or phrases from text). In the latter case perturbing of the input is done randomly. This process creates instances that do not have some feature that is present in the original instance and are in the local region (with respect to the space of instances) of the original instance. These perturbations result in a new, local dataset around the instance being explained. This local dataset consists of the perturbed versions of the original instance, along with the predictions of the classifier for these perturbed instances. LIME then trains an interpretable model, such as a linear regression or decision tree, on this local dataset. The interpretable model is designed to approximate the predictions of the complex model as closely as possible, but only in the vicinity of the instance being explained. The contribution of each feature is then computed by passing it through the new explainable classifier. If a change in a feature changes the way the instance is classified, the contribution of that feature is assigned a value of one and if it does not the contribution of the feature is zero (binary feature's contribution). If the feature changes the classification then its presence constitutes a part of the explanation for why the original instance is classified the way it is. In this way, a feature can either be important or not important to the classification. The entirety of the explanation in LIME amounts to a binary vector where each element is a feature assigned one or zero (see Figure~\ref{Fig:LIME}). The authors conduct extensive experiments to validate the effectiveness of LIME and SP-LIME. These include simulated user experiments to measure the impact of explanations on trust, as well as evaluations with human subjects on tasks like choosing the best classifier, improving an untrustworthy classifier, and gaining insights into classifier behaviour. Subsequent implementations of LIME extend this framework to extract pseudo-relevance scores based on the relevance of the classification to the surrogate models. However, given that this is importance to an over-fitted surrogate, the relevance scores are somewhat misleading, and are often combined with some form of regularisation. As such, the default explanation is typically binary. 

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/LIME.png}
	\end{center}
	\caption{Explanations for an image classification prediction made by Google’s Inception neural network for electric guitar, acoustic guitar and labrador. The photos shown are samples from~\cite{Ribeiro0G16}}
	\label{Fig:LIME}
\end{figure} 


LIME is foundational work in the area of ML interpretability and many methods have either been inspired by LIME or address some of its limitations~\cite{ElenbergDFK17, Ribeiro0G18, WhiteG20}. One limitation of the method is the turnaround time. In order to compute each feature contribution, LIME has to sample a multitude of instances in the case when there is no human-in-the-loop, each of them perturbing a different area of the original instance (a feature) and then classify all those perturbed instances and compare them to the original classification. This results in slow turnaround time, making the method unusable for explanations on-the-fly. The problem of \emph{slow explanation generation}, led Elenberg~\textit{et al.}'s~\cite{ElenbergDFK17} work for an efficient streaming algorithm, named Streak. The algorithm is efficient in terms of memory usage and does not require prior knowledge. When compared to LIME is shown to be produced at a far greater speed with a turnaround time up to 10 times faster than LIME. This improvement in speed is particularly significant for large neural networks like Inception V3. Another problem of Lime is the inability of the method to create counterfactual explanations, which are critical in scenarios requiring a nuanced understanding of model behaviour, as they indicate how a model's predictions could change with alterations in feature values. Methods like CLEAR (Counterfactual Local Explanations via Regression)~\cite{WhiteG20} have been developed to address this shortcoming, offering high-fidelity counterfactuals that outperform LIME in various domains. One of the significant limitations of LIME is its lack of a fidelity measure, as defined in the CLEAR paper, where fidelity reflects how accurately an explanation model represents the underlying decision boundary and generates reliable counterfactuals.

Another limitation of LIME that was noted by its authors is that there is a lack of clarity whether the explanation produced can be used only for the original instance, or for the original instance as well as other similar instances (i.e.\ instances which are in the local region with respect to the space of instances). This is referred to as the problem of ``\emph{unclear region of explanation}''

This has been addressed in subsequent work by Ribeiro~\textit{et al.} in a method called Anchors~\cite{Ribeiro0G18}. The method uses a concept called "anchors," which are high-precision rules representing local "sufficient" conditions for predictions. These explanations are designed to be model-agnostic, meaning they can be applied to any black-box model, providing high-probability guarantees for their accuracy. It finds all the features that matter for the classification and then creates if--then rules, where the antecedent part of the statement lists the features that can lead to a change in classification and the consequent part contains the classification itself. If a different data instance is presented that contains all the features in the ``if'' statement, then that feature is in the region of explanation of the original instance and the explanation will remain the same. The rules apply only when all the features are present in the data instance (all \emph{conditions} are met). The if statement is seen as the anchor of this local prediction, which is where the method takes its name. 
The process involves defining a precision level for an anchor and ensuring that the anchor achieves this precision with high probability (see Figure~\ref{Fig:Anchors}). This approach takes into account the probabilistic nature of predictions in machine learning, where absolute certainty is often unattainable. The goal is to maximise the coverage of an anchor, defined as the probability that it applies to samples from a perturbation distribution. To address the limitations of a greedy approach in finding anchors, the authors propose a beam-search method. This method maintains a set of candidate rules and selects the ones with the highest coverage, optimising the search for anchors that describe a larger part of the input space effectively. Sometimes, even the local behaviour of a model may be
extremely non-linear, so linear explanations like the ones used by LIME~\cite{Ribeiro0G16} could lead to poor performance. Anchors~\cite{Ribeiro0G18} could model the non-linear relationship as the local approximation due to the utilisation of the "if-then" rules. One of the problems of the Anchors approach is that in some more complex domains they may create very specific anchors (if conditions), which have very low coverage or are specific to the instances the model is trained on. Despite this limitation, the approach manages to achieve its original aim to have clear coverage and high precision for interpretable explanations of the local model's behaviour. 


\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/Anchors.pdf}
	\end{center}
	\caption{Explanations produced by Anchors for both image and text input. The photos shown are samples from~\cite{Ribeiro0G18}}
	\label{Fig:Anchors}
\end{figure} 



\subsubsection{Loss-based explainability}

One of the reasons to create explanations is to see if the classifier is a ``Clever Hans predictor'', meaning that it makes correct predictions but for the wrong reasons. If it turns out that it is, (i.e.\ it has erroneously explored features in the data that \textit{should be} irrelevant) the classifier is retrained under changed conditions (e.g.\ without the part of the data that contained spurious correlations). An alternative way to solve this issue is by feature selection. Feature selection is a technique that only exposes the machine learning algorithm to ‘good’ input features.

Work by Ross~\textit{et al.} focuses on creating ML algorithms that are “right for the right reasons” (RRR)~\cite{RossHD17}. The method uses LIME to produce explanations of how the model creates it's decisions and uses a human-in-the-loop to specify whether the feature should be relevant to the classification. If the feature is not relevant, the method constrains the input gradients to be small in irrelevant areas of the input space, based on an annotation matrix that indicates whether certain dimensions should be irrelevant for predicting specific observations. RRR uses binary masks to encompass the user's specification on whether the features are relevant or not, and so prevents the machine learning algorithm from learning spurious correlations which might be present within the training data. The paper shows that input gradient penalties enable learning of generalisable decision logic, even in datasets with inherent ambiguities. This method is especially effective for continuous inputs and provides a basis for further advancements in explanation optimisation.



\subsubsection{Class Activation Mapping Methods}
\label{cam}

Class Activation Map (CAM)~\cite{ZhouKLOT16} is introduced as a technique to generate visual explanations for CNN-based classification decisions. It highlights the discriminative regions of an image used by the CNN to identify a specific category. The authors show that CNNs, even when trained only with image-level labels (not object locations), can localise objects. This is attributed to the use of global average pooling layer (GAP), which calculates the average contribution of each feature map in the last convolutional layer (see Figure~\ref{Fig:CAM}). The paper proves that the use of GAP preserves the spatial information discarded by fully-connected layers. In the end, CAM overlays the activation map on the input image to identify the areas of interest the CNN used to make its prediction. CAM allows CNNs trained for classification to localise class-specific image regions in a single forward pass. This is significant for understanding and interpreting the decisions made by the network. The method achieves a top-5 error of 37.1\% for object localisation on ILSVRC 2014, close to the 34.2\% error rate of a fully supervised CNN. The paper compares global average pooling layer (GAP) over global max pooling layer (GMP), concluding that GAP is more effective for localisation tasks as it encourages the network to consider the entire extent of the object rather than focusing on the most discriminative part. The paper includes extensive experiments demonstrating the efficacy of GAP and CAM for weakly-supervised object localisation, comparing the performance with other methods on the ILSVRC dataset. They show that the localisation ability of GAP-trained networks is generic and applicable to various tasks beyond those it was trained for, including scene recognition and concept discovery. Application of the method to fine-grained recognition tasks (like bird species identification) shows significant improvement in accuracy when using localised regions identified by CAM. The technique is also used for discovering common elements in images and localising high-level concepts from weakly labelled images. The primary drawback of CAM is that it modifies the architecture of the model that is being interpreted, which does effect the prediction accuracy.

\begin{figure}[ht!]
	\begin{center}
	\includegraphics[width=0.8\linewidth]{Figures/CAM.png}
	\end{center}
	\caption{Explanations produced by CAM. The photos shown are samples from~\cite{ZhouKLOT16}}
	\label{Fig:CAM}
\end{figure} 

Grad-CAM~\cite{SelvarajuCDVPB20} was proposed to overcome the drawbacks of CAM and it does not change the CNN architecture. This technique uses the gradients of any target concept (\ie classification in a classification network) flowing into the final convolutional layer to produce a localisation map. This map highlights important regions in the image for predicting the concept. Unlike CAM, Grad-CAM does not require any specific model architecture. It can be applied to a broad range of CNN models, including those with fully connected layers and complex architectures designed for various tasks beyond simple classification. Grad-CAM is evaluated for localisation tasks and for its ability to faithfully represent the model's decision-making process. The technique provides insights into model failures and helps identify biases in datasets.

Grad-CAM++~\cite{ChattopadhyaySH18} is a more generalised version of Grad-CAM, designed to address some of its limitations, particularly in cases of localising multiple instances of the same object in an image and fully capturing single objects. Grad-CAM++ modifies the approach used in Grad-CAM by focusing on the positive gradients of the class score with respect to feature maps and incorporating higher-order derivatives to calculate the weights for the activation maps. This approach allows for a more detailed and class-specific heatmap generation, leading to better visualisation of the regions in the image that contribute to the model's decision.


Grad-CAM++~\cite{ChattopadhyaySH18} is further improved by Smooth Grad-CAM++~\cite{abs-1908-01224}, which combines elements from SmoothGrad~\cite{SmilkovTKVW17} --- methods for reducing the noise in the explanation by adding random noise to the input image and averaging the sensitivity maps across multiple such noisy images, and Grad-CAM++~\cite{ChattopadhyaySH18}. It provides sharper and more precise visual explanations of deep CNN model decisions. Smooth Grad-CAM++ allows visualisation at the level of specific layers, subsets of feature maps, or even subsets of neurons within a feature map, which is a significant advancement over previous techniques. The technique integrates gradient smoothing into Grad-CAM++, where noise is added to the sample image of interest and the average of all gradient matrices generated from each noised image is taken (see Figure~\ref{Fig:Grad-CAM}). This process leads to visually sharper maps. Similar to Grad-CAM++, it involves pixel-wise weighting of the gradients of the output with respect to a particular spatial position in the final convolutional feature map of the CNN, providing a measure of the importance of each pixel.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.7\linewidth]{Figures/Grad-CAM.png}
	\end{center}
	\caption{Explanations produced by Grad-CAM, Grad-CAM++ and Smooth Grad-CAM++. The photos shown are samples from~\cite{abs-1908-01224}}
	\label{Fig:Grad-CAM}
\end{figure} 
\subsubsection{Deconvolution Based Methods}
\label{decov}

While the advancements in CAM methods like CAM, Grad-CAM, and their successors have significantly improved the ability to interpret CNNs through localization and visualization of discriminative regions, another approach to interpretability emerges with deconvolution based methods. Unlike CAM methods, which focus on generating class activation maps to highlight important regions, deconvolution based methods like Deconv and its variants focus on reconstructing input images from learned feature maps. This shift marks a transition from methods primarily enhancing object localization to those unraveling the CNN's learned image representations, offering a complementary perspective on understanding deep neural networks. 

The deconvolutional network model (Deconv)~\cite{ZeilerKTF10} aims to build robust low and mid-level image representations beyond simple edge primitives, addressing the challenge of capturing mid-level cues like edge intersections, parallelism, and symmetry. This approach uses the filters that the CNN has learned from its training phase to break down an input image into a set of feature maps. These feature maps represent the essential components or features of the image as understood by the CNN. The process then involves reconstructing the original image from these feature maps, effectively demonstrating the CNN's understanding and internal representation of the input image. The process typically involves reversing the operations of the CNN. For each convolutional layer, a corresponding deconvolutional layer is used, where the forward and backward passes are swapped. The presence of pooling layers (especially max pooling) in CNNs presents a challenge for deconvolution. This is because pooling operations are not reversible without additional information (\eg forward pass). In standard deconvolution approaches, `switches' that record the positions of the maximum values in the pooling layers during the forward pass are often used to guide the deconvolution.

To analyse the network and understand what features it learns, the authors of~\cite{SpringenbergDBR14} introduce a variant of the ``deconvolution approach" called Guided Backpropagation. This new method is applicable to a broader range of networks compared to existing techniques and helps in visualising features learned by CNNs. The proposed networks only use convolutional layers with occasional dimensionality reduction achieved by strided convolutions. Fully connected layers are replaced by $1\times 1$ convolutions. The paper shows that max-pooling layers can be effectively replaced with convolutional layers that have increased stride, without compromising the network's accuracy. This leads to an architecture that is more homogeneous and possibly easier to analyse and understand. Since their network architecture lacks pooling layers, their deconvolution approach doesn't need to deal with the typical challenges posed by max-pooling. This simplifies the deconvolution process. The model used deconvolutional layers and guided backpropagation to generate saliency maps. However, choosing to drop or keep max-pooling layers is challenging as it depends on several factors, such as domain area, dataset, and network architecture. 

\subsubsection{Back-propagation Based Methods}
\label{sec:backprop}

Back-propagation based methods calculate the gradient of a specific output with respect to the input . In the simplest case, the gradient can be back-propagated~\cite{SimonyanVZ13}. The method introduced the concept of using gradients to create saliency maps, which visualise the importance of each pixel for the classification decision of a convolutional neural network (see Figure~\ref{Fig:Saliency_Method}). This paper is often cited as introduction of Input$\times$Gradient, as it laid significant groundwork for gradient-based interpretability methods in deep learning. This saliency maps highlight which parts of the image are most important for classifying it into a particular category. The authors show that class saliency maps can be used for object segmentation in images, even with weak supervision (i.e., without detailed annotations for training). This implies that convolutional neural networks trained for classification can also assist in locating objects in images. The paper establishes a relationship between the gradient-based CNN visualisation methods and deconvolutional networks, showing how the former can be considered a generalisation of the latter.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.8\linewidth]{Figures/Gradient.pdf}
	\end{center}
	\caption{Explanations produced by Saliency Method. The photos shown are samples from~\cite{SimonyanVZ13}}
	\label{Fig:Saliency_Method}
\end{figure} 


While DeepLIFT offers a significant advancement in interpreting neural network decisions by back-propagating the contribution of each neuron to the final output, a parallel approach known as Integrated Gradients emerges to further refine the interpretability of deep learning models. Integrated Gradients~\cite{SundararajanTY17}, unlike DeepLIFT, which focuses on comparing the activation of each neuron to a reference input, integrates over a path of inputs between a baseline and the actual input. This method provides a more continuous and comprehensive attribution, capturing the importance of each input feature across a spectrum of changes. This shift from the discrete, reference-based approach of DeepLIFT to the path-integrated framework of Integrated Gradients addresses the issues like vanishing gradients.

Layer-wise Relevance Propagation~\cite{bach2015pixel}, widely referred to as \LRP, is a type of backward propagation that is widely applicable to general network structures~\cite{LapuschkinBMMS16} and has been specifically designed for explanations. The method propagates the prediction backwards using a set of rules that are subject to a \textit{relevance conservation property}, which refers to the equal redistribution of weightings from the network's output to the preceding neurons. Techniques that implement such a \textit{relevance conservation property} can also identify negatively relevant areas from the input. There are many different \LRP\/ propagation rules that one can use at any given step, and depending on the rule used one can extract a different quality explanations. The development of propagation rules and the choice of which rule should be applied at any given situation are research areas in their own right~\cite{MontavonLBSM17}. One type of a \LRP\/ propagation rule used in~\cite{bach2015pixel}, known as $\alpha\beta$-rule, has been proven to work well. However as mentioned, rules that deal with specialised layers (e.g.\ an input layer, pooling layers, normalisation layer, etc.) exist, but they are omitted in this literature review for the sake of brevity (for further propagation rules, see~\cite{MontavonLBSM17}).


Deep Taylor decomposition is a special type of backward propagation technique, where the \LRP\ rule, namely the $\alpha\beta$-rule is applied with parameters $\alpha = 1$ and $\beta = 0$ ( denoted $\alpha_{1}\beta_{0}$). This parameter setting simplifies the propagation rule by only distributing the positive relevance though the network. When applying this rule to a deep ReLU networks (piecewise linear network) for a single layer it is equivalent to computing a Taylor decomposition for that same layer~\cite{MontavonLBSM17}. When applying a simple Taylor decomposition the classification is explained by approximating the deep neural network's function around the instance one is trying to explain using a Taylor series. A Taylor series find an approximation of a non-polynomial function by summing the derivatives of that function. The more the series are expanded the better the approximation is. When computing a simple Taylor decomposition, a Taylor series is used to only compute the first order derivative (i.e a linear approximation) of the function returned by the algorithm around the instance one is trying to explain. However, it is often the case that the higher order terms are not 0. Hence, the approximations provided by a simple Taylor decomposition, which takes only the first-order expansion, often provides an incomplete explanation of the function used to classify the given instance. There is an exception to this, when the ML function is piecewise linear\footnote{the instance lies on the same linear trajectory as 0, which makes the second and higher order terms zero}, however, often this is not the case and the generated explanation may be incorrect or incomplete. Taylor decomposition is then repeated for all layers starting from the last one until the input is reached, which is where deep Taylor decomposition takes it's name from.

RAP~\cite{NamGCWL20} is a development on \LRP~\cite{bach2015pixel} designed to decompose the output predictions of DNNs. It introduces a new perspective by separating positive and negative attributions based on their relative influence between the layers of the network. The technique assigns a bi-polar importance score to each neuron relative to the output, ranging from highly positive to highly negative. This approach allows for a more nuanced understanding of each neuron's influence on the network's decisions. This is in contrast to all other techniques where neurons have either positive or negative influence. The method shows clearer and more attentive visualisations of separated attributions compared to conventional explaining methods (see Figure~\ref{Fig:LRP}).


\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=\linewidth]{Figures/LRP.pdf}
	\end{center}
	\caption{Explanations produced by Gradient, Input$\times$Gradient, Integrated Gradients, Guided Backpropagation, Pattern Attribution, \LRP$\alpha_1\beta_0$ (Deep Taylor decomposition), \LRP$\alpha_2\beta_1$ and RAP. The photos shown are samples from~\cite{NamGCWL20}}
	\label{Fig:LRP}
\end{figure} 



In a line of research undertaken by Lundberg~\textit{et al.}~\cite{LundbergL17} called SHAP (SHapley Additive exPlanations) the authors note a similarity and prove that LIME~\cite{Ribeiro0G16}, DeepLIFT~\cite{ShrikumarGK17}, and Layer-Wise Relevance Propagation~\cite{bach2015pixel} can all be classified under additive feature attribution methods, where an effect is attributed to each feature and the sum of these effects approximates the output of the original model. The authors introduce SHAP values as a unified measure of feature importance. These values are based on the concept of Shapley values from cooperative game theory and provide a consistent and unique solution for additive feature attribution. SHAP values assign each feature an importance value for a particular prediction, which is done by sub-sampling the feature inputs in varying combinations to understand their interactions and effects on the decision. The paper provides theoretical results showing that there is a unique solution within the class of additive feature attribution methods that has a set of desirable properties. These properties include local \emph{accuracy}, \emph{missingness}, and \emph{consistency}. The paper includes experiments to demonstrate the computational efficiency and the alignment of SHAP values with human intuition. 


% The EBAnO engine~\cite{abs-1908-04348} aims to increase the transparency of black-box algorithms, particularly in image processing and classification. It explains the inner workings of these algorithms by analysing the impact of each interpretable input feature on the final outcome. To identify portions of image to be used as interpretable features EBAnO performs a Simultaneous Detection and Segmentation (SDS) analysis~\cite{HariharanAGM14} based on hypercolumns~\cite{HariharanAGM15} and cluster analysis via the K-Means algorithm~\cite{JuangR90}. EBAnO performs iterative input perturbation and classification to analyse the impact of each interpretable feature. The paper presents preliminary results of EBAnO applied to a dataset of 85 images, demonstrating its effectiveness in providing insights into the relationship between interpretable features and the classification made by the CNN model. The EBANO model was assessed using two indices: IR (Image Retention) and IRP (Image Retention Perturbation). The IR index determined the likelihood of a class in the original image relative to the perturbed image. This contrasts with the IRP index, which evaluated how each feature impacted the group of classes. However, setting the initial value of 'k' in the k-means clustering algorithm presents difficulties, especially when dealing with medical images and extensive datasets. This initialisation challenge is crucial because it influences the effectiveness of the clustering process in these complex and large-scale data environments.


\subsection{Discussion: Post-hoc Explanations}
\label{lit:discussion}


The effectiveness of post-hoc explanations plays a crucial role in understanding and interpreting the behaviour of neural networks. These methods are tasked with identifying the significance of different input features in the network's decision-making process. However, the reliance on post-hoc methods for explaining the behaviour of machine learning models based on single data points can be problematic due to the fragility of these explanations~\cite{abs-1806-08049}. Such an approach risks drawing misleading conclusions about the model's overall performance. This issue arises because explanations generated for individual data points may not accurately represent the model's behaviour in a broader context. Furthermore, attempting to comprehend the intricacies of complex models through one or even several point-wise explanations shouldn't be without the support of theoretically sound metrics. 


To ensure the reliability and usefulness of these methods, several key properties such as fidelity, input invariance, handling saturation and sensitivity must be rigorously maintained~\cite{NielsenDRRB22}. Table~\ref{tab:comparison} provides a comparative overview of various explanation methods across these properties, highlighting their respective strengths and weaknesses. Section~\ref{sec:comp} conducts an in-depth analysis of each method, examining how they address these critical aspects.
\newpage


\begin{table}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{cXXXXXXXXX}
\hline
\textbf{Method} & \textbf{Input Invariant} & \textbf{Handles Saturation} & \textbf{Input Sensitive} & \textbf{Fast Generation} & \textbf{Easy Interpretability}\\
\hline
Saliency Maps & no & no & yes  & yes & no\\
Input$\times$Gradient & no & no & yes  & yes & no\\
SmoothGrad & partially & partially & yes  & no & no\\
Guided Backprop. & no & no & yes  & yes & no\\
Grad-CAM & yes & partially & yes & yes & no\\
Integrated Grad. & no & yes & yes  & no & no\\
DeepLIFT & partial & partially & yes  & yes & no\\
LRP & yes & yes & yes  & yes & no\\
\hline
\end{tabularx}
\caption{Comparison of explanation methods across different properties.}
\label{tab:comparison}
\end{table}

At the core of robust post-hoc explanation methods is the principle of \emph{fidelity}~\cite{TomsettHCGP20} of an explainability method, which is is closely tied to its ability to accurately identify and quantify the relevance of different input features. High-fidelity methods are characterised by their precision in assigning significant importance scores to features that critically influence the network's performance. Conversely, features that have a minimal impact on the output should receive low attribution scores. This accuracy in feature relevance assessment is crucial for deciphering the decision-making process within the network. Another critical aspect of attribution methods is \emph{input transformations resilience}~\cite{KindermansHAASDEK19, 11700}. Neural networks often exhibit invariance to modifications such as constant shifts in the input data. Explainability methods in this category, therefore, must also demonstrate this invariance, ensuring that their output remains stable and reliable under such transformations. This stability is essential for understanding the network's behaviour in dynamic environments where input data might vary. In ML models, especially those involving nonlinear activation functions like Sigmoids, input features can cause \emph{saturation} in the network~\cite{SundararajanTY17}. For instance, in a network using a Sigmoid activation function, an input greater than a certain threshold might not alter the network's output, leading to a zero gradient. Explainability methods especially the ones using gradients to produce their explanation must effectively address this saturation phenomenon. One approach is to incorporate reference inputs or baselines, such as zero values, random numbers, or averages calculated from the input dataset. This helps in accurately estimating attributions even in saturated networks. Lastly, the concept of \emph{sensitivity} is pivotal in attribution methods~\cite{AnconaCOG19, 11700}. It requires that the model's output for a given input can be broken down into the sum of contributions from individual input features. In practice, this means that an attribution method should assign non-zero scores to features that uniquely distinguish between two similar inputs. Moreover, features that have no impact on the network's output should be attributed a zero score. This property, also known as \emph{completeness} or \emph{summation to delta}~\cite{SundararajanTY17, ShrikumarGK17}, ensures that the attribution reflects the true influence of each feature in the model's decision process.


The inherent difficulty in validating attribution methods in machine learning stems from the absence of definitive, ground truth explanations for the decisions made by these models. This challenge is central to the field: a comprehensive evaluation of attribution methods ideally requires a complete understanding of the decision-making process of the ML model, which is precisely what these methods aim to solve. Moreover, distinguishing between inaccuracies originating from the ML models themselves and those from the attribution methods further complicates this validation process~\cite{SundararajanTY17}. Since explanations are often tailored to align with human visual perception, the assessments of these attribution methods have predominantly been based on subjective interpretations. This subjective approach, while valuable, is limited in its ability to provide a robust and thorough understanding. The importance of objective evaluation in this context cannot be overstated. Objective measures are crucial for establishing a solid theoretical framework for attribution methods~\cite{yeh2019infidelity}. They enable a more systematic comparison and contrast of different approaches, shedding light on their respective strengths and weaknesses. 

\subsubsection{Methods Comparison}
\label{sec:comp}

The fidelity of an interpretability method is a critical aspect, often considered the most crucial criterion. When an interpretability method lacks fidelity, its interpretations can be misleading or completely incorrect, potentially obscuring the true capabilities and limitations of the model. This can result in false assessments of the model's reliability, effectiveness, or potential biases. For example, a method with low fidelity might misleadingly suggest that the model is focusing on relevant features, whereas in reality, it could be relying on irrelevant or spurious correlations. To quantitatively assess the fidelity of an explainability model, \emph{infidelity metrics} have been proposed. One such metric requires the explanations to capture the function values~\cite{PlumbMT18}, another assess how well an explanation captures the changes in a model's output in response to significant alterations in the input features~\cite{YehHSIR19}. The work by~\cite{YehHSIR19} contends that the technique of perturbing images by randomly removing subsets of pixels could be of limited significance. This stems from the observation that such perturbations often result in minimal information loss, considering that the surrounding, non-removed pixels still retain much of the image's information. Additionally, the approach of considering every possible subset of pixels for removal, as employed in methods like SHAP~\cite{LundbergL17} and LIME~\cite{Ribeiro0G16}, becomes impractical in the context of high-dimensional images due to the vast number of potential combinations.


As part of checking if the model is faithful a ```sanity'' check was introduced. The research conducted by Adebayo~\textit{et al.} proposed two important sanity checks to assess the responsiveness of attribution methods in relation to changes in model parameters and the dataset used~\cite{AdebayoGMGHK18}. These checks are designed to evaluate whether the attribution methods accurately reflect the underlying mechanics of the neural network and its training data. The first sanity check involves replacing the learned neural network parameters with random values. The attribution maps generated from this modified network are then compared with those from the original, unaltered network. The comparison employs various correlation metrics to determine if the attribution maps effectively capture the impact of these changes in network parameters. If the attribution maps do not significantly change despite the drastic alteration in network parameters, this raises questions about their the methods faithfulness. The second sanity check involves using neural networks trained on data with randomly shuffled labels. The objective is to see if the attribution methods produce different attribution maps under these conditions. These sanity checks serve as critical tools for verifying the robustness of attribution methods. They ensure that these methods are not just producing visually appealing or superficially consistent maps, but are genuinely representative of the model's learning and operational dynamics. If an attribution method fails these checks, it suggests that the method may be unreliable, potentially offering misleading insights into how the model is processing its inputs and making predictions. Adebayo~\textit{et al.} found that Guided Backpropagation~\cite{SpringenbergDBR14} and Guided Grad-CAM~\cite{SelvarajuCDVPB17} were insensitive to the learned parameters in the layers near to the output~\cite{AdebayoGMGHK18}. 

\emph{Input invariance} refers to an attribution method's ability to remain unaffected by specific transformations to the input that do not alter the model's predictions~\cite{YehHSIR19}. A good attribution method will exhibit low sensitivity to such changes (\ie produce the same explanations for minor variations in the input). The Saliency method~\cite{SimonyanVZ13} is based on the gradient of the model’s output with respect to the input, it is therefore not input invariant. Input$\times$Gradient~\cite{SimonyanVZ13}, similar to Saliency, is also based on gradients. Since it involves multiplying the gradient by the input it is even less input invariant. Guided Backpropagation modifies standard backpropagation to focus on positive gradients, this does not make it any more input invariant. Integrated Gradients~\cite{SundararajanTY17} calculates attributions by integrating the gradients along a path from a baseline input to the actual input. The method is therefore generally not input invariant. DeepLIFT~\cite{ShrikumarGK17} compares activations to a reference activation and assigns contribution scores accordingly. Its input invariance depends on the choice of the reference input and how it relates to the transformed inputs. SmoothGrad~\cite{SmilkovTKVW17} enhances traditional gradient-based methods by averaging the gradients of multiple noisy versions of the input. This averaging process can potentially reduce sensitivity to small changes in the input, thereby increasing input invariance to some extent. LRP~\cite{bach2015pixel} redistributes the output prediction back through the network layers. Its input invariance largely depends on how it handles different layers and their activations and the rules deployed. Input that changes the activation drastically is however unlikely to not alter the model's predictions, so \LRP\ is seen as mostly input invariant. Grad-CAM~\cite{SelvarajuCDVPB20} uses the gradients flowing into the last convolutional layer to produce a localisation map. Therefore it is input invariant, as insignificant changes do not lead to changes in the gradient in the last layers~\cite{AdebayoGMGHK18}.



\emph{Saturation} is another critical aspect to consider when evaluating interpretability methods in machine learning. It refers to how the attribution method deals with the non-linearities in a network, especially in the context of activation functions like ReLU (Rectified Linear Unit). In simple terms, saturation occurs when changes in the input feature do not affect the output of the network due to the nature of the activation function. The Saliency method~\cite{SimonyanVZ13} is based on the gradient of the output with respect to the input. In cases of saturation, where gradients may become zero or near-zero, the saliency method might face challenges in accurately capturing the influence of input features on the output. Similar to Saliency, Input$\times$Gradient~\cite{SimonyanVZ13} also uses gradients but multiplies them by the input, so it still could struggle in regions of saturation where gradients are minimal. SmoothGrad~\cite{SmilkovTKVW17} enhances saliency maps by averaging the gradients of multiple noisy versions of the input. This approach can reduce the noise in the gradient-based explanations, potentially providing more clarity even in saturated regions. However, its effectiveness in saturation still largely depends on how well the underlying gradient method copes with saturation. Guided Backpropagation~\cite{SpringenbergDBR14} modifies standard backpropagation by only allowing positive gradients to flow backward through the network. While this can create clearer visualisations, it doesn't have an effect on the saturation problem and can similarly to the other methods have this problem in regions where the gradient is close to zero. Grad-CAM~\cite{SelvarajuCDVPB20} uses the gradients of the target concept (like a class label) flowing into the final convolutional layer to produce a coarse localisation map highlighting important regions in the image for predicting the concept. While Grad-CAM can be effective in highlighting relevant areas in the input space, its performance in the context of saturation is more dependent on the behaviour of the final convolutional layers and may vary depending on the network architecture. DeepLIFT~\cite{ShrikumarGK17} compares the activation of each neuron to its 'reference activation' and assigns contribution scores accordingly. The method is specifically designed to address the saturation issue, as it considers the difference from a reference point. Despite this it may still struggle in highly saturated regions. Integrated Gradients~\cite{SundararajanTY17} improves DeepLIFT~\cite{ShrikumarGK17} by integrating the gradient along the path from a baseline to the actual input. This approach helps in capturing the contribution of features even in regions where the activation functions of the network are saturated. Finally, \LRP~\cite{bach2015pixel} redistributes the output prediction value back through the layers of the network, which could potentially handle saturation effectively. It considers the contribution of each neuron relative to others, which helps in understanding the impact of saturated features.


Sensitivity in the context of attribution methods refers to how responsive these methods are to changes in the input features. A sensitive attribution method should ideally change its output significantly if the input features that are important for the model's decision change. Integrated Gradients~\cite{SundararajanTY17}, Saliency~\cite{SimonyanVZ13} Input$\times$Gradient\cite{SimonyanVZ13} and Guided Backpropagation~\cite{SpringenbergDBR14} all compute gradients with respect to the input and therefore directly reflect the immediate impact of input changes on the output. SmoothGrad~\cite{SmilkovTKVW17} averages the gradients of multiple noisy versions of the input and retains sensitivity to input changes but attempts to provide a clearer signal by averaging out the effects of minor, non-critical variations. DeepLIFT~\cite{ShrikumarGK17} compares the activation of each neuron to a reference activation, which can make it sensitive to changes in input, particularly those that significantly alter neuron activations compared to the reference state. \LRP~\cite{bach2015pixel} redistributes the output back through the layers of the network, it is designed to be sensitive to changes in inputs that significantly affect the network’s output. Grad-CAM~\cite{SelvarajuCDVPB20} uses the gradients flowing into the last convolutional layer to produce a localization map. Its sensitivity is more focused on the spatial aspects of the input (like in image data) and is sensitive to changes in features that strongly influence these convolutional layers. The majority of methods are sensitive to the input. However some interpretability methods can lack sensitivity to changes in class labels, when computing gradients. This issue is evident when, for a given input image, similar attribution maps are generated regardless of the class label.


% For instance, methods like Saliency Maps~\cite{SimonyanVZ13}, Input$\times$Gradient~\cite{SimonyanVZ13}, Guided Backpropagation~\cite{SpringenbergDBR14} and Integrated Gradients~\cite{SundararajanTY17} often show this insensitivity, as demonstrated in Figure~\ref{Fig:sensitivity}.


% \begin{figure}[ht!]
% 	\begin{center}
% 		\includegraphics[width=1\linewidth]{Figures/methods_insesitive.png}
% 	\end{center}
% 	\caption{This figure shows the lack of sensitivity of some methods to the class being interpreted. Initially, the input image is showcased in the first column, followed by various attribution maps in the subsequent columns. The top of the first column's image highlights the target class and its soft-max probability. The photos shown are samples from~\cite{NielsenDRRB22}}
% 	\label{Fig:sensitivity}
% \end{figure} 


The discussion in this section has highlighted several crucial aspects of post-hoc explanation methods in neural networks, emphasising their significance in interpreting the behaviour of these complex models. The section delves into the principles of fidelity, input invariance, saturation, and sensitivity, underscoring the importance in ensuring reliable and meaningful explanations. However, despite the progress made in developing and evaluating these explanation methods, several challenges and limitations persist. These challenges not only pertain to the technical aspects of the explanation methods but also to their interpretation and integration into broader machine learning workflows.

\section{Future Directions}

As machine learning models are increasingly used in critical decision-making, ensuring that these models are fair and unbiased becomes paramount. Inherently interpretable models focus primarily on either learning an interpretable version of the model, modifying the existing model to enhance its interpretability or changing the learning process to produce a more interpretable model. This type of approaches offer a compromise, allowing the use of somewhat accurate, complex models while still gaining some level of interpretability. Work by Markus~\textit{et al}~\cite{MarkusKR21} suggests that instead of focusing solely on developing inherently interpretable models, which may have lower predictive performance, post-hoc explanations can be used to provide insights into the workings of an AI model. This allows for the full accuracy and learned complex function of these models to be used.


In order for the post-hoc explanations to be useful they should not only be interpretable, but also have a very high degree of faithfulness. Faithfulness in this context indicates how accurately an explanation reflects the true reasoning process of the model. Higher faithfulness means the explanation is more precise and reliable in representing how the model actually works. Interpretability on the other hand, refers to how well a human can understand the reasons behind a model's decision or prediction. Higher interpretability often means the explanation is more accessible and comprehensible to non-experts. 


Early methods for post-hoc explanations started around 2014 and 2015. They were not very interpretable and lacked fidelity. Saliency~\cite{SimonyanVZ13}, Input$\times$Gradient~\cite{SimonyanVZ13} and Guided Backpropagation~\cite{SpringenbergDBR14} produced noisy explanations which were not easy to comprehend by a human user, while also not being input invariant, having saturation issues and lacking sensitivity to changes in the output being interpreted. 


The next generation of methods are easier for humans to understand but might not accurately represent the model's internal workings. Methods include LIME~\cite{Ribeiro0G16} and SHAP~\cite{LundbergL17}, as well as others~\cite{ElenbergDFK17, Ribeiro0G18}. They choose sub-parts of the input that are already meaningful to a human and then present their importance to the made classification. By grouping individual units (\eg pixels in an image) into a bigger feature (\eg a superpixel) these types of methods select a limited amount of information to show to the user. It has been shown that a human can not comprehend more than three to five meaningful item at once~\cite{cowan2001magical, starkey1995development, morris2018human}, so showing higher level explanations in the way that input perturbing-based techniques do, makes them highly interpretable methods. Input perturbing-based explanations, assign a value of importance to each feature based on whether an alteration of that feature leads to a change in the classification or a lowered classification certainty. These methods do not qualify as input invariant. It also inevitably leads to unfaithful explanations, as the methods assume that if a feature doesn't change the activation of the classification that means that the feature doesn't contribute to the classification, which doesn't hold, as proven by the saturation problem explained earlier. From this it follows that these methods don't detect all the features that lead to a classification. Aside from that the input perturbing methods sometimes assign importance to features that are irrelevant. This occurs whenever the data instance is classified with a low degree of certainty, as it is easier to identify features that when perturbed lead to a different classification. 


The identification of these fidelity issues is a research direction in its own right~\cite{KindermansHAASDEK19, DombrowskiAAAMK19, GhorbaniAZ19} and the preceding methods described are focused on either fixing fidelity problems in the explanation or making the explanation more interpretable (\eg in the case of convolutional neural networks making the heatmaps ``less noisy''). For example, DeepLift~\cite{ShrikumarGK17}, which focuses on comparing the activation of each neuron to a reference input, reduces the saturation problem. It also is less input variant to insignificant changes as long as the neuron chosen for comparison is not the one being varied. The method still shows sensitivity to big changes in the output or in the input. Integrated Gradients~\cite{SundararajanTY17} improve DeepLift~\cite{ShrikumarGK17} by integrating the change over a path of inputs between a baseline and the actual input. Taking more than one reference point makes the algorithm even more resilient to non-linearities in the network. However, it does take a step back in terms of input invariance and sensitivity. SmooothGrad~\cite{SmilkovTKVW17} in comparison, doesn't suffer from sensitivity issues and due to taking the average of perturbed inputs also tends to improve input invariance. However, similar to older approaches it suffers from the saturation problem. GradCAM~\cite{ShrikumarGK17} and GradCAM++~\cite{ChattopadhyaySH18} use the gradients flowing into the last convolutional layer to produce a localisation map. They are input invariant and still sensitive to big changes. Their
performance in the context of saturation is dependent on the behaviour of the final convolutional layers and may vary depending on the network architecture, but it not consistently struggling like other methods. Finally \LRP~\cite{bach2015pixel} works by tracing the output prediction value backward through the network's layers. This approach could effectively manage saturation issues. \LRP\ assesses the contribution of each neuron in relation to others, aiding in the comprehension of the influence exerted by saturated features. It is also structured to be highly responsive to alterations in inputs that have a substantial impact on the network’s output and not have too big of an impact to small input changes. 

Another consideration, when choosing to deploy interpretability methods is computational resources. Gradient-based methods in machine learning are notably efficient in terms of computational resources, Saliency maps~\cite{SimonyanVZ13}, Input$\times$Gradient~\cite{SimonyanVZ13}, Guided Backpropagation~\cite{SpringenbergDBR14}, \LRP~\cite{bach2015pixel} and  Grad-CAM~\cite{SelvarajuCDVPB20} and its successors~\cite{SelvarajuCDVPB20, ChattopadhyaySH18, abs-1908-01224, SmilkovTKVW17} require one forward and one backpropagation step to generate an explanation. This efficiency is particularly important when considering the growing need for explainability in machine learning models. As these models become more complex and are used in critical applications, there is an increasing demand for methods that can provide insights into how decisions are made without significantly adding to computational costs. In contrast, the process of generating an explanation for Integrated Gradients~\cite{SundararajanTY17} and SmoothGrad~\cite{SmilkovTKVW17} requires 50 to 200 steps depending upon the problem domain, dataset, and the scope of explanation. Methods like SHAP~\cite{LundbergL17} and LIME~\cite{Ribeiro0G16} require numerous perturbation as well. Such methods might not be as feasible in scenarios where computational resources are limited or when quick explanations are needed~\cite{GhorbaniAZ19}.


The future direction in machine learning interpretability is arguably moving towards a blend of approaches, where explanations are both highly interpretable like LIME~\cite{bach2015pixel} and SHAP~\cite{LundbergL17} and extremely faithful to the actual workings of the model, similar to \LRP~\cite{bach2015pixel}, while not requiring too much computational resources like gradient based methods~\cite{SimonyanVZ13, SimonyanVZ13, SpringenbergDBR14, bach2015pixel, SelvarajuCDVPB20, SelvarajuCDVPB20, ChattopadhyaySH18, abs-1908-01224, SmilkovTKVW17}. The goal is to create techniques that encompass all key properties discussed so far: fidelity, input invariance, and effective handling of saturation and sensitivity issues. Incorporating these aspects into future interpretability methods will require innovative approaches that combine the best of both worlds:  human-friendly explanations and model-faithful insights. This would lead to more robust, understandable, and trustworthy AI systems, crucial for their successful deployment in critical decision-making domains.


\section{Conclusion}
This literature review has extensively explored the dynamic field of Explainable Artificial Intelligence (XAI), delving into its trajectory and the various methodologies developed to enhance the interpretability of complex neural networks. The review highlighted two primary categories of explainability methods: intrinsically interpretable models (see Section~\ref{sec:Intrinsic}) and post-hoc explanations (see Section~\ref{sec:post-hoc}), each with their unique approaches and implications.

Intrinsically interpretable models, both globally and locally focused, underscore the importance of integrating transparency directly into the AI model's design. This approach is particularly vital in sensitive applications where decisions must be transparent and justifiable. However, the trade-off between interpretability and model performance remains a significant challenge, emphasising the need for continued innovation in designing models that balance these two critical aspects effectively. Post-hoc explanations, encompassing both global and local perspectives, offer an alternative way to interpret complex neural networks. These methods, applied to already-trained models, aim to shed light on the decision-making processes of AI systems without compromising their performance. This approach has proven particularly useful in scenarios where intrinsic transparency is not feasible or where detailed insights into specific predictions are necessary.

The ongoing development and refinement of these explainability methods are crucial in advancing the field of AI, particularly as these models become increasingly integrated into critical domains such as healthcare, finance, and autonomous systems. The trajectory of interpretability in machine learning seems to be converging towards a harmonious integration of methodologies. Future research will aim for explanations that are not only highly interpretable but also true to the model's inner workings. Moreover, this evolution is mindful of minimising computational demands, a challenge often encountered in gradient-based methods. The objective is to create techniques that embody the essential characteristics discussed: fidelity, input invariance, and adeptness in addressing saturation and sensitivity complexities. This demands innovative strategies that combine easily comprehensible explanations with faithful reflections of the model’s processes.

While interpretability is important across various data domains, the urgency and complexity of this need vary. In natural language processing, textual data inherently possess a granular and interpretable structure—words, phrases, and sentences carry explicit meanings. Techniques like topic modeling or sentiment analysis often provide straightforward insights without requiring extensive additional methods.

Similarly, audio data, especially in controlled environments, tends to be less complex. Key features such as pitch, tone, and rhythm are more directly interpretable, and the dimensionality of audio data is generally lower. This makes understanding and interpreting models in this domain more manageable without necessitating advanced clustering techniques.

In contrast, image data presents unique challenges due to its high dimensionality. A single image comprises of three dimensions and thousands to millions of pixels, each contributing subtly to the overall picture, making it difficult to discern which features are most influential in a model's decision. Critical applications like medical diagnostics, autonomous driving, and surveillance rely heavily on computer vision systems. Misinterpretations in these domains can have profound consequences.

In this thesis, the aim is to harmonise faithfulness and interpretability in the context of image data. Instead of offering importance values for each pixel or distilling outputs into binary judgements, we propose ways to cluster related features together, thereby reducing dimensionality and enhancing interpretability. Chapter ~\ref{chap:clustering} will focus on clustering techniques to group similar features in image data, providing a foundation for assigning a single importance scores to each cluster in Chapter~\ref{chapter:revLRP} and~\ref{chapter:REVEAL}.


