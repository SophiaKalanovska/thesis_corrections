\chapter{Conclusion and Future Work}
\label{chapter:conclusion}
\section{Implications of the Research}
In this thesis, a significant gap in the field of explainability of deep neural networks is investigated: the need for interpretability methods that increases the ease of interpretability, while preserving faithfulness. This thesis proposes a methodology for finding complex input features (see Chapter~\ref{chap:clustering}) and two different approaches for assigning a single value of relevance to them (see Chapter~\ref{chapter:revLRP} and Chapter~\ref{chapter:REVEAL}). This clustering reduces the cognitive load typically associated with interpreting the vast array of features that DNNs find relevant. Further, this thesis goes beyond traditional binary interpretations of complex input features (`important' or `not important'), which, while easy to understand, often lack fidelity to the actual workings of the model due to the complex non-linear function learned by the model. Instead, the new framework provides a single importance score for each feature cluster, which offers higher fidelity to the model's predictions and allows for a \emph{ranking} of the importance of feature clusters, which adds another layer of interpretability. This approach presents a manageable volume of information, allowing for an intuitive understanding without losing the nuanced detail of each feature's contribution to the model's decision-making process.

The implications of the proposed framework and the methods that implement it amount to a theoretical and empirical contribution. Reverse relevance distribution tracing in Chapter~\ref{chapter:revLRP}, while computationally intensive, offers a solution to practically attribute relevance to specific features. Further, forward pass retracing through contribution to classification (\CTC\/) emerges as a more efficient and scalable alternative, particularly suitable for larger and more complex networks. The findings from the evaluation of forward pass retracing performed on the isolated complex features reveal that the method's fidelity is not compromised despite the simplification of the explanation. 

\section{Discussion with Respect to the Wider Field}
Chapter~\ref{chap:lit} describes relevant literature in the field of explainable AI. Relevant methods and techniques were divided into a number of categories. \CTC\/ falls into the categories of \textit{post-hoc} and \textit{propagation-based} explainable AI techniques.  However, the framework encompassing heatmap clustering and \CTC\/ described in this thesis extends beyond traditional propagation-based techniques. Clear parallels can be drawn to other methods, which are discussed below.

Conceptually, the overarching framework described could be seen as a post-hoc attention-based technique. Attention-based techniques typically enforce that a model learns to focus on specific parts of the input. Rather than enforcing attention during learning, \CTC\/ enforces that the explanation focuses on specific parts of the input during explanation generation. In particular, Xaio et al. \cite{XiaoXYZPZ15} introduce a framework which uses a combination of attention on certain `patches' of the input, derived from a bottom up proposal, and then uses top-down filtering (i.e. introducing some prior knowledge) in order to make a classification. The resulting classifier uses clear features to make a decision making interpretable. This technique however relies on application during training, and cannot be applied post-training to any generic CNN. Despite this difference, there are clear parallels here to \CTC\/. In essence, one could interpret \CTC\/ as a post-hoc counterpart to \cite{XiaoXYZPZ15}'s framework, where the initial heatmap generation proposes candidate patches (the bottom-up approach), and the use of SAM or clustering replaces the presence of top-down/prior knowledge to select meaningful features. Moving beyond conceptual similarities to \cite{XiaoXYZPZ15}'s work, it seems fair to refer to \CTC\/ as an attention-based explanation, as it fundamentally draws attention to distinct features, which are selected by the clustering mechanism --- relying less on any anthropomorphic interpretation of features of traditional pixel-wise explanations provided by propagation-based techniques.


Unlike many other fields of AI research, explainable AI is still in its infancy, and as such questions over methodology and evaluation are open. This is primarily driven by the lack of an objective measure of ground-truth. Fidelity refers to how accurate an explanation is at illustrating the decision-making process used by the model. It is noted repeatedly throughout the literature that it is a desired quality, and yet there is no way to directly measure fidelity.  In practice, it is evaluated by adding noise to the input as in Section~\ref{sec:results} of the \CTC\/ chapter or by perturbation-based methods, which remove part of the input and monitor the resulting change in the output. Yeh~\textit{et. al.}~\cite{YehHSIR19} note that simple removal of pixels is not a reliable measurement of fidelity, as omitting parts of the input is not an accurate measurement of the importance of features. To understand the contribution of a small part of the input, propagation rules that are true to the models functioning during the original instance inference are needed. The rules defined in Chapter~\ref{chapter:REVEAL} propose a more faithful way in which this can be achieved than simply performing inference on the modified input. Ultimately, there is a need for novel metrics to accurately assess fidelity and research into forward propagation techniques is likely to provide an alternative fidelity measure. 

Ribiero's LIME \cite{Ribeiro0G16} builds a local surrogate model, which is trained based on the removal of parts from the input and monitoring the resulting change in the output. This model can then be queried for the importance of complex input features. This surrogate model demonstrates a loss of fidelity, as discussed by~\cite{YehHSIR19}. However, LIME similar to the method combining identification of complex input features and \CTC\/ described in this thesis --- indicates a relevance score by region or segment, as opposed to pixel-level. Indeed, \cite{Ribeiro0G16} reports studies with human subjects including both graduate ML students and non-technical subjects from Mechanical Turk, which demonstrate that explanations of this sort are preferred by users, and are able to lead to better decision-making in model-selection, feature-engineering and identifying irregularities in model-classification. Given that the explanations of the method, combining identification of complex input features and \CTC\/, are functionally equivalent to LIME from the user's perspective, it stands to reason that \CTC\/ would also be preferred by users, while also offering a more computationally efficient, and crucially more faithful explanation.

Computational cost is a considerable factor in any explanation method. While \CTC\/ employs only a single forward to generate explanations, the majority of computational complexity and time is spent in computing the feature masks. In terms of overall time required for an explanation, this renders \CTC\/ as fast as most gradient based methods~\cite{SimonyanVZ13, SimonyanVZ13, SpringenbergDBR14, bach2015pixel, SelvarajuCDVPB20, SelvarajuCDVPB20, ChattopadhyaySH18, abs-1908-01224, SmilkovTKVW17}. Even by taking the feature masks generation into account the computational time required is still far less than Integrated Gradients~\cite{SundararajanTY17} and SmoothGrad~\cite{SmilkovTKVW17}'s, which require 50 to 200 steps to compute an explanation or methods like SHAP~\cite{LundbergL17} and LIME~\cite{Ribeiro0G16}, which require numerous perturbations. It is pertinent to note that the \CTC\/ implementation at present is not optimised. The author expects an optimised implementation to be capable of achieving a speed-up of 10--20 times faster than the current implementation and a speed-up of up to 52 times faster than the current implementation of the clustering. While this is likely to still fall behind some methods with respect to computational cost, this is likely to further increase the usable capacity of \CTC\/ with feature selection.



\begin{table}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{cXXXXXXXXX}
\hline
\textbf{Method} & \textbf{Input Invariant} & \textbf{Handles Saturation} & \textbf{Input Sensitive} & \textbf{Fast Generation} & \textbf{Easy Interpretability}\\
\hline
Saliency Maps & no & no & yes  & yes & no\\
Input$\times$Gradient & no & no & yes  & yes & no\\
SmoothGrad & partially & partially & yes  & no & no\\
Guided Backprop. & no & no & yes  & yes & no\\
Grad-CAM & yes & partially & yes & yes & no\\
Integrated Grad. & no & yes & yes  & no & no\\
DeepLIFT & partial & partially & yes  & yes & no\\
LRP & yes & yes & yes  & yes & no\\
Relevance Tracing & yes & -- & yes  & no & yes\\
\CTC\/ & yes & yes & yes &  yes & yes\\

\hline
\end{tabularx}
\caption{Comparison of explanation methods across different properties.}
\label{tab:comparison_2}
\end{table}

Table~\ref{tab:comparison_2} highlights the key desiderata outlined in the literature review, with a focus on Relevance Distribution Tracing and \CTC\/ methods. Both \CTC\/ and Relevance Tracing produce interpretable explanations, but they differ significantly in practicality. While Relevance Tracing is computationally demanding to the point of being impractical for many applications, \CTC\/ achieves comparable interpretability with computational efficiency akin to other distribution-based methods. 

Faithfulness is evaluated through properties such as input invariance, saturation handling, and input sensitivity. Both \CTC\/ and Relevance Tracing are input invariant and sensitive, aligning with the high standards of faithfulness. However, as Relevance Tracing effectively reverses the LRP process, it inherits LRP's strengths and limitations. As shown in Section~\ref{sec:results}, LRP and \CTC\/ consistently demonstrate high sensitivity across various noise types. They reliably distinguish between scenarios where classification changes occur and those where they do not. LRP is particularly reactive to Gaussian blur due to its dependence on edge features, while \CTC\/ balances global and local feature sensitivity, making it robust against both random noise (Gaussian and Uniform) and structural changes (blur).

All methods in Table~\ref{tab:comparison_2} are marked as input-sensitive, as they exhibit statistically significant responses to large noise levels compared to minor perturbations. However, CTC and LRP stand out with the most pronounced differences in explanation quality between cases of output changes and minor input variations. Saturation handling, while not explicitly tested, is another key criterion. Since \CTC\/ avoids gradients in its computations, it is inherently resistant to saturation issues. In contrast, Relevance Tracing, which relies on Jacobians, may be susceptible to this limitation. These distinctions further underscore the versatility and effectiveness of \CTC\/ for faithful and computationally efficient explanations.

A significant limitation of any approach that presents relevances as a group of features—whether through heatmaps or other representations—is their inability to address high-level concepts or reasoning. While these methods effectively highlight areas of importance in the input, they do not inherently capture abstract relationships or broader contextual understanding. For instance, in medical diagnosis, distinct features like certain biomarkers or imaging characteristics may be associated with multiple outcomes, and a diagnosis might still be accurate even when none of the identified features is explicitly present in the input. This limitation underscores a gap in current approaches, which often focus on feature-level relevance without delving into the reasoning processes that underpin complex decision-making.

To bridge this gap, methods that construct relationships on top of feature relevances—such as causal models or graph-based reasoning—can be employed~\cite{abs-1911-10500, heinzedeml2017causalstructurelearning, DBLP:journals/corr/abs-1904-12584}. These methods can encode and interpret dependencies between features and outcomes, potentially addressing situations where features are shared across outcomes or where outcomes arise independently of specific features. In Chapters 6 and 7 of this thesis, concrete results demonstrate the utility of feature clustering and post-hoc interpretability frameworks in addressing certain complexities, providing a foundation for extending the capabilities of current methods. These contributions are directly relevant to promising areas of future work, particularly in addressing contemporary concerns regarding the reasoning capabilities of large language models (LLMs). The increasing deployment of LLMs in domains requiring high-level reasoning and contextual understanding amplifies the need for methods that go beyond feature-based explanations to include reasoning frameworks.

\section{Future Work}
The advancements made through the unified framework and the techniques of feature isolation, reverse relevance distribution tracing, and forward pass retracing, open several options for future research. In this section a number of possible research avenues are discussed with motivations and possible limitations for each.

\subsection{Feature Isolation}
The feature isolation approach defined in Chapter~\ref{chap:clustering} integrates object detection with advanced heatmap-based clustering to isolate and analyse features in neural network models. This methodology holds significant potential for simplifying the explanations of different relevance methods. Even without the integration of the proposed methods for assigning a single value of contribution to the features identified, merely taking the sum of the relevances withing the feature can provide a more easily comprehensible explanations. There are several promising directions for further research in this area.

One key area is the exploration of how various types of attribution methods can be effectively combined with object detection techniques. Different types of attribution methods have a different spread of relevances, different mean and median of intensity, some have only positive relevances, while some have negative relevances too. By clustering the heatmaps generated from different attribution methods, there is an opportunity to experiment with different clustering parameters such as the distance metric or the threshold for cluster formation. In the presence of negative contributions one could split the clustering in two, where one part clusters the negative contributions, while another clusters the positives, or take the absolute value and only consider the intensity of the relevance. Patterns and relationships within the data that might be detected by the relevance propagation methods may remain hidden, if not analysed in detail after the explanation is generated. 

Additionally, investigating different clustering approaches other than DBSCAN as used in Chapter~\ref{chap:clustering} can offer further advancements in this field. Techniques like hierarchical clustering, density-based clustering, or graph-based clustering could provide alternative perspectives on the data. It is important to consider the input type of the data when choosing the clustering mechanism. When dealing with images as input, the number of clusters is unknown and the shapes of the clusters are not spherical. Therefore clustering techniques that have a centroid point and a predefined number of clusters are not appropriate. However, if the task is better defined, such as separating background noise from not background noise, a clustering technique that has a predefined number of classes might be more appropriate. In some cases isolated features may be part of a bigger feature, if the extraction of such relationships is important, the use of hierarchical clustering could provide this extra information, allowing the inspection of features at different levels of the clustering process. Each of these methods has unique strengths and can be particularly suited to different types of input data, different applications and different neural networks.

\subsection{Reverse Relevance Distribution}
\label{dis:jac}
The proposed reverse relevance distribution tracing in Chapter~\ref{chapter:revLRP} could significantly contribute to the interpretability of neural networks. The core issue with the method lies in its intensive computational requirements. This method demands a significant amount of memory and processing power due to its reliance on computing Jacobian matrices. This computation is both memory-intensive and time-consuming, posing a barrier to its application in larger, more complex networks and applications where an explanation on the fly is needed. To address these challenges, it is worth exploring innovative approaches that maintain the depth and fidelity of interpretations while reducing computational overhead. One such approach is to develop techniques that bypass the need for Jacobian calculations, aiding in the reduction of the memory load. This could involve leveraging alternative mathematical frameworks or simplifying the relevance tracing process while retaining its interpretative power.

% One promising avenue to explore is the use of approximation methods for computing Jacobians. Finite-difference methods are a class of technique that transform ordinary differential equations (ODEs) or partial differential equations (PDEs), which might be nonlinear, into a set of linear equations. This conversion enables the use of matrix algebra methods for solving these equations. With the power of modern computers, executing these linear algebra operations is highly efficient, therefore making the approximate Jacobian computation possible. It is important to consider the accuracy and reliability when deploying this solution for finding the relevance of a feature. Finite-difference methods are subject to two primary types of errors: round-off error and truncation error, also known as discretisation error. The round-off error occurs due to the inherent limitations of computers in representing decimal numbers. Computers can only store a finite number of digits, and as a result, they often round these numbers to fit their storage capabilities. This rounding can lead to small inaccuracies in calculations, known as round-off errors. These errors accumulate over successive computational steps (often the case in deep neural networks), sometimes significantly impacting the final result. The truncation error arises from the method itself. Finite difference methods approximate continuous differential equations by discretising them – breaking them down into a finite number of intervals or steps. The exact solution of the original differential equation and the solution obtained through this discretised approximation will not perfectly align. This deviation is called the truncation or discretisation error. It's the difference between the true, continuous solution and the solution as calculated using the finite difference method, assuming that there are no round-off errors. The size of this error depends on how the differential equation is discretised and the size of the steps taken in the approximation.


One promising avenue to explore is the use of sparse approximation, which seeks to represent the signal with a minimal number of active components or features, which can reduce the computational complexity. This often involves approximating the original data with a combination of a small number of basis elements. Low-rank matrix estimation~\cite{1102314} provides another alternative, which aims is to find a matrix that is close to the original matrix in terms of some distance measure, but with a lower number of linearly independent rows or columns (\ie lower rank). Parallel processing techniques also offer a viable solution. By distributing the computation across multiple processors or using specialised hardware, the process can be expedited. These methods, while potentially sacrificing some accuracy, can dramatically lessen the computational burden. 


However, the memory requirement is still prominent with these solutions. Memory-efficient data structures, such as boom filters, which test whether an element is a member of a set are highly space-efficient, but are probabilistic and can have false positives. Sparse arrays are a common memory-efficient data structure that already has implementations in TensorFlow~\cite{tensorflow2015} and PyTorch~\cite{NEURIPS2019_9015}. In such arrays only non-zero elements are stored. This can save significant memory particularly for the layers closer to the input, as only part of the input's relevance is being reversed and therefore all parts that are not part of the feature are zero. Segmenting tensors into subparts and processing each in isolation could be a divide-and-conquer approach for memory efficiency. Each segment's relevance can be computed independently, reducing the overall memory requirements. As relevance is distributed layer by layer each segment of a layer would need to be completed before the next layer is started, which is likely to create a computational overhead.
This strategy has not been proposed before and may be difficult to implement. It is worth mentioning that the integration of emerging hardware technologies designed for high-intensity computing tasks could make reverse relevance distribution tracing a feasible method of interpretability. New processors and computing architectures, specifically tailored for AI and machine learning applications, could provide the necessary power to run these intensive computations more efficiently without any change in the implementation of the reverse relevance propagation method.

\subsection{Forward Pass Retracing}
\label{futurectc}
Chapter~\ref{chapter:REVEAL}'s novel forward pass retracing method proposes a set of rules for finding a single value of relevance to a complex input feature. These rules mimic the behaviour of each layer's function during the forward pass given the entire input. As the rules propagate a smaller portion of the input than the original activations, the rules are designed to preserve the signal during the propagation. This is particularly challenging for layers with learned parameters. The scaling of learned parameter was the biggest challenge encountered during this research. Given that learned parameters do not change after training, scaling them changes the behaviour of the network, but is a necessary step in preserving the signal of the feature being propagated. Other methods for preserving the signal of the contributions through layers may be necessary. It is worth exploring scaling the learned parameters differently across layers. An example of this can be seen by \LRP~\cite{bach2015pixel}, which proposed a composite strategy~\cite{SamekBLM17} where different rules are used at parts of the network. This is very likely to be a good research direction for improving the contribution to classification (\CTC\/) value found for a complex feature. Contributions are sparse in the layers close to the input, as all other relevances that are not part of the complex feature are zeroed, so scaling the learned parameters more heavily at these layers may reduce the issues of preserving the signal of the contributions. Layers closer to the output, due to the presence of dense and convolutional layers, have contributions distributed in a uniform way, so such layers may need less parameter scaling. Another direction worth investigating is the scaling of contributions rather than learned parameter values. This can also have a different strength across different parts of the network. The alpha beta rules proposed by \LRP\/ present another way to scale contributions. It can be a beneficial strategy if one is more interested in the positive than the negative contributions. The novel rules proposed in Chapter~\ref{chapter:REVEAL} are the ``basic rules'', which can suit as the foundation for different types of contribution scaling.


The research in this thesis primarily focuses on convolutional neural networks (CNNs) like VGG16~\cite{SimonyanZ14a}, VGG19~\cite{SimonyanZ14a}, and InceptionV3~\cite{szegedy2015rethinking}. These models have been extensively studied in this thesis due to their widespread use in image processing and computer vision tasks. However, the landscape of deep learning encompasses a much broader array of architectures, each with unique characteristics and applications. Extending beyond CNNs to gain a comprehensive understanding of the performance of the method across various models is a natural extension of the research proposed in this thesis. One promising direction is the exploration of interpretability in recurrent neural networks (RNNs). RNNs, known for their effectiveness in handling sequential data like text and time series~\cite{10.1162/neco.1997.9.8.1735}, pose a fundamentally different architecture from CNNs. The temporal aspect of RNNs makes the signal of the relevance harder to trace through the network. Issues of expanding and vanishing contributions are likely and further investigation as to how they can be managed in recurrent architectures is needed. Adapting to RNNs could provide valuable test for the interpretability offered by forward pass tracing method and the robustness of the proposed solution for 
preserving the contribution signal through the network.


Similarly, transformer models, which have revolutionised the field of natural language processing~\cite{rothman2021transformers, chernyavskiy2021transformers} and have also been applied to vision (ViTs)~\cite{dosovitskiy2021imageworth16x16words}, present another avenue for extending the forward pass retracing method. Transformers, particularly known for their self-attention mechanisms~\cite{VaswaniSPUJGKP17}, require a different approach to trace and understand the contribution of input features. Currently, methods for calculating the contribution of complex features in such layers are nonexistent. Therefore, a significant future contribution is the development and refining of rules for these specialised layers. Transformers are the largest models currently being deployed~\cite{VaswaniSPUJGKP17}, so the issues described of signal preservation and learned parameter scaling are a significant part of the difficulty in integrating forward pass retracing for such architectures. Further, the feedback loop for the development of rules for the specialised layers in these architectures would be slow and the computational resources needed for such experimentation would be large. The development of the forward pass retracing as a method for interpreting transformers would involve not only devising new rules but also validating their efficacy in providing faithful and interpretable explanations. 

Generative adversarial networks (GANs) also offer a unique landscape for interpretability research. As models that learn to generate data that is indistinguishable from real data, understanding the decision-making process in GANs could provide insights into their generative capabilities and limitations. This is particularly relevant in applications like image generation, where understanding the contribution of different features to the generated output can be of significant interest. However, applying the forward pass tracing method to these architectures is not straightforward. Each type of neural network layer, be it in RNNs, transformers, or GANs, requires specialised rules to correctly evaluate how complex input features contribute to the classification or output generation. While common layers like dense, convolutional, max pooling, average pooling, and batch normalisation have been explored in this thesis (as detailed in Chapter~\ref{chapter:REVEAL}), layers unique to RNNs and transformers pose new challenges. 

Recent advances in deep learning architectures such as 
Diffusion models~\cite{ho2020denoisingdiffusionprobabilisticmodels}, which have gained prominence in generative tasks, also represent a compelling direction for extending the forward pass tracing method. These models iteratively refine noise to generate high-quality data, such as images or audio. Understanding the interpretability of these iterative processes is critical for assessing how input noise and intermediate steps contribute to the final output. Diffusion models operate with unique mechanisms, such as noise addition and de-noising, requiring novel interpretability rules for these operations. The iterative nature of these models introduces additional complexity, as the contribution of each iteration to the final result must be traced and contextualised. 

\subsection{Evaluation}
\label{evl}
Chapter~\ref{sec:results} evaluates the forward pass retracing both qualitatively and quantitatively. It is worth exploring forward pass retracing on a wider range of networks and analysing the performance of the method across them. While the qualitative evaluation offers valuable insights into the interpretability of the method, a more thorough quantitative analysis is needed. This should involve extending the evaluation to additional networks beyond VGG16~\cite{SimonyanZ14a}. A critical aspect of this quantitative analysis is to examine the input invariance and sensitivity of the forward pass retracing method on different network architectures. Understanding how the method responds to varying inputs and its sensitivity to changes can provide crucial information about its reliability and consistency.

An interesting area of exploration is the method's response to different types and intensities of noise in the input. In the current evaluation, we assessed the method's performance with two levels of noise intensity: a lower level representing a minor perturbation and a higher level indicative of substantial noise. Further research could involve a more nuanced analysis of these noise levels. By calculating and comparing the differences in explanations under varying noise intensities, we can gain insights into the threshold at which noise significantly impacts the explanations. This analysis can be crucial for understanding the robustness of the method and determining its reliability under different conditions. Moreover, it is important to evaluate how the significance of noise varies across different network architectures. The point at which noise begins to substantially affect the explanation might differ from one network to another, depending on factors like network depth, complexity, and the type of data it is trained on. 

\subsection{Interpretability and Usability}
\label{study}
This thesis is dedicated to enhancing the interpretability of explanations provided by deep neural networks. It draws on research underscoring human cognitive limitations, notably the challenge in extracting meaningful insights when confronted with more than five significant items simultaneously, as highlighted in seminal works by Cowan~\textit{et al.}~\cite{cowan2001magical}, Starkey~\textit{et al.}~\cite{starkey1995development}, and Morris~\textit{et al.}~\cite{morris2018human}. Addressing these cognitive constraints is central to the thesis. To empirically validate the effectiveness of the proposed interpretability method, a structured user study is proposed here as a direction for future work. This study intricately examines how different interpretability methods influence user comprehension and decision-making. Participants, selected to represent a diverse range of familiarity with neural networks, will be presented with a series of explanations generated by various interpretability methods.

Each participant will engage in a task where they are required to rank the importance of different areas of the input data, based on the explanation they are provided. This task is designed to be intuitive yet comprehensive, ensuring that participants of varying expertise levels can understand and effectively engage with the material. To facilitate this, a brief training session and clear instructions will be provided before the commencement of the study. The core assessment metric in this study is the consistency of the areas chosen by the participants as being of biggest importance to the network as identifyed by the interpretability method. This consistency will be quantitatively evaluated, with statistical methods applied to analyse the rankings across different participants. The interpretability method that yields the highest consistency in the ranking among participants will be considered the most effective. Furthermore, the study will include a qualitative component where participants can provide feedback on each explanation method. This feedback will be instrumental in understanding the subjective aspects of interpretability, such as clarity, relevance, and overall user satisfaction. 


To enhance the usability and accessibility of neural network interpretability methods for a broader audience, including those without deep technical expertise, the development of intuitive, user-friendly interfaces and visualisation tools is essential. The goal is to demystify the complex internal workings of neural networks and make them more transparent and understandable to non-experts. Future work in this area could focus on designing interactive platforms which allow users to select specific parts of the input data they are curious about, and then find out how much this part contributes to the model's output by employing the methods described in the thesis. For instance, in an image recognition task, a user could select a particular object in the image and receive a visualisation showing how that object influenced the model's classification decision. Such tools would not only make it easier for users to grasp the model's reasoning but also promote a more hands-on approach to explainability.



\section{Final Thoughts}
As we navigate the rapidly evolving realm of artificial intelligence, the importance of interpretability in AI systems cannot be overstated. The development of more powerful computers and the explosion of data available for analysis allowed AI to grow in sophistication and practicality.

As AI systems become more advanced and autonomous, the implications of their operations grow in complexity and importance. The research in this thesis is aligned with the global movement towards explainable AI, advocating for systems that are not just efficient and powerful, but also transparent and fair. As AI continues to evolve and become an ever-growing part of our lives, the work done in this thesis contributes to a future, where AI is not an inscrutable black box, but a transparent, understandable, and trustworthy tool used to further progress and innovation.