\chapter{Conclusion and Future Work}
\label{chapter:conclusion}
\section{Implications of the Research}
In this thesis, a significant gap in the field of explainability of deep neural networks is investigated: the need for interpretability methods that provide deep insights into model operations while remaining accessible and easy to understand. The unified framework proposed in Chapter~\ref{chap:framework} tackles this by introducing a methodology for clustering related features and assigning a single value of relevance to them. This clustering reduces the cognitive load typically associated with interpreting the vast array of features that DNNs find relevant. Further, this thesis goes beyond traditional binary interpretations of complex input features (`important' or `not important'), which, while easy to understand, often lack fidelity to the actual workings of the model due to the complex non-linear function learned by the model. Instead, the new framework provides a single importance score for each feature cluster, which offers higher fidelity to the model's intricate decision-making processes and allows for a \emph{ranking} of the importance of feature clusters, which adds another layer of interpretability. This approach presents a manageable volume of information, allowing for an intuitive understanding without losing the nuanced detail of each feature's contribution to the model's decision-making process.

The implications of the proposed framework and the methods that implement it amount to a theoretical and empirical contribution. Reverse relevance distribution tracing, while computationally intensive, offers a solution to practically attribute relevance to specific features. Further, forward pass retracing emerges as a more efficient and scalable alternative, particularly suitable for larger and more complex networks. The findings from the evaluation of forward pass retracing performed on the isolated complex features reveal that the method's fidelity is not compromised despite the simplification of the explanation. 

\section{Research Output with Respect to Research Objectives}
In Chapter~\ref{chap:framework}, a set of desiderata for clustering methods are outlined, including \emph{relevance}, \emph{scalability}, \emph{robustness}, \emph{interpretability}, \emph{compactness and separation}, \emph{efficiency}, and \emph{consistency}. The clustering approach presented in Chapter~\ref{chap:clustering} introduces a method for identifying input features in images, uniquely combining heatmap clustering with object identification. This integration allows for the selection of the most relevant features as determined by a relevance attribution algorithm, ensuring that the clusters formed are both \emph{relevant} and \emph{interpretable}. The proposed method also exhibits \emph{robustness}, though further testing is required to assess its full capacity to withstand noise. In terms of \emph{scalability}, various parameters for point distance in the DBSCAN algorithm are explored until achieving a desired cluster size and outlier ratio. This process is \emph{efficient}: clustering 4857 points with seven distance parameters on a Quad-Core Intel Core i5 processor takes only 46 seconds. However, incorporating the SAM method for entire image analysis slightly increases processing time. For example, processing 299$\times$299 images (as required by InceptionV3~\cite{szegedy2015rethinking}) takes 178.802 seconds (or 2.98 minutes), while 224$\times$224 images (for models like VGG16~\cite{SimonyanZ14a}, VGG19~\cite{SimonyanZ14a}, ResNet50~\cite{he2015deep}, and DenseNet121~\cite{huang2018densely}) requires 100.351 seconds (or 1.67 minutes). A noteworthy advancement is FastSAM~\cite{abs-2306-12156}, which promises a 52-fold speed increase over the current implementation, though its integration remains a task for future work. Notably, the method \emph{scales} linearly with the number of input features, often exceeding 50,176 for most networks, yet still retains relative \emph{efficiency} for practical applications. \emph{Compactness and separation} is achieved through DBSCAN's criteria for cluster formation, where a minimum number of points must be within a specified distance from a point to be considered part of a cluster. This criterion ensures dense, compact clusters, with non-conforming points treated as noise or outliers, thereby maintaining clear data separation. Lastly, the proposed clustering method is deterministic, ensuring \emph{consistency} by producing the same results when run multiple times on the same input.

The method through which an importance value is assigned to a complex input feature has a different set of desiderata as defined in Chapter~\ref{chap:framework}, including \emph{faithfulness}, \emph{consistency}, \emph{granularity}, \emph{interdependence awareness}, \emph{robustness to network architecture}, \emph{bias and distortion mitigation}, \emph{transparency} and \emph{scalability}. The first proposed method for assigning importance values is by reverse relevance distribution tracing. This method inverts the function of the underlying relevance distribution method, specifically focusing on reverse-engineering the Layer-wise Relevance Propagation (LRP) method~\cite{bach2015pixel} as detailed in Chapter~\ref{rev_LRP}. \LRP\ adheres to vital fidelity criteria such as input invariance, sensitivity, and saturation problem~\cite{NielsenDRRB22}, which the reverse relevance distribution tracing method utilises to assign a \emph{faithful} single value. This value represents the combined importance of input features that comprise the complex feature, in relation to the network's output. Both \LRP\ and reverse relevance distribution tracing employ deterministic rules for relevance assignment to neurons. Hence, ensuring \emph{consistency} and reproducibility of results. However, it's important to note that limited information transfer through the network might introduce small division errors, potentially leading to inconsistencies. These can be addressed through quantisation techniques. The \emph{granularity} of the proposed method comes from its layer-by-layer reversal of LRP's relevance, allowing for a detailed examination of a complex feature's importance, not just for the output classification but also for any specific neuron or layer. Additionally, the method demonstrates \emph{robustness} to various network architectures. It uses a Jacobian matrix to evaluate the relevance distributed from each neuron in one layer to each neuron in the preceding layer, ensuring no adaptation for new architectures is needed. One area yet to be fully explored is \emph{bias and distortion mitigation}, which is needed to understand how outliers, extreme values, and the size of the complex input feature affect the relevance assigned. The rules used for importance value assigned are clear and provide \emph{transparency} and easily interpretability as to how the value was derived. However, despite meeting most desiderata, this method faces significant \emph{scalability challenges}. The computation of Jacobian matrices is memory-intensive and time-consuming, which limits its application in larger, more complex networks, especially in scenarios requiring real-time explanations. Overcoming this limitation is a critical area for future research, as discussed in Chapter~\ref{dis:jac}.


The \CTC\ method proposed in Chapter~\ref{chapter:REVEAL} provides a much more computationally efficient and \emph{scalable} technique for assigning a single relevance score to a complex input feature. It only requires a forward pass through the network, where the functions at each layer are modified to mimic the behaviour of each layer’s function during the forward pass given the entire input. The faithfulness of the method is tested using sensitivity and input invariance metrics. Both of these qualities were assessed by introducing noise to the input and observing the resultant variations in the explanations provided. An ideal interpretability method should exhibit low sensitivity to minor noise perturbations, thereby demonstrating input invariance, particularly under the assumption of a robust network. Conversely, a substantial introduction of noise should lead to a noticeable
change in explanation. The results show that not only is the \CTC\ method \emph{faithful}, but that it observes a higher degree of input invariance than any other method in the literature, while still maintaining sensitivity. The rules to propagate relevance at each layer are deterministic and therefore the \CTC\ method allows for easy reproducibility and \emph{consistency}. Further the relevance is again distributed layer by layer allowing the contribution of a complex feature to be examined not only for the output classification, but also for any neuron or layer of interest. There has been a particular focus in making the \CTC\ method \emph{interdependence aware}, where specific rules are defined (see Section~\ref{sec:scale}) to deal with the scaling of learned parameters, so that no overemphasis, underrepresentation, or dilution of individual feature's importance is introduced through the propagation of contributions. The method is \emph{robust to network architecture} and results can be seen for the most used CNNs in Chapter~\ref{chapter:results}. For other types of architectures new rules that allow for the propagation of relevance need to be defined, which is discussed in future work Section~\ref{futurectc}. Similar to the reverse relevance distribution tracing method, \emph{bias and distortion mitigation} has not been considered for this approach and require further investigation as to how different properties of the complex input feature, such as its size, effect the relevance distributed. Finally, the method through which the rules distribute relevance are clearly defined and provide a \emph{transparent} way for assigning a contribution value to a complex input feature.

\section{Discussion with Respect to the Wider Field}
Chapter~\ref{chap:lit} describes relevant literature in the field of explainable AI. Relevant methods and techniques were divided into a number of categories. \CTC\ falls into the categories of \textit{post-hoc} and \textit{propagation-based} explainable AI techniques.  However, the framework encompassing heatmap clustering and \CTC\ described in this thesis extends beyond traditional propagation-based techniques. Clear parallels can be drawn to other methods, which are discussed below.

Conceptually, the overarching framework described could be seen as a post-hoc attention-based technique. Attention-based techniques typically enforce that a model learns to focus on specific parts of the input. Rather than enforcing attention during learning, \CTC\ enforces that the explanation focuses on specific parts of the input during explanation generation. In particular, Xaio et al. \cite{XiaoXYZPZ15} introduce a framework which uses a combination of attention on certain `patches' of the input, derived from a bottom up proposal, and then uses top-down filtering (i.e. introducing some prior knowledge) in order to make a classification. The resulting classifier uses clear features to make a decision making it interpretable. This technique however relies on application during training, and cannot be applied post-training to any generic CNN. Despite this difference, there are clear parallels here to \CTC. In essence, one could interpret \CTC\ as a post-hoc counterpart to \cite{XiaoXYZPZ15}'s framework, where the initial heatmap generation proposes candidate patches (the bottom-up approach), and the use of SAM or clustering replaces the presence of top-down/prior knowledge to select meaningful features. Moving beyond conceptual similarities to \cite{XiaoXYZPZ15}'s work, it seems fair to refer to \CTC\ as an attention-based explanation, as it fundamentally draws attention to distinct features, which are selected by the clustering mechanism --- relying less on any anthropomorphic interpretation of features of traditional pixel-wise explanations provided by propagation-based techniques.


Unlike many other fields of AI research, explainable AI is still in its infancy, and as such questions over methodology and evaluation are open. This is primarily driven by the lack of an objective measure of ground-truth. Fidelity refers to how accurate an explanation is at illustrating the decision-making process used by the model. It is noted repeatedly throughout the literature that it is a desired quality, and yet there is no way to directly measure fidelity.  In practice, it is evaluated by adding noise to the input as in Chapter~\ref{chapter:results} or by perturbation-based methods, which remove part of the input and monitor the resulting change in the output. Yeh~\textit{et. al.}~\cite{YehHSIR19} note that simple removal of pixels is not a reliable measurement of fidelity, as omitting parts of the input is not an accurate measurement of the importance of features. To understand the contribution of a small part of the input, propagation rules that are true to the models functioning during the original instance inference are needed. This is the same issue identified by Chapter~\ref{chap:framework} in the context of relevance propagation and the rules defined in Chapter~\ref{chapter:REVEAL} propose a more faithful way in which this can be achieved than simply performing inference on the modified input. Ultimately, there is a need for novel metrics to accurately assess fidelity and research into forward propagation techniques is likely to provide an alternative fidelity measure. 

Ribiero's LIME \cite{Ribeiro0G16} builds a local surrogate model, which is trained based on the removal of parts from the input and monitoring the resulting change in the output. This model can then be queried for the importance of complex input features. This surrogate model demonstrates a loss of fidelity, as discussed by~\cite{YehHSIR19}. However, LIME similar to the method combining identification of complex input features and \CTC\ described in this thesis --- indicates a relevance score by region or segment, as opposed to pixel-level. Indeed, \cite{Ribeiro0G16} reports studies with human subjects including both graduate ML students and non-technical subjects from Mechanical Turk, which demonstrate that explanations of this sort are preferred by users, and are able to lead to better decision-making in model-selection, feature-engineering and identifying irregularities in model-classification. Given that the explanations of the method, combining identification of complex input features and \CTC, are functionally equivalent to LIME from the user's perspective, it stands to reason that \CTC\ would also be preferred by users, while also offering a more computationally efficient, and crucially more faithful explanation.

Computational cost is a considerable factor in any explanation method. While \CTC\ employs only a single forward and backward pass to generate explanations, the majority of computational complexity and time is spent in computing the feature masks. In terms of overall time required for an explanation, this renders \CTC\ slower than most gradient based methods~\cite{SimonyanVZ13, SimonyanVZ13, SpringenbergDBR14, bach2015pixel, SelvarajuCDVPB20, SelvarajuCDVPB20, ChattopadhyaySH18, abs-1908-01224, SmilkovTKVW17}. However the computational time required is still far less than Integrated Gradients~\cite{SundararajanTY17} and SmoothGrad~\cite{SmilkovTKVW17}'s, which require 50 to 200 steps to compute an explanation or methods like SHAP~\cite{LundbergL17} and LIME~\cite{Ribeiro0G16}, which require numerous perturbations. It is pertinent to note that the \CTC\ implementation at present is not optimised. The author expects an optimised implementation to be capable of achieving a speed-up of 10--20 times faster than the current implementation and a speed-up of up to 52 times faster than the current implementation of the clustering. While this is likely to still fall behind some methods with respect to computational cost, this is likely to further increase the usable capacity of \CTC.

\section{Future Work}
The advancements made through the unified framework and the techniques of feature isolation, reverse relevance distribution tracing, and forward pass retracing, open several options for future research. In this section a number of possible research avenues are discussed with motivations and possible limitations for each.

\subsection{Feature Isolation}
The feature isolation approach defined in Chapter~\ref{chap:clustering} integrates object detection with advanced heatmap-based clustering to isolate and analyse features in neural network models. This methodology holds significant potential for simplifying the explanations of different relevance methods. Even without the integration of the proposed methods for assigning a single value of contribution to the features identified, merely taking the sum of the relevances withing the feature can provide a more easily comprehensible explanations. There are several promising directions for further research in this area.

One key area is the exploration of how various types of attribution methods can be effectively combined with object detection techniques. Different types of attribution methods have a different spread of relevances, different mean and median of intensity, some have only positive relevances, while some have negative relevances too. By clustering the heatmaps generated from different attribution methods, there is an opportunity to experiment with different clustering parameters such as the distance metric or the threshold for cluster formation. In the presence of negative contributions one could split the clustering in two, where one part clusters the negative contributions, while another clusters the positives, or take the absolute value and only consider the intensity of the relevance. Patterns and relationships within the data that might be detected by the relevance propagation methods may remain hidden, if not analysed in detail after the explanation is generated. 

Additionally, investigating different clustering approaches other than DBSCAN as used in Chapter~\ref{chap:clustering} can offer further advancements in this field. Techniques like hierarchical clustering, density-based clustering, or graph-based clustering could provide alternative perspectives on the data. It is important to consider the input type of the data when choosing the clustering mechanism. When dealing with images as input, the number of clusters is unknown and the shapes of the clusters are not spherical. Therefore clustering techniques that have a centroid point and a predefined number of clusters are not appropriate. However, if the task is better defined, such as separating background noise from not background noise, a clustering technique that has a predefined number of classes might be more appropriate. In some cases isolated features may be part of a bigger feature, if the extraction of such relationships is important, the use of hierarchical clustering could provide this extra information, allowing the inspection of features at different levels of the clustering process. Each of these methods has unique strengths and can be particularly suited to different types of input data, different applications and different neural networks.

\subsection{Reverse Relevance Distribution}
\label{dis:jac}
The proposed reverse relevance distribution tracing in Chapter~\ref{chapter:revLRP} could significantly contribute to the interpretability of neural networks. The core issue with the method lies in its intensive computational requirements. This method demands a significant amount of memory and processing power due to its reliance on computing Jacobian matrices. This computation is both memory-intensive and time-consuming, posing a barrier to its application in larger, more complex networks and applications where an explanation on the fly is needed. To address these challenges, it is worth exploring innovative approaches that maintain the depth and fidelity of interpretations while reducing computational overhead. One such approach is to develop techniques that bypass the need for Jacobian calculations, aiding in the reduction of the memory load. This could involve leveraging alternative mathematical frameworks or simplifying the relevance tracing process while retaining its interpretative power.

% One promising avenue to explore is the use of approximation methods for computing Jacobians. Finite-difference methods are a class of technique that transform ordinary differential equations (ODEs) or partial differential equations (PDEs), which might be nonlinear, into a set of linear equations. This conversion enables the use of matrix algebra methods for solving these equations. With the power of modern computers, executing these linear algebra operations is highly efficient, therefore making the approximate Jacobian computation possible. It is important to consider the accuracy and reliability when deploying this solution for finding the relevance of a feature. Finite-difference methods are subject to two primary types of errors: round-off error and truncation error, also known as discretisation error. The round-off error occurs due to the inherent limitations of computers in representing decimal numbers. Computers can only store a finite number of digits, and as a result, they often round these numbers to fit their storage capabilities. This rounding can lead to small inaccuracies in calculations, known as round-off errors. These errors accumulate over successive computational steps (often the case in deep neural networks), sometimes significantly impacting the final result. The truncation error arises from the method itself. Finite difference methods approximate continuous differential equations by discretising them – breaking them down into a finite number of intervals or steps. The exact solution of the original differential equation and the solution obtained through this discretised approximation will not perfectly align. This deviation is called the truncation or discretisation error. It's the difference between the true, continuous solution and the solution as calculated using the finite difference method, assuming that there are no round-off errors. The size of this error depends on how the differential equation is discretised and the size of the steps taken in the approximation.


One promising avenue to explore is the use of sparse approximation, which seeks to represent the signal with a minimal number of active components or features, which can reduce the computational complexity. This often involves approximating the original data with a combination of a small number of basis elements. Low-rank matrix estimation~\cite{1102314} provides another alternative, which aims is to find a matrix that is close to the original matrix in terms of some distance measure, but with a lower number of linearly independent rows or columns (\ie lower rank). Parallel processing techniques also offer a viable solution. By distributing the computation across multiple processors or using specialised hardware, the process can be expedited. These methods, while potentially sacrificing some accuracy, can dramatically lessen the computational burden. 


However, the memory requirement is still prominent with these solutions. Memory-efficient data structures, such as boom filters, which test whether an element is a member of a set are highly space-efficient, but are probabilistic and can have false positives. Sparse arrays are a common memory-efficient data structure that already has implementations in TensorFlow~\cite{tensorflow2015} and PyTorch~\cite{NEURIPS2019_9015}. In such arrays only non-zero elements are stored. This can save significant memory particularly for the layers closer to the input, as only part of the input's relevance is being reversed and therefore all parts that are not part of the feature are zero. Segmenting tensors into subparts and processing each in isolation could be a divide-and-conquer approach for memory efficiency. Each segment's relevance can be computed independently, reducing the overall memory requirements. As relevance is distributed layer by layer each segment of a layer would need to be completed before the next layer is started, which is likely to create a computational overhead.
This strategy has not been proposed before and may be difficult to implement. It is worth mentioning that the integration of emerging hardware technologies designed for high-intensity computing tasks could make reverse relevance distribution tracing a feasible method of interpretability. New processors and computing architectures, specifically tailored for AI and machine learning applications, could provide the necessary power to run these intensive computations more efficiently without any change in the implementation of the reverse relevance propagation method.

\subsection{Forward Pass Retracing}
\label{futurectc}
Chapter~\ref{chapter:REVEAL}'s novel forward pass retracing method proposes a set of rules for finding a single value of relevance to a complex input feature. These rules mimic the behaviour of each layer's function during the forward pass given the entire input. As the rules propagate a smaller portion of the input than the original activations, the rules are designed to preserve the signal during the propagation. This is particularly challenging for layers with learned parameters. The scaling of learned parameter was the biggest challenge encountered during this research. Given that learned parameters don't change after training, scaling them changes the behaviour of the network, but is a necessary step in preserving the signal of the feature being propagated. Other methods for preserving the signal of the contributions through layers may be necessary. It is worth exploring scaling the learned parameters differently across layers. An example of this can be seen by \LRP~\cite{bach2015pixel}, which proposed a composite strategy~\cite{SamekBLM17} where different rules are used at parts of the network. This is very likely to be a good research direction for improving the contribution to classification (\CTC) value found for a complex feature. Contributions are sparse in the layers close to the input, as all other relevances that are not part of the complex feature are zeroed, so scaling the learned parameters more heavily at these layers may reduce the issues of preserving the signal of the contributions. Layers closer to the output, due to the presence of dense and convolutional layers, have contributions distributed in a uniform way, so such layers may need less parameter scaling. Another direction worth investigating is the scaling of contributions rather than learned parameter values. This can also have a different strength across different parts of the network. The alpha beta rules proposed by \LRP\ present another way to scale contributions. It can be a beneficial strategy if one is more interested in the positive than the negative contributions. The novel rules proposed in Chapter~\ref{chapter:REVEAL} are the ``basic rules'', which can suit as the foundation for different types of contribution scaling.


The research in this thesis primarily focuses on convolutional neural networks (CNNs) like VGG16~\cite{SimonyanZ14a}, VGG19~\cite{SimonyanZ14a}, and InceptionV3~\cite{szegedy2015rethinking}. These models have been extensively studied in this thesis due to their widespread use in image processing and computer vision tasks. However, the landscape of deep learning encompasses a much broader array of architectures, each with unique characteristics and applications. Extending beyond CNNs to gain a comprehensive understanding of the performance of the method across various models is a natural extension of the research proposed in this thesis. One promising direction is the exploration of interpretability in recurrent neural networks (RNNs). RNNs, known for their effectiveness in handling sequential data like text and time series~\cite{10.1162/neco.1997.9.8.1735}, pose a fundamentally different architecture from CNNs. The temporal aspect of RNNs makes the signal of the relevance harder to trace through the network. Issues of expanding and vanishing contributions are likely and further investigation as to how they can be managed in recurrent architectures is needed. Adapting to RNNs could provide valuable test for the interpretability offered by forward pass tracing method and the robustness of the proposed solution for 
preserving the contribution signal through the network.


Similarly, transformer models, which have revolutionised the field of natural language processing~\cite{rothman2021transformers, chernyavskiy2021transformers, tunstall2022natural}, present another avenue for extending the forward pass retracing method. Transformers, particularly known for their self-attention mechanisms~\cite{VaswaniSPUJGKP17}, require a different approach to trace and understand the contribution of input features. Currently, methods for calculating the contribution of complex features in such layers are nonexistent. Therefore, a significant future contribution is the development and refining of rules for these specialised layers. Transformers are the largest models currently being deployed~\cite{VaswaniSPUJGKP17}, so the issues described of signal preservation and learned parameter scaling are a significant part of the difficulty in integrating forward pass retracing for such architectures. Further, the feedback loop for the development of rules for the specialised layers in these architectures would be slow and the computational resources needed for such experimentation would be large. The development of the forward pass retracing as a method for interpreting transformers would involve not only devising new rules but also validating their efficacy in providing faithful and interpretable explanations. 


Generative adversarial networks (GANs) also offer a unique landscape for interpretability research. As models that learn to generate data that is indistinguishable from real data, understanding the decision-making process in GANs could provide insights into their generative capabilities and limitations. This is particularly relevant in applications like image generation, where understanding the contribution of different features to the generated output can be of significant interest. However, applying the forward pass tracing method to these architectures is not straightforward. Each type of neural network layer, be it in RNNs, transformers, or GANs, requires specialised rules to correctly evaluate how complex input features contribute to the classification or output generation. While common layers like dense, convolutional, max pooling, average pooling, and batch normalisation have been explored in this thesis (as detailed in Chapter~\ref{chapter:REVEAL}), layers unique to RNNs and transformers pose new challenges. 

\subsection{Evaluation}
\label{evl}
Chapter~\ref{chapter:results} evaluates the forward pass retracing both qualitatively and quantitatively. It is worth exploring forward pass retracing on a wider range of networks and analysing the performance of the method across them. While the qualitative evaluation offers valuable insights into the interpretability of the method, a more thorough quantitative analysis is needed. This should involve extending the evaluation to additional networks beyond VGG16~\cite{SimonyanZ14a}. A critical aspect of this quantitative analysis is to examine the input invariance and sensitivity of the forward pass retracing method on different network architectures. Understanding how the method responds to varying inputs and its sensitivity to changes can provide crucial information about its reliability and consistency.

An interesting area of exploration is the method's response to different types and intensities of noise in the input. In the current evaluation, we assessed the method's performance with two levels of noise intensity: a lower level representing a minor perturbation and a higher level indicative of substantial noise. Further research could involve a more nuanced analysis of these noise levels. By calculating and comparing the differences in explanations under varying noise intensities, we can gain insights into the threshold at which noise significantly impacts the explanations. This analysis can be crucial for understanding the robustness of the method and determining its reliability under different conditions. Moreover, it's important to evaluate how the significance of noise varies across different network architectures. The point at which noise begins to substantially affect the explanation might differ from one network to another, depending on factors like network depth, complexity, and the type of data it is trained on. 

\subsection{Interpretability and Usability}
This thesis is dedicated to enhancing the interpretability of explanations provided by deep neural networks. It draws on research underscoring human cognitive limitations, notably the challenge in extracting meaningful insights when confronted with more than five significant items simultaneously, as highlighted in seminal works by Cowan~\textit{et al.}~\cite{cowan2001magical}, Starkey~\textit{et al.}~\cite{starkey1995development}, and Morris~\textit{et al.}~\cite{morris2018human}. Addressing these cognitive constraints is central to the thesis. To empirically validate the effectiveness of the proposed interpretability method, a structured user study is proposed here as a direction for future work. This study intricately examines how different interpretability methods influence user comprehension and decision-making. Participants, selected to represent a diverse range of familiarity with neural networks, will be presented with a series of explanations generated by various interpretability methods.

Each participant will engage in a task where they are required to rank the importance of different areas of the input data, based on the explanation they are provided. This task is designed to be intuitive yet comprehensive, ensuring that participants of varying expertise levels can understand and effectively engage with the material. To facilitate this, a brief training session and clear instructions will be provided before the commencement of the study. The core assessment metric in this study is the consistency of the areas chosen by the participants as being of biggest importance to the network as identifyed by the interpretability method. This consistency will be quantitatively evaluated, with statistical methods applied to analyse the rankings across different participants. The interpretability method that yields the highest consistency in the ranking among participants will be considered the most effective. Furthermore, the study will include a qualitative component where participants can provide feedback on each explanation method. This feedback will be instrumental in understanding the subjective aspects of interpretability, such as clarity, relevance, and overall user satisfaction. 


To enhance the usability and accessibility of neural network interpretability methods for a broader audience, including those without deep technical expertise, the development of intuitive, user-friendly interfaces and visualisation tools is essential. The goal is to demystify the complex internal workings of neural networks and make them more transparent and understandable to non-experts. Future work in this area could focus on designing interactive platforms which allow users to select specific parts of the input data they are curious about, and then find out how much this part contributes to the model's output by employing the methods described in the thesis. For instance, in an image recognition task, a user could select a particular object in the image and receive a visualisation showing how that object influenced the model's classification decision. Such tools would not only make it easier for users to grasp the model's reasoning but also promote a more hands-on approach to explainability.



\subsection{Applications}
The ability to uncover biases in commonly used ML models and datasets is invaluable. The research presented in this thesis can aid in identifying such biases, thereby promoting more fair AI systems. For examples in medical domains, by enabling the identification of subject-specific traits and patterns, the method can aid in the personalisation of healthcare, tailoring treatments to individual patient needs and characteristics. This capability is also invaluable in microscopy, where highlighting cellular structures can lead to more accurate diagnoses and a deeper understanding of cellular behaviours. Additionally, the method can have applications in molecular interactions and drug efficacy by highlighting relevant molecular sections in protein interactions. This research can also significantly refine facial expression recognition systems, enabling more nuanced and accurate interpretations of human emotions. For applications in audio engineering and surveillance, the methods developed can pinpoint relevant features for precise audio source localisation. This can be particularly beneficial in environments with high levels of ambient noise or where sound quality is compromised. In surveillance, these methods can help in isolating important audio cues that might otherwise be lost in background noise, thereby improving the efficiency and effectiveness of security systems. Furthermore, the research contributes to the field of autonomous vehicle technology. By explaining the contribution of objects identified by object recognition algorithms, the method offers a new layer of trust and explainability of the deep learning systems used in autonomous driving. This includes better understanding of the contribution of detected pedestrians, other vehicles, and road signs, as well as how challenging conditions such as poor weather or low light might effect the network's confidence. As described in Chapter~\ref{input_inv} the method proposed is input invariant, while still being sensitive and therefore suitable to asses if the change in contribution is significant for objects detected under different conditions. In the realm of environmental monitoring, the techniques developed in this thesis can assist in analysing satellite imagery and sensor data to detect changes in ecosystems, track wildlife, and monitor climate change indicators. 

While the applications outlined here present some of the current uses of explainability in various fields, it's important to note that this is by no means an exhaustive list. The potential of unbiased and fair AI systems and the use of the contributions proposed in this thesis extends far beyond these examples. 


\section{Final Thoughts}
As we navigate the rapidly evolving realm of artificial intelligence, the importance of interpretability in AI systems cannot be overstated. In its early days, AI was primarily focused on solving specific problems in areas like mathematics and logic, often in academic settings. This was followed by researchers exploring the potential of computers to perform tasks that typically required human intelligence, such as understanding natural language and recognising patterns. The development of more powerful computers and the explosion of data available for analysis allowed AI to grow in sophistication and practicality. The journey of AI from a relatively small area of computer science research to a technology that is a crucial component of many industries has been rapid and transformative.

As AI systems become more advanced and autonomous, the ethical implications of their operations grow in complexity and importance. The research in this thesis is aligned with the global movement towards ethical AI, advocating for systems that are not just efficient and powerful, but also transparent and fair. The growing presence of AI in diverse sectors highlights the urgency of this pursuit. As AI continues to evolve and become an ever-growing part of our lives, the work done here lays a foundation for a future where AI is not an inscrutable black box, but a transparent, understandable, and trustworthy tool used to further progress and innovation.