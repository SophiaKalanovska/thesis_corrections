\chapter{Background}
\label{chap:background}
\section{Introduction}

Neural networks have emerged from a combination of mathematical concepts, applied statistics and computational structures~\cite{du2013neural}. This chapter delves into the essential foundations of neural networks, shedding light on the motivation for their use, notation, basic principles, and architectural elements that underpin their functioning.

Neural networks, are computational models designed to \textit{extract information} from data, \textit{recognise useful patterns} in data or \textit{make decisions} based on data~\cite{basu2010use, perlovsky2001neural}. In this chapter the rotational conventions commonly used to describe neural networks are defined. Subsequently, the fundamental building block of neural networks is explored: the neuron. The methods by which artificial neurons process information through weighted connections and activation functions is described. This is followed by a discussion on layers, which are formed by interconnected neurons. The mathematical operations within each layer are shown.

This chapter lays the foundation for the forthcoming discussions on explainability methods for neural networks in Chapter~\ref{chap:lit}. How neurons process input and the different function associated with different layers and the notation introduced are necessary for the contribution chapters of this thesis, where rules are defined as to how contribution and relevance can be propagated forward (see Chapter~\ref{chapter:REVEAL} \& Chapter~\ref{chapter:revLRP})

\section{Neural Network Motivation and General Structure}

Artificial neural networks consist of \textit{processing units}, called neurons, which communicate though \textit{weighted connections} (expressed as directed edges) (see Figure~\ref{fig:neural}). A neural network can have million or even billion of neurons, each one performing a simple computation (see Section~\ref{Sec:process}). A neuron's output of a computation is fed though the weighted connections into the subsequent connected neurons. These neurons in turn use that output as their input for the simple computation they perform. This operation, performed by all neurons jointly, typically implements a non-linear mapping from the input to the output and allows neural networks to perform complex tasks~\footnote{The mapping could be linear in certain cases, such as in the case of a single layer perceptron or a neural network where all layers are linear}. 

Neurons are typically organised into sequential layers, forming the architecture of a neural network. Each layer is a function that receives input from the previous layer and produces output that serves as input for the subsequent layer. The layer's function is implemented by all neurons that form it. Figure~\ref{fig:neural} shows four functions connected in a chain $f_{0}$, $f_{1}$, $f_{2}$ and $f_{3}$, which form the overall neural network function $f(x) = f_{3}(f_{2}(f_{1}(f_{0}(x))))$, where $f_{0}$ is the input layer and $f_{3}$ is the final output layer. The length of the chain determines the depth of the model. 

\begin{Definition}{Neural Network}{nn}

A \emph{(trained) neural network} is a tuple $\NN=\big(\Lambda,\passto,(f_k)_{k\in \Lambda}\big)$ where $(\Lambda,\passto)$ is a directed graph over a set of \emph{layers} $\Lambda=\{0,\dots, N\}$, with a single source $0\in \Lambda$, referred to as the \emph{input layer} and single sink $N\in \Lambda$, referred to as the \emph{output layer}. Each $f_k:\bbR^{n_j}\to \bbR^{m_k}$ describes a vector transformation associated with each layer, with the dimensionality conditions that $m_k=\sum_{j\passto k} n_j$, for all $k=1,\dots N$.
%
We say that layer $j$ \emph{precedes} layer $k$ if $j\passto k$, and that layers $j$ and $k$ are \emph{consecutive}. In recurrent neural networks $j = k$ in some consecutive layers. 
\end{Definition}

The network $\NN$ computes a function $f_{\NN}:\bbR^{d}\to \bbR^{d'}$ on inputs of dimension $d=n_0$ to output of dimension $d'=m_N$, by successively computing the output of each layer, starting from the input layer. The~\Cref{def:nn} is later used in Chapter~\ref{chapter:REVEAL} and Chapter~\ref{chapter:revLRP} when showing the rules that allow for the propagation of relevance and the identification of single value of relevance for features.

The arrangement and function of layers and the number of neurons within each layer determine the network's capacity to learn and generalise from data. Layers in neural networks can be divided into two groups, \textit{visible layers} and \textit{hidden layer(s)}. Visible layers, namely the \textit{input layer}, which receives inputs from the environment (\textit{e.g.\ }an image of a cat to be classified) and the \textit{output layer}, which sends outputs to the environment (\textit{e.g.\ }the label ``bengal cat'' --- the classification of the input). The hidden layers are the layers between the input and the output and they can be any natural number $n \in \mathbb{N}$ (see Figure~\ref{fig:wide} B). 

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=\linewidth]{Figures/neural_network.pdf}
	\end{center}
	\caption{The image represents a five-layer neural network. The input layer receives the vector [x1, x2, x3, x4], which passes through three hidden layers connected through weighted connections, demonstrating the complex data processing within the network. The final output layer computes the vector [y1, y2, y3].}
	\label{fig:neural}
\end{figure} 

A network without hidden layers (\ie a two-layer neural network with only an input and an output layer) can only compute linear mappings between the input and the output. In order to deal with more complex tasks artificial neural networks have to be able to encode complex mappings (\ie non-linear) between the input and the output. To express such mappings the network needs at least one hidden layer and enough parameters in the hidden layer(s) (\ie neurons and weighted connections between neurons) to allow for the information to be learned and later stored and used. 


In principle, artificial neural networks can implement any computable function using only three-layers (\ie a network containing only one input, hidden and output layer)~\cite{Hornik91}. However, given that the input layer is fixed, as the input the network receives should always be the same size and type, and the output layer is fixed to all the possible types of outputs the network can produce, the only layer left to learn the task is the hidden layer. Therefore, to implement a complex task using only a three-layer neural network, the hidden layer of the network must contain a lot of neurons to allow for more parameters that will learn the task in question. The example below shows the increasingly more complicated decision boundary a neural network can learn, the more neurons are being added to the hidden layer of the network.


\begin{Example}{ANN decision boundary}{}
An example of how a network can have an increasingly more complex non-linear decision boundary, when more non-linear hidden neurons are added to its design is illustrated in Figure~\ref{Fig:decision}. The first sub-figure Figure~\ref{Fig:decision} (a) shows a non-linearly separable data set, where the light blue circles belong to class 1 and the dark blue circles to class 2. Figure~\ref{Fig:decision} (b), (c) and (d) illustrate the decision boundaries of neural networks with 1, 2 and 3 non-linear hidden neurons respectively, where the points in the purple region are classified by the neural network as belonging to class 1 and the points in the yellow regions as class~2. 
Figure~\ref{Fig:decision} (b) shows a neural network with a single non-linear hidden neuron and illustrates the linear decision boundary that this network has. Figure~\ref{Fig:decision} (c) shows the same neural network with one more non-linear hidden neuron ($\ie$ two in total) and illustrates the reduction in classification error and the overall non-linear (and discontinuous) decision boundary of that neural network. Finally, Figure~\ref{Fig:decision}~(d) shows a neural network with 3 non-linear hidden neurons that also achieves 100\% accuracy on this dataset.


\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Figures/decision_bounderies.png}
	\end{center}
	\caption{Illustration of decision boundaries of a neural network where hidden neurons with a non-linear activation function are added incrementally (the non-linear activation function used is sigmoid (see Equation~\ref{eq:sigmoid}))}
	\label{Fig:decision}
\end{figure} 

\end{Example}

\subsection{Feature Representation}
Machine learning systems require an extensive volume of data to acquire the expertise needed for specific tasks. The performance of all machine learning algorithms is heavily contingent on the manner in which data is represented and structured. Finding the optimal set of features that enables the algorithm to grasp a task proficiently is challenging and stands as the main motivation for neural networks, which not only acquire a mapping from input to output, but also the pertinent features from the input.

Whenever the number of hidden layers of the network is small and the hidden layers have many neurons, then the neural network often memorises the training data rather than learning features from the input~\cite{PoggioMRML17}. The architecture of such neural network is referred to as wide (see Figure~\ref{fig:wide}~A). The limited amount of layers in such architectures prevents the network from learning concepts and patterns from the data and leads to a failure of the network to perform well (\textit{generalise}) to unseen samples. This problem is referred to as \textit{memorisation of the  training data}. To allow for hierarchical representations to be learned majority of neural networks used and designed are deep neural networks (DNNs) with a number of hidden layers (see Figure~\ref{fig:wide}~B). Neural networks that are created with more that one hidden layer, have the capability of learning their own complex features, capturing the natural hierarchy that many machine learning tasks have. They can learn complicated concepts by building them out of simpler ones, which allows for complex mappings to be learned without memorisation of the training data. An example of this hierarchy of abstractions can be seen in the process of arriving at a class label for an image recognition task. The input layer recognises pixels, the hidden layers following the input layer can learn to recognise edges in the image and deeper into the network the algorithm may detect more complex things such as object and textures, until it finally arrives at a label that describes the object identified. 


\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Figures/neural_network_wide_deep.pdf}
	\end{center}
	\caption{Illustration of a wide neural network and a deep neural network.}
	\label{fig:wide}
\end{figure} 


Many machine learning algorithm need to have their features selected (a process commonly referred to as feature engineering) to make their task easier to learn. However, as briefly discussed (see Section~\ref{featureeng}) this can be a challenging endeavour and the ability of deep neural networks to learn their own feature representations is one of the main reasons behind their popularity. The inputs to a neural network is referred to as a an \textit{input feature map} $F$ and is a $n$-dimensional array that can take any form. 

\begin{Definition}{Input Feature Map}{inputfeature}
Input feature map $F \in \mathbb{R}^{d_1 \times d_2 \times \ldots \times d_n}$, where $d_1, d_2, \ldots, d_n$ are the dimensions along each axis of the tensor and the number of dimensions $n$ could vary based on the type of data.
\end{Definition}

In image data $F \in \mathbb{R}^{H \times W \times D}$, where $H$ is the height of the feature map, $W$ is the width and $D$ is the depth, usually corresponding to the number of colour channels (\eg three for an RGB image) and a single value in the feature map is $F_{i, j, k}$, where $i$ is the row index ($1 \leq i \leq H$), $j$ is the column index ($1 \leq j \leq W$), an $k$ is the depth index ($1 \leq k \leq D$). When learning from text data each word or token could be represented as a vector in a high-dimensional space, often called an embedding. A sequence of words can then be viewed as $F \in \mathbb{R}^{N \times D}$, where $N$ is the number of tokens in the sequence and $D$ is the dimensionality of the embeddings.


Deep neural networks transform and extract information from the input feature map $F$ through their layers. This higher-level, abstract representations of input data is referred to as a \textit{learned feature}~\cite{hinton2006reducing}. Depending on the task the learned features can be discrete (\eg labels such as ``ears”, ``mouth”, etc.) or continuous (\eg numeric values representing height, pixel values, etc.). As data passes through layers of the neural network, it undergoes a series of transformations (discussed further in Section~\ref{Sec:process}). These operations create new feature maps at each layer, which capture increasingly complex and abstract characteristics of the original input. The learned feature vector after each layer is denoted as $F_i$ that is a result of the transformation of the layer $\lambda_i$, represented as:

\begin{eqnarray}
    F_i = \lambda_i(F_{i-1})
\end{eqnarray}


\begin{Definition}{Feature Vector}{}
Each layer \(\Lambda_i\) in a neural network, as defined in~\Cref{def:nn}, has a feature vector \(F_i \in \mathbb{R}^{d_1 \times d_2 \times \ldots \times d_n}\). This feature vector \(F_i\) is the result of the transformation applied by the function \(f_i\) at layer \(\Lambda_i\), acting on the output feature maps of all preceding layers \(\Lambda_j\) where \(j \passto i\). 
\end{Definition}

Feature vectors are learned by the neural network and as such there is no transparency on what the features look like or how important they are to the networks output. An explanation for the decision of a neural network can therefore be defined as:

\begin{Definition}{Explanation}{}
An explanation for the decision of a neural network is a mapping \( E: F_i \rightarrow I_i \), where \( F_i, I_i \in \mathbb{R}^{d_1 \times d_2 \times \ldots \times d_n} \). The importance vector \( I_i = E(F_i) \) assigns to each feature in \( F_i \) a numerical importance score \( (I_i)_{k_1, k_2, \ldots, k_n} \), representing the importance of \( (F_i)_{k_1, k_2, \ldots, k_n} \) to the neural network's output. This mapping provides a faithful representation of how each feature influences the network's decision.
\end{Definition}

\subsection{Specialised Neural Network Architectures}
Neural networks are versatile and can learn from input data in a number of forms (\textit{e.g.\ }video, sound, images, EEG recordings, salary etc.) depending on the task. The foundational architecture that can be trained on various inputs is the multi-layer perceptron (MLPs). Characterised by their feed forward structure and multiple layers, MLPs can capture complex relationships in data and have been used for variety of tasks. They have since been subsumed by more specialised architectures aimed at specific tasks. Specific neural network architectures are better equipped and applied to a subset of problems. For instance, when processing image data, a \textit{convolutional neural network} (CNN) is often the architecture of choice~\cite{egmont2002image}. CNNs contain layers, such as convolutional layers and pooling layers (see Section~\ref{sec:conv} \& Section~\ref{section:max}), that can detect and learn hierarchical patterns in images. On the other hand, for sequential data like time series or natural language, \textit{recurrent neural networks} (RNNs), Long Short-Term Memory (LSTM) networks, and increasingly in recent years, transformer networks are often chosen~\cite{husken2003recurrent, langkvist2014review, zhou2021informer}. These architectures have the capability to process sequential inputs allowing them to establish connections between past and present data points. Networks like \textit{autoencoders} are specialised in data compression and noise reduction. The autoencoder architecture starts wide with layers containing many neurons followed by middle layers with fewer neurons and ending with wide layers again. This forces the network to compress the input into a compact representation and then reconstructing it. Neural networks are also deployed in situations where an agent learns to perform actions in an environment to achieve certain objectives (\ie reinforcement learning tasks), in such cases Deep Q-Networks (DQNs) networks  are often employed. It is important to note, that here are listed only the most popular architectures and that there are many more neural network architectures available and a wide range of values that the parameters in these networks can have.


\subsection{Convolutional Neural Networks (CNNs)}


The increased interpretability and faithfulness of the techniques in this thesis are examined on image data. This section covers some of the more popular neural networks for processing images.

Convolutional Neural Networks (CNNs) are specialized deep learning architectures designed to process data with a grid-like topology, such as images~\cite{726791}. They leverage spatial hierarchies by applying convolutional filters (see Section~\ref{sec:conv}) to local regions of the input, enabling the network to learn local patterns and assemble them into complex features through successive layers. CNNs are the main architecture used for image recognition tasks due to their ability to capture spatial and temporal dependencies in data efficiently.

Several CNN architectures have been developed to enhance performance on image classification and recognition tasks. This section discusses some of the most influential architectures: VGG16~\cite{SimonyanZ14a}, VGG19~\cite{SimonyanZ14a}, ResNet50~\cite{he2015deep}, InceptionV3~\cite{szegedy2015rethinking}. 

\subsubsection{VGG16 and VGG19}

VGG16 and VGG19 are deep convolutional networks introduced by Simonyan and Zisserman~\cite{SimonyanZ14a}. These architectures are known for their simplicity and depth, utilizing small $3 \times 3$ convolutional filters throughout the entire network. VGG16 consists of 16 weight layers, while VGG19 extends this to 19 weight layers. Both networks follow a straightforward design pattern: stacking multiple convolutional layers, each followed by a rectified linear unit (ReLU) activation, and periodically applying max-pooling layers for spatial downsampling.

The uniform use of $3 \times 3$ filters allows the network to capture intricate features while keeping the receptive field manageable. However, the increased depth leads to a significant number of parameters—approximately 138 million for VGG16—which demands substantial computational resources and increases the risk of overfitting. Despite these challenges, VGG networks have demonstrated excellent performance on image classification benchmarks, serving as a baseline for many subsequent architectures.

\subsubsection{ResNet50}

ResNet50 is a 50-layer deep convolutional network introduced by He et al.~\cite{he2015deep}. It addresses the degradation problem observed in very deep networks, where adding more layers leads to higher training errors. ResNet50 employs a novel architecture called \textit{residual learning}, which uses shortcut connections (also known as skip connections) to create identity mappings that bypass one or more layers.

These shortcut connections enable the network to learn residual functions with reference to the layer inputs, effectively allowing the layers to fit a residual mapping rather than the entire transformation. This approach mitigates the vanishing gradient problem, facilitating the training of much deeper networks without performance degradation. ResNet50 has approximately 25.6 million parameters, significantly fewer than VGG networks, and has achieved state-of-the-art results in image recognition tasks.

\subsubsection{InceptionV3}

InceptionV3 is an advanced version of the Inception architecture developed by Szegedy et al.~\cite{szegedy2015rethinking}. The Inception modules within this architecture are designed to capture multi-scale spatial information by performing parallel convolutions (see Equation~\ref{eq:conv_layer}) with different filter sizes ($1 \times 1$, $3 \times 3$, and $5 \times 5$) within the same layer. InceptionV3 achieves a balance between computational efficiency and model complexity, making it suitable for deployment in environments with limited resources. With approximately 23.8 million parameters, it offers competitive performance on large-scale image recognition datasets.

\section{Neurons Transforming Data from Layer to Layer}
\label{Sec:process}

In a neural network, the layers serve as building blocks, stringing together neurons to help the network process and understand complex patterns in data. This capability does not solely lie in the architecture or the number of neurons, but critically in how these neurons transform their input into output. 
\subsection{A Neuron}
A neuron $j$ in an artificial neural network receives an input vector $\vec{x}$ either from the outputs of the connected neurons from the previous layer or from the external environment. The neuron then performs a simple calculation, which transforms the input and determines the response of the neuron. Such functions, are known as \textit{response functions} and they play a pivotal role in defining the behaviour and learning capacity of a neural network. By applying a response function on the input vector $\vec{x}$ the neuron produces the output $y_j$ (response) (see Figure~\ref{fig:neuron}). This response function is comprised of two sequential functions --- a transfer function (see Section~\ref{section:trans}) and an activation function (see Section~\ref{section:act}).
% \mike{intuition}


\begin{Definition}{Neuron}{}
A \emph{neuron} is a tuple $j = \langle t, \act\rangle$ comprising a transfer function $t:\mathbb{R}^d \to \mathbb{R}$ and an activation function $\act:\mathbb{R}\to \mathbb{R}$, where $d \in \mathbb{N}$. The neuron takes as input a vector $\vec{x}\in \mathbb{R}^d$ and applies the transfer function $t$ to produce a real value $t(\vec{x})=\net\in \mathbb{R}$. The activation function $\act$ is then applied to $\net$ to produce a real-valued output $\act(\net)=y_j\in \mathbb{R}$.
\end{Definition}


The input vector $\vec{x}$ contains values from $x_1$ to $x_d$, where $d$ is the number of elements in the vector. Each value $x_i\in \mathbb{R}$ is a real number and the vector of inputs $\vec{x}\in \mathbb{R}^d$ is a vector of real number of dimension $d\in \mathbb{N}$. Each input $x_i$ can be connected (\textit{i.e.\ }be an input) to more than one neuron. 

The neuron takes the inputs from the previous layer and applies the \textit{response function}. The response function computation consists of two parts: a \textit{transfer function} and an \textit{activation function}. The transfer function sometimes has associated parameters with it (discussed in Section~\ref{section:trans}), where the most commonly used parameter is a weight vector $\vec{w}_{j}$, where each element is a connection weight between input $x_i$ and neuron $j$, defined as $w_{ji}$. Each weight $w \in \mathbb{R}$ is a real number and it determines the strength of influence that input has on the neuron $j$. If $w_{ji}>0$ then neuron $i$ has a positive (\emph{excitatory}) influence on neuron $j$, while if $w_{ji}<0$ then $i$ has a negative (\emph{inhibitory}) influence on $j$.

The transfer function integrates all inputs and weights (if there are weights associated with the function) into a single value $net_j$ (see Section~\ref{section:trans}). The activation function takes $net_j$ as input, applies a function (see Section~\ref{section:act}) and determines the response $y_j$. Figure~\ref{fig:neuron} illustrates the computation that is performed by neuron $j$.


% \mike{intuition}
% \chrislong{The notation allows you to be more precise here: for example ``If $w_{ji}>0$ then neuron $i$ has a positive (excitatory) influence on neuron $j$, while if $w_{ji}<0$ then $i$ has a negative (inhibitory) influence on $j$''}
\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Figures/neuron.pdf}
	\end{center}
	\caption{Illustration of a neuron $j$}
	\label{fig:neuron}
\end{figure} 



\subsection{A Layer}

Each level of interconnected neurons in a neural network forms a layer in the network. All neurons that belong to a layer perform the same calculation, by applying the layer's associated response function.

\begin{Definition}{Layer}{}
A layer is a function $\lambda: \mathbb{R}^d \to \mathbb{R}^t$, where $\lambda(\vec{x}) = [\act_1(t_1(\vec{x})), \act_2(t_2(\vec{x})) \dots, \act_k(t_k(\vec{x}))] $, where the input is a vector $\vec{x}\in \mathbb{R}^d$ and each element in the output vector is the output of a neuron $j_i = \langle t_i, \act_i \rangle$, where each transfer function $t_i:\mathbb{R}^{d_i} \to \mathbb{R}$, with the dimensionality
conditions that $d_1 = d_2 = d_3 \dots = d_k$.
\end{Definition}

The response function that neurons in a layer utilise is not just a mathematical tool for data transformation, but rather a defining aspect of the layer's purpose and behaviour within the neural network. 

\subsection{Response function}

The choice of a response function can shape the tasks a layer is most adept at handling. For instance, a layer which has an activation function that squashes the input into a range can be ideal for outputting probabilities. A non-linear activation function can capture complex patterns in data, allowing the network to model non-linear relationships. Without non-linear activation functions the neural network layers would just be a composition of linear transformations which would itself be a linear transformation, so complex relationships would not be learned. 

The response function can prioritise kinds of patterns during training (\eg edge detection), it can make the network more robust to different input ranges (\eg by normalising the input) and it is crucial for the successful training (\ie convergence) of deep neural networks. Given the variety and use of both transfer and activation functions, they can be combined in countless ways to yield desired behaviours in neural networks. In this section, we will first explain some of the most prominent transfer and activation functions. Subsequently we explore prevalent combinations employed in widely recognised types of layers, with a particular focus on computer vision and CNNs.

\subsubsection{Transfer function}
\label{section:trans}
The transfer function is the first step in calculating the response function. It determines how the inputs from the previous layer or the external environment are integrated and transformed. 

\begin{Definition}{Transfer function}{}
A neuron's \emph{transfer function} $t:\mathbb{R}^d \to \mathbb{R}$ takes as input an input vector $\vec{x}\in \mathbb{R}^d$ and integrates those values into a single real value $net_j$ by applying the transfer function $t(\vec{x})=net_j\in \mathbb{R}$.
\end{Definition}


% The function takes the input vector $\vec{x}\in \mathbb{R}^d$
% , and the associated weight vector $\vec{w}_j\in \mathbb{R}^d$, which are the inputs for neuron $j$. The function also has an associated parameter, which is a constant, called a threshold or bias $w_{j 0}\in \mathbb{R}$ or $\theta \in \mathbb{R}$. 

\textbf{Weighted Sum}
\label{sec:weightedsum}
Many layer's transfer functions have an associated weight vector $\vec{w}_j\in \mathbb{R}^d$, where each element from the vector weights the inputs for neuron $j$ and has an associated parameter, called a threshold or bias a threshold or bias $\theta= \beta= w_{j,0}\in \mathbb{R}$. The most common way of integrating all the inputs of the transfer function is by taking the weighted sum of the inputs. This is done by taking the sum of all inputs $x$ multiplied by their corresponding weight $w$ and then adding the threshold $w_{j 0}$ to that sum to produce the final integrated input $net_j$ for a given neuron $j$. The integration of the constant term $w_{j 0}$ allows for faster convergence and provides a minimum activation value for the neuron regardless of the input. 

The type of transfer function that integrates the input values by taking the weighted sum of the inputs is therefore expressed, as:
\begin{eqnarray}
    \notag t_{wsum}(\vec{x}) & = & x_{1} w_{j 1}+x_{2} w_{j 2}+\cdots+x_{d} w_{j d}+w_{j 0} \\
    \notag & = & \sum_{i=1}^{d} x_{i} w_{j i}+w_{j 0} \\
     & = & \sum_{i=0}^{d} x_{i} w_{j i} \ = \  \vec{{w}_j}^T\, \vec{x}
    \label{eq:weightredsum}
\end{eqnarray}

where the subscript $i$ denotes the index of the neurons in the source layer, $j$ denotes the index of the current neuron, $w_{j0}$ is the bias, $x_0$ is 1 (to allow for the augmented notation), $x_1$ to $x_d$ are the values contained in the input vector $\vec{x}$ and $w_{ji}$ denotes the weight of source-to-current neuron at a neuron $j$ and 
\begin{equation*}
  \vec{w}_{j}=\left[\begin{matrix}w_{j 0}\\ w_{j 1} \\ w_{j 2} \\ \vdots  \\ w_{j d }\end{matrix}\right] 
  \qquad \mbox{and}\qquad
  \vec{x}=\left[\begin{matrix} 1\\ x_{1} \\ x_{2} \\ \vdots \\ x_{d} \end{matrix}\right]
\end{equation*}
and ${T}$ in $\vec{{w}_j}^T$ is the transpose of $\vec{w}_{j}$.


The bias is often a one dimensional vector that has the size of the feature dimension of the activation as shown in Figure~\ref{fig:dimentionality_bias}. 
\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.8\linewidth]{Figures/Tensor_bias.pdf}
	\end{center}
	\caption{This figure shows the addition of a one dimensional bias vector with the the size of the feature dimension of the $\cnet^-$ matrix.}
	\label{fig:dimentionality_bias}
\end{figure} 

When looking at the addition of the bias on a layer level rather than on a neuron level, it represents a constant shift of the mean of the activations' distributions as shown in Figure~\ref{fig:addingbias} (for a single dimension). This follows from a more general formula $E[a*X+b] = a*E[X] + b$ for the linearity of the expectation/mean of a random variable $X$, where $a =1$ in the case of the bias addition. 
\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Figures/distribution.pdf}
	\end{center}
	\caption{The figure shows for a single feature dimension how the mean changes from the one of the weighted activation to the the mean of the weighted activation plus the bias constant.}
	\label{fig:addingbias}
\end{figure} 

\textbf{Max Pooling Transformation}
The max pooling transformation function operates on small regions of the input feature map and picks the maximum value from each region. Each small region is fed as input to a neuron and the neuron picks the max value and outputs it as $net_j$.
\label{section:maxout}
\begin{equation}
t_{max}(\vec{x}) =\max _{j \in \vec{x}} x_j, 
\label{eq:max}
\end{equation}
This transfer function is commonly used by max pooling layers (see Section~\ref{section:max})

\textbf{Average Transformation}
\label{section:avg}
The layer's average transformation function also operates on small regions (sub-matrices) of the input feature map, but calculates the average value for each region rather than the maximum one. Each region is fed into a neuron from the layer and the transformation that neuron performs is defined as:
\begin{equation}
f(\Vec{x})=\frac{1}{n} \sum_{i=1}^n x_i,
\label{eq:avg}
\end{equation}
where $\vec{x}$ is the input vector (or the region that is fed tot h neuron) and $n$ is the number of elements in the input vector $\vec{x}$. This transformation function is often used by average pooling layers (see Section~\ref{section:avglayer})

\textbf{Convolutional Transformation}
\label{section:conv}
The convolutional transformation operates on small regions (sub-matrices) of the input feature map and calculates the dot product between the input and a learned filter associated with this transfer function, often referred to as a kernel. Each neuron from the layer has an associated region slice of the input feature map $X$ of dimensions $h \times w$ that the filter $W$ of dimensions \( f_h \times f_w \) is covering. For simplicity, assume that both the input feature map and filter have only one channel. The neuron computes the following weighted sum of its inputs (which is the convolution operation):

\begin{equation}
t_{conv}(X) = \sum_{i=0}^{f_h - 1} \sum_{j=0}^{f_w - 1} X(i, j) \cdot W(i, j) + b.
\label{eq:conv}
\end{equation}

This is the output of that particular neuron for the given region of the input feature map. This transformation function is often used by convolutional layers (see Section~\ref{sec:conv})

\textbf{Batch Normalisation Transformation}
The batch normalisation transformation has four associated learned parameters, mean $m$, variance $\sigma$, scale $\gamma$ and a shift $\beta$. The desired effect of the batch normalisation is to first shift the input to have a mean of zero and a standard deviation of one and then re-scale it appropriately using scale $\gamma$ and a shift $\beta$.
During training the values of the mean and the standard deviation are calculated as usual by:

\begin{equation}
\mu^\prime = \frac{1}{N} \sum_{i=1}^{N} x_i \qquad \sigma^{2\prime} = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2
\end{equation}

Using those values a learnable mean $m$ and variance $\sigma$ are updated by using back propagation (see Section~\ref{sec:backprop}). Each neuron in this layer has as input a single input value $x$, which has to be normalised and then scaled and shifted. The normalisation is achieved through :

\begin{equation}
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}},
\end{equation}
where $\epsilon $ is a small constant added for numerical stability.

Finally, the normalised output $\hat{x}$ is scaled and shifted using the parameters  $\gamma$  and  $\beta$ :

\begin{equation}
net_j = \gamma \hat{x} + \beta
\end{equation}


The batch normalisation transformation for a given neuron can be summarised to:
\begin{equation}
    t_{norm}(x) = \gamma \frac{(x-\mu)}{\sqrt{\sigma^2 +\epsilon}} + \beta ,
\end{equation}

\subsubsection{Activation function}
\label{section:act}
The activation function is the final step in calculating the response function of a neuron. It transforms the integrated neurons input $\net$ and determines the proportion of the transformed value that should be transmitted from one layer to the next by calculating the output of neuron~$j$. 
\begin{Definition}{Activation function}{}
A neuron's \emph{activation function} $\act:\mathbb{R} \to \mathbb{R}$ takes as input $\net$ and returns a real-valued overall response (activation) $y_j$ of neuron~$j$ $\act(\net)=y_j\in \mathbb{R}$ .
\end{Definition}

The final layer of neural networks often needs to output a probability. This layer's neurons often apply activation functions that transform the input into a number within a range (referred to as a \textit{squashification function}). One such activation function is the \textit{sigmoid function} defined by:

\begin{equation}
\label{eq:sigmoid}
\act(x)=\frac{1}{1+\exp (-x)}
\end{equation}

and can be visualised as a S-shaped curve with a range of activation for this function is $(0,1)$ (see Figure~\ref{Fig:act}~(a).

Another squashification function is the \textit{hyperbolic tangent function} (\textit{tanh function}), defined as
\begin{equation*}
\act(x)=\frac{exp(x) - exp (-x)}{exp (x)+\exp (-x)}.
\end{equation*}
The tanh function is just a scaled sigmoid function, it also has an S-shape, but with a range between $(-1, 1)$ (see Figure~\ref{Fig:act}~B). 

Squashification functions are useful, as they allow to determine how positive the input to the function is. A drawback of using this type of activation function in the hidden layers of neural networks is that the output will not respond to or will respond little to changes in the value of $\net$ that is on either side of the curve. This makes neural networks using this type of activation function harder to train ($\ie$ the network stops learning or learns very slowly). This problem is referred to as the \textit{vanishing gradient problem}.

An example of non-linear activation function that overcomes the vanishing gradient problem is \textit{rectifier activation function (ReLu)}. In modern neural networks, the default recommendation is to use ReLU~\cite{JarrettKRL09, NairH10, Heaton18}, which is deﬁned by the activation function 

\begin{equation}
\act(x)=\max \left(x, 0\right)= \begin{cases}x &  \text { if } x>0 \\ 0 & \text { otherwise }\end{cases}
\label{eq:relu}
\end{equation}

that is a linear in the positive axis and $0$ otherwise (see Figure~\ref{Fig:act}~C). Because of the linearity in the positive axis the function is unbound and can produce an output $y_j$ in the rage of $[0,\infty)$, which may cause the activations to blow up. An advantage of the ReLu function is that some neurons remain inactivate in comparison to the the sigmoid or tanh activation functions where the neurons fire in an analog way. The inactivate neurons when using ReLu make the activations more sparse, which in turn requires the network to perform fewer computations and increases efficiency. The disadvantage of ReLu is that it can stop responding to training, as the rate of change may become $0$, because of the constant output for any negative value of $\net$. This is overcome by allowing small negative values for for all negative values of $\net$. This is referred to as \textit{leaky ReLu}, defined by the activation function 
\begin{equation*}
\act(x) =\max \left(x, 0\right)= \begin{cases} x & \text { if } x>0 \\ \epsilon x & \text { otherwise }\end{cases}
\end{equation*}
which has a slightly inclined line rather than horizontal line for all negative values of $\net$ (see Figure~\ref{Fig:act}~D), the $\epsilon$ value is often set to 0.01.


\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=\linewidth]{Figures/act.pdf}
	\end{center}
	\caption{Illustration of non-linear activation functions}
	\label{Fig:act}
\end{figure} 



While many more activation functions exist and can potentially be employed in neural networks, those discussed here are among the most widely adopted in the field. Below we discuss some of the layers that deploy them and the effect their application has on the network learning. 

\subsection{Types of Layers Based on Response Functions}

\subsubsection{Dense Layer}
\label{section:dense}
In dense layers, often referred to as fully connected layers, each neuron is connected to every other neuron in the previous layer. In other words, the outputs of all neurons in the previous layer are inputs to each neuron in the following layer. This connectivity pattern allows information to flow freely and interact between neurons, giving the network the ability to learn intricate relationships and make complex predictions. They have been used extensively in various architectures from traditional multi-layer perceptrons to more modern structures like convolutional neural networks and recurrent neural networks.

Each neuron in a dense layer performs a weighted sum of its inputs (see Section~\ref{eq:weightredsum}) as a transformation function, where the weights associated with each input determine the strength of the connection between neurons. After the linear transformation (dot product of inputs with weights and addition of bias), the result is typically passed through a non-linear activation function, commonly ReLU (Rectified Linear Unit) (see equation~\eqref{eq:relu}), in some cases the activation function is just bypassed through an identity function $f(x) = x$.

\begin{Definition}{Dense Layer}{}
The \emph{dense layer function} $f_k:\bbR^{n_j}\to \bbR^{m_k}$, which describes the vector transformation associated with the layer $\lambda_k$. The transfer function of dense layer neurons is a weighted sum transformation defined in Equation~\eqref{eq:weightredsum}. The activation function part of dense layers may be any function of the ones defined in Section~\ref{section:act}
\end{Definition}

\subsubsection{Max Pooling Layer}
\label{section:max}

Max pooling layers play a pivotal role in extracting essential features from input data, especially in convolutional neural networks. They divide the input of the layer into non-overlapping regions and select the maximum value within each region. The size of the pooling operation or filter is smaller than the size of the feature map. Commonly, the filter is $2\times 2$ pixels applied with a stride of 2 pixels. Stride denotes by how many pixels apart are all filters. As max pooling does not have overlapping regions, the stride and the filter dimension are the same. In the case where we have a filter of $2\times 2$, the resulting layer will have each dimension halved, reducing the number of pixels or values in each feature map to one quarter the size. 

More concisely max pooling layer divides the input $X$ in to into groups of $k$ values. Each group is fed as input to a neuron $j_i$ from the max pooling layer, which performs a max pooling transformation $t_{max}$ (see Section~\ref{section:maxout}) and outputs the maximum element from the group. The overall function the layer performs can be summarised by 

\begin{equation}
\operatorname{MaxPooling}(X)= [t_{max}(X[i \cdot s_x, j \cdot s_y, c])]_{i,j,c},
% \label{eq:maxpooling}
\end{equation}

where is the input, $(i,j)$ are the indices of the output, $c$ is the channel index, $s_x$ and $s_y$ are the stride values in the horizontal and vertical directions, respectively. Figure~\ref{Fig:maxpooling} shows an example of the output of a max pooling layer.

The max-pooling process has many advantages. Notably, taking the maximum value across every group of $k$ features, reduces the parameters that the subsequent layer operates with by a factor of $k$. This allows the layer to capture the most salient features present in the input while discarding less significant information. Max pooling is also known for removing invariances, meaning the network can still recognise features regardless of their position, rotation and scale. As a result the max pooling operation enhances the network's ability to generalise and makes it more robust to noise. Max pooling layers do not have learnable parameters.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Figures/maxpool.pdf}
	\end{center}
	\caption{A max pooling transformation, where the $t_{max}$ function is performed and the workings are shown on the first filter-sized part of the input (in pink).}
	\label{Fig:maxpooling}
\end{figure} 


\begin{Definition}{Max Pooling Layer}{}
The \emph{max pooling function} $f_k:\bbR^{n_j}\to \bbR^{m_k}$, which describes the vector transformation associated with the layer $\lambda_k$, with the dimensionality conditions that $n > m$. The transfer function neurons in the layer performs is max pooling transformation described in equation~\eqref{eq:max}. The activation function that follows is often ReLU~\eqref{eq:relu}, but could also be linear activation. This type of layer has an associated filter size and no learnable parameters.
\end{Definition}

\subsubsection{Average Pooling Layer}
\label{section:avglayer}
Average pooling layers, similarly to max pooling layers, are a type of down-sampling or sub-sampling layer. These layers reduce the spatial dimensions of the input volume during the forward pass by taking, as the name suggests, an average of a group of neighbouring pixels and outputting this average as the new value for the corresponding region in the output feature map.

More concisely average pooling layer divides the input $X$ in to into groups of $k$ values. Each group is fed as input to a neuron $j_i$ from the average pooling layer, which performs a average transformation $t_{avg}$ (see Section~\ref{section:avg}) and outputs the group's mean. The overall function the layer performs, can be summarised by 

\begin{equation}
\operatorname{AvgPooling}(X)= [t_{avg}(X[i \cdot s_x, j \cdot s_y, c])]_{i,j,c}
\label{eq:averagepooling}
\end{equation}

where is the input, $(i,j)$ are the indices of the output, $c$ is the channel index, $s_x$ and $s_y$ are the stride values in the horizontal and vertical directions, respectively. Figure~\ref{Fig:avg} shows an example of the output of a average pooling layer. Average pooling layers also do not have learnable parameters.


\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Figures/avgpooling.pdf}
	\end{center}
	\caption{A average pooling transformation, where the $t_{avg}$ function is performed and the workings are shown on the first filter-sized part of the input (in pink).}
	\label{Fig:avg}
\end{figure} 
Similarly to max pooling layers,  they reduce the spatial dimensions of the input volume, which subsequently reduces the computational complexity of the model. This can make it easier to train the model and can help prevent overfitting. Contrary to max pooling, which only keeps the maximum value in a local region, average pooling is less aggressive in reducing the input dimensions. This means that more information can be preserved when down-sampling, although this can also be a disadvantage if too much irrelevant information is retained. Average pooling has a smoothing effect on the input. This is sometimes beneficial for tasks where the fine-grained details are not as important as the broader structure. 

\begin{Definition}{Average Pooling Layer}{}
The \emph{average pooling function} $f_k:\bbR^{n_j}\to \bbR^{m_k}$, which describes the vector transformation associated with the layer $\lambda_k$, with the dimensionality conditions that $n > m$. The transfer function neurons in the layer performs is average transformation described in equation~\eqref{eq:avg}. There is often no activation function that follows. This type of layer has an associated filter size and no learnable parameters.
\end{Definition}
\subsubsection{Convolutional Layer}
\label{sec:conv}
Convolutional layers are a type of layer often used for processing image information and a main building block of convolutional neural networks. They are designed to automatically learn spatial hierarchies of features, focusing on local regions at first and later capturing more global information as the layers progress. The stacking of convolutional layers enables the learning of higher-level features by composing them from lower-level ones. For instance, the first layer might learn edges, the second layer shapes by combining edges, and deeper-level layers might learn more complex structures or objects. Figure~\ref{Fig:faces} shows the amalgamation of feature learned in a CNN as a result of using convolutional layers.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/faces.pdf}
	\end{center}
	\caption{Learned features from a Convolutional Deep Belief Neural Network. Source~\cite{LeeGRN09}}
	\label{Fig:faces}
\end{figure} 

Convolutional layers learn the features through a \textit{kernel} (also known as filter), which is associated with the transfer function. A kernel is a small matrix of numbers, which is passed over the input and performs a transformation based on the the values in the filter. The kernel can have predefined values, so that it is specialised at detecting particular patters (\eg edges) or can be sampled from a distribution, such as normal or uniform distribution or set to a constant like 0 or 1. The kernel values (when not pre-defined) are usually learnable parameters of a convolutional layer.

More concisely convolutional layer divide the input $X$ in to into groups of $k$ values. Each group is fed as input to a neuron $j_i$ from the convolutional layer, which performs a a convolution  $t_{conv}$ (see Section~\ref{section:conv}) outputs the dot product of the group input with the associated kernel $W$. The overall function the layer performs can be summarised by 

\begin{equation}
\operatorname{Conv}(X)= [t_{conv}(X[i \cdot s_x, j \cdot s_y, c])]_{i,j,c}
% \underset{m, n}{t_{conv}}(X_{i \cdot s_x+m, j \cdot s_y+n, k}, h)
\label{eq:conv_layer}
\end{equation}

where $X$ is the input, $(i,j)$ are the indices of the output, $c$ is the channel index, $s_x$ and $s_y$ are the stride values in the horizontal and vertical directions, respectively. Figure~\ref{Fig:conv} shows an example of the output of a convolutional layer.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/conv.pdf}
	\end{center}
	\caption{A convolutional transformation, where the $t_{conv}$ function is performed and the workings are shown on the first kernel-sized part of the input (in pink). The stride values $s_x$ and $s_y$ are set to $1$, which means the convolutional operation is performed to each part of the input that is $1$ position away from the previous part. In this example the bias term is set to zero.}
	\label{Fig:conv}
\end{figure} 

Another advantage of this layer is that the only parameters that need learning are the ones in the kernel and the same kernel is implied to different parts of the input. The reduction in parameters not only makes the network easier to train, but also helps to avoid overfitting to some extent.

\begin{Definition}{Convolutional Layer}{}
The \emph{convolutional function} $f_k:\bbR^{n_j}\to \bbR^{m_k}$, which describes the vector transformation associated with the layer $\lambda_k$, with the dimensionality conditions that $n > m$. The transfer function neurons in the layer performs is convolution in equation~\ref{eq:conv}. The activation function that follows is often ReLU~\eqref{eq:relu}, but could also be linear activation. This type of layer has an associated filter size and a stride and learnable kernel values $W$. 
\end{Definition}

\subsubsection{Batch Normalisation Layer}
\label{section:bn}
The purpose of batch normalisation is to address the issue of internal co-variate shift. Internal co-variate shift refers to the change in the distribution of layer inputs that occurs during the training process. Batch normalisation tackles this problem by first normalising the inputs to the layer and second scaling and shifting the normalised activations. This allows the model to learn the optimal scale and offset for each feature. The normalisation step is achieved by computing the mean and standard deviation of the activations within a mini-batch and using these statistics to normalise the activations. The normalisation is applied independently to each feature dimension, ensuring that the mean becomes zero and the standard deviation becomes one. The scale and shift operation is performed on the normalised activations and uses learned parameters called gamma and beta. 

In practice batch normalisation works differently during training and during inference. The mean and standard deviation for each mini-batch are calculated only during training, which sets the mean and the standard deviation of the activations precisely to 0 and 1 respectively. Meanwhile values for a typical mean and standard deviation are being learned and updated as each mini batch is passed through for training. Those values are then used during the forward pass for ease of computation.

\begin{Definition}{Batch Normalisation Layer}{}
The \emph{batch normalisation function} $f_k:\bbR^{n_j}\to \bbR^{m_k}$, which describes the vector transformation associated with the layer $\lambda_k$, with the dimensionality conditions that $n = m$. The transfer function is batch normalisation $t_{norm}$ and there is often no activation function that follows.
\end{Definition}


The batch normalisation layer $k\in \Lambda$ during the forward pass can be expressed as a vector-valued function $f_k:\bbR^{n_k}\to \bbR^{m_k}$ ($n_k=m_k$) given~by:
\begin{equation}
    f_k(\vec{x}) = \gamma \frac{(\vec{x}-\mu)}{\sqrt{\sigma^2 +\epsilon}} + \beta ,
\end{equation}
where $\frac{(\vec{x}-\mu)}{\sqrt{\sigma^2 +\epsilon}}$ normalises the input, the multiplication with $\gamma$ scales the values within the distribution, while the addition of $\beta$ shifts them. The normalisation component of the layer has an $\epsilon$ value which is a small constant to avoid division by zero and maintain numerical stability, $\mu$ is the learned mean and $\sigma$ is the learned standard deviation from all the examples seen during training.

\section{Conclusion}

Artificial neural networks are widely used for their computational capabilities --- they can learn and generalise even if they are trained on incomplete or conflicting data sets and do not need explicit human instructions. Importantly, neural networks possess remarkable fault tolerance once deployed. They can produce reliable outputs even if significant portions of the network are corrupted or lost. This makes them highly adaptable and useful in a range of applications where traditional algorithms fall short.

Starting with the underlying motivations and general structure, this chapter established the foundation for understanding how data is processed and transformed through the layers of a neural network. A detailed look into the constituent elements like neurons, layers, and activation functions shed light on their specific roles in the data transformation process.

In the following chapter, we will delve into explainability metrics, which are designed to address the issue of the nontransparent nature of what deep neural network learn. These metrics aim to provide insights not only into what the neural network has actually learned, but also how important the features are, offering a more comprehensive understanding of the model. Understanding what a model has learned is crucial for trust and validation of neural network models, particularly in critical applications where explainability is of paramount importance.