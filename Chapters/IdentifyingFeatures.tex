\chapter{Multi-faceted Clustering Approaches for Isolating Complex Input Features}
\chaptermark{Isolating Complex Input Features}
\label{chap:clustering}
\section{Introduction}

This chapter introduces two novel approaches that group input features into complex input features specifically tailored for image data, as defined in~\Cref{def:cifv}. This abstraction aligns closely with how humans naturally process visual information~\cite{fiantika2018internal}, thereby making the interpretation of neural networks more intuitive. By grouping pixels into meaningful patterns or regions, we enable the development of methods that assign a single relevance value to these complex input features as a whole, rather than treating each pixel individually. See Chapters~\ref{chapter:revLRP} and~\ref{chapter:REVEAL} for how to assign a single importance score to a complex input feature.

Isolating complex input features enhances neural network interpretability in image classification tasks by reducing the high dimensionality of pixel-level data and providing consolidated representations that highlight important visual patterns or regions. This approach simplifies user focus, thereby promoting standardised interpretations and minimising the risk of misinterpreting data due to noise or insignificant details. By offering a clearer and more distilled view of the image, users gain confidence in understanding the model's decisions, leading to more informed actions. Additionally, feature clustering simplifies comparisons between different models by focusing on a few key complex features rather than numerous individual pixels, aiding in understanding and identifying differences in model behaviour.

The contributions of this chapter are as follows:

\begin{enumerate} 
\item Provided a formal definition of \emph{Complex Input Features} for image data (\Cref{def:cifv}), establishing a foundation for subsequent methods.

\item Introduced a novel heatmap-based clustering approach using relevance heatmaps to detect and cluster coherent groups of pixels. The algorithm applies two stages of filtering to remove irrelevant features before employing DBSCAN for clustering. Evaluated the method qualitatively using various interpretability techniques and neural networks.

\item Developed an enhanced method combining object detection algorithms with the heatmap-based clustering approach to improve human interpretability. This method identifies distinct objects or regions within an image and selects the most relevant ones based on heatmap-derived importance. Proposed multiple augmentation steps to improve the applicability of object detection algorithms for interpretability tasks, and introduced three distinct techniques for integrating object detection with heatmap-based clustering. 
\end{enumerate}

The clustering techniques in this chapter aim to achieve a dual objective: minimise computational overhead while simultaneously augmenting the comprehensibility of interpretative mechanisms. The central premise is to focus on groups of pixels that collectively form features in the deeper layers of the network and contribute to the CNN's decision-making process.

\subsection{Complex Input Features}

The objective is to group pixels that are close in some feature space into meaningful clusters representing complex input features. We define a complex input feature vector to be a cluster of pixels from the input image, as:

\begin{Definition}{Complex Input Feature Vector}{cifv}
Given an input feature map (image) $F \in \mathbb{R}^{d_1 \times d_2 \times \ldots \times d_n}$ as defined in~\Cref{def:inputfeature}, where $d_1, d_2, \ldots, d_n \in \mathbb{N}$ are the dimensions along each axis of the tensor (e.g., height, width, and color channels), we define a \emph{complex input feature vector} of $F$ to be a subset $CF \subseteq \mathbb{R}^n$ encompassing some subset of pixels $CF \cap (d_1 \times d_2 \times \ldots \times d_n)$ from the input feature map.
\end{Definition}

In image data, individual pixels or simple edges alone often lack significant meaning until they form coherent shapes or objects. Clustering can help identify these higher-level visual patterns. For example, one complex input feature vector may correspond to the edges forming the contour of an object, while another might represent texture patterns, or even entire objects like the muzzle of a tiger. This grouping makes it easier to understand why an image was classified a certain way by the neural network. Since images are inherently high-dimensional, feature clustering reduces this complexity into a set of meta-features, simplifying the interpretability of the model's decision-making process.


\section{Heatmap-Based Clustering}
\label{heatmap_clustering}

\subsection{Motivation for Heatmap-Based Clustering}

Post-hoc locally interpretable methods function by identifying parts of the input which most affect the output. Pixels that are ascribed extreme relevance values are often indicative of their association with deeper learned feature vectors within the network's architecture. By directing attention to these specific pixels and their associated values, a more in-depth insight into the network's interpretative processes and decision-making mechanisms can be attained.

Input features get transformed and when they reach the deeper layers of the network they do not just function as individual entities, but rather operate together, forming interconnected feature vectors. When an input feature receives a high relevance score through a relevance propagation technique, it does not do so in isolation. Instead, this relevance is the result of the interaction the feature had with other input features during the forward pass that together lead to the activation of higher level concepts deeper in the network.

ERIC~\cite{townsend2020ericextractingrelationsinferred} further highlights the role of convolutional kernels, demonstrating that these kernels encapsulate semantically meaningful concepts and their interrelations. This underscores the importance of grouping features influenced by the same deep feature vector to reveal the network's reliance on symbolic patterns and abstract representations.

\begin{Conjecture}{Highly Relevant Input Clusters}{}
Input features with pronounced relevance scores (as attributed by an interpretation technique) and exhibit proximity in the feature space are conjectured to derive their relevance from the \textit{same} deep feature vector. 
\end{Conjecture} 

From a human perspective, individual pixels gain significance when collectively representing an entity. In CNNs, convolutional layers, as detailed in Section~\ref{sec:conv}, amalgamate activations of features within the same kernel. Based on this, we propose that when a heatmap contains a region of pronounced relevance, the significance attributed to this group of input features likely originates from the same deep feature vector. By grouping such features together the concept of feature hierarchy and dependency within the network is emphasised.

\subsection{Heatmap-Based Clustering Approach}

The heatmap-based clustering approach is achieved based on heatmap relevance scores. Notably, this method does not give equal weight to all pixels. Instead, it particularly prioritises those pixels with either notably high or significantly low relevance scores. The rationale behind this prioritisation lies in the behaviour observed within deeper network layers. By employing this clustering approach, the complex input features generated are a direct reflection of what the neural network deems relevant. More critically, they encapsulate input features that have had mutual interactions within the network. This methodology sidesteps human biases regarding perceived importance and centres exclusively on the network's intrinsic decision-making and feature relevance.

This understanding of pixel relevance can be further articulated mathematically. Consider an input to a convolutional neural network to be a \emph{($C$-channel) image} of dimensions \( H \times V \), described by a real-valued function \( I: H \times V \times C \to \mathbb{R} \), associating each colour channel \( c \in C \) of each pixel \( (x, y) \in H \times V \) with a colour value (typically in the range \([0,255]\)).

\begin{Definition}{Relevance Heatmap}{rh}
A relevance heatmap over an image \( I \) is a real-valued function \( \HM: \dom{I} \to \mathbb{R} \) over the domain of \( I \), describing the relevance values \( \vec{R}_0 \) associated with each pixel/input neuron of the input layer \( 0 \in \Lambda \).
\end{Definition}

Given Definition~\ref{def:rh}, we may define a three-dimensional point space, described as:
\begin{equation*}
    S = \big\{ (x, y, \max_{c \in C} |\HM(x, y, c)| ) : x \in H, y \in V \big\} \subseteq \mathbb{R}^3
\end{equation*}

Each pixel coordinate \( (x, y) \) within the dimensions \( H \times V \), has its position elevated based on the maximum absolute relevance it holds across all colour channels. This process essentially 'lifts' the pixel into a three-dimensional space, represented as \( \mathbb{R}^3 \). In Figure~\ref{Fig:3D}(A), an image of a chimpanzee is shown. The relevance values \( \vec{R}_0 \) associated with each pixel in the image's three colour channels are determined, leading to the creation of three heatmaps, as seen in Figure~\ref{Fig:3D}(B). The max value for each colour channel is then extracted, resulting in a single heatmap as displayed in Figure~\ref{Fig:3D}(C). The heatmap can subsequently be represented in three dimensions by lifting each pixel based on its importance value, as depicted in Figure~\ref{Fig:3D}(D).

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/3D_plot.pdf}
	\end{center}
	\caption{In the presented figure: (A) showcases an image of a chimpanzee; (B) depicts the relevance values of the input, which are determined using layer-wise relevance propagation in the context of the VGG16 classification; (C) displays a consolidated heatmap formed by extracting the maximum values from each pixel in the initial three-channel allocation of relevances; (D) represents this heatmap in a three-dimensional point space.}
	\label{Fig:3D}
\end{figure}

Once the pixel relevances are transformed into this 3D point distribution, the aim is to separate the space into clusters of pixels that are not only in close spatial proximity but also exhibit similar relevance values. This allows for the identification of complex input features which have a high probability of being identified together from the same deeper layer feature vector.

A drawback of adopting a simplistic approach of clustering with an existing algorithm is computational complexity. Clustering algorithms struggle with efficiency due to the vast volume and high dimensionality of the data. Recognising these limitations, an alternative approach becomes necessary to enhance the scalability and effectiveness of the process. Therefore, first a filtering of the unimportant pixels is employed (see Section~\ref{sec:filtering}), followed by a clustering over the selected pixels (see Section~\ref{sec:clustering}).

\section{Filtering Relevances}
\label{sec:filtering}

To address computational complexity and poor scalability, a filtering of the input data is proposed before clustering. This filtering reduces the dataset to a manageable size by retaining only pixels with highly positive or negative relevance values, ensuring that clustering focuses on truly meaningful data points. This approach not only improves clustering efficiency but also yields more relevant and informative results.

\subsection{Threshold}

In this stage, areas of the relevance heatmap that do not meet a minimum relevance threshold, defined as \( \theta > 0 \), are discarded. Specifically, pixels for which the maximum relevance across all channels \( \max_{c \in C} |\HM(x, y, c)| \geq \theta \) (as defined in Definition~\ref{def:rh}) are retained, while the rest are filtered out. The threshold \( \theta \) is determined by the \( p \)th percentile of the relevance values, meaning that \( p\% \) of the relevance points are below or equal to \( \theta \), while \( (100 - p)\% \) are greater than or equal to it.

The choice of the \( p \) value is crucial in determining which pixels are retained for clustering. However, identifying an optimal \( p \) value is challenging due to the varying distribution of relevance scores across different inputs. A lower \( p \) value is effective for inputs rich in features, as it allows the inclusion of most significant regions. However, in feature-sparse inputs, a low \( p \) may retain too many irrelevant pixels, complicating the clustering process by adding unnecessary noise.

On the other hand, higher \( p \) values focus solely on the most dominant features, potentially overlooking subtler yet important regions. For example, in Figure~\ref{Fig:Fisher}, at \( p = 98\% \) (C), a smaller spider is excluded due to its lower number of relevant pixels. When inputs have more evenly distributed relevance scores, higher \( p \) values can create sparse and fragmented heatmaps, as shown in Figure~\ref{Fig:Fisher} at \( p = 98\% \) (D, E), leading to incomplete representation of relevant pixels within objects, which complicates the clustering process.

Rather than aiming for a universally optimal \( p \) value—which may be impossible even within a single input, where some areas are densely packed with features while others have few—the method proposed in this thesis adopts a more flexible approach. It starts with a lower \( p \) value of 90 to capture a broader set of relevant pixels and then applies further filtering using Fisher-Jenks Natural Breaks to refine the selection.

\subsection{Fisher-Jenks Natural Breaks}
\label{sec:fjnb}

To further increase input in the denser portions of the image, the method segments the input into \( n \) number of regions. Each region vector \( \HMhat_i: U_i \to \mathbb{R} \), where \( \HMhat_i(x, y) = \max_{c \in C} |\HM(x, y, c)| \) is the maximum absolute relevance across all colour channels for the pixel in position \( (x, y) \). 

Each region \( U_i \) is further divided into a tuple of subsets \( (U_{i1}, \dots, U_{i\ell}) \), for some \( \ell \geq 1 \), where the highest values in region \( U_i \) are in the subset \( U_{i1} \). The segmentation or division of \( U_i \) into these smaller sections is done using a method called the Fisher-Jenks natural breaks algorithm~\cite{fisher1958grouping}. This algorithm is a well-known method used to classify or segment data into natural classes or bins. Its main purpose is to determine the best arrangement of values into different classes in such a way that the variance within each class is minimised.

The primary objective of the method is to minimise the sum of the squared deviations from the mean in each subset. Consider each \( U_{ij} \) subset having its own mean value \( \mu_{ij} \). The squared deviation from the mean for a data point in this subset is the square of the difference between the data point and \( \mu_{ij} \). Calculating the squared deviation for all data points within \( U_{ij} \) and summing them up results in the total squared deviations from the mean. Using the algorithm, the resulting subsets \( (U_{i1}, \dots, U_{i\ell}) \) are such that the total of the sum of the squared deviations from the mean \( \mu_{ij} \) for each subset \( U_{ij} \) is minimised, across all \( 1 \leq j \leq \ell \).

We define a set of points for each region \( U_i \) that are in the subset \( U_{i1} \) and are above the \( p \)th percentile value:
\begin{equation*}
    U_{i_p} = \left\{ (x, y) \in U_{i1} \mid \HMhat(x, y) \geq p_{\text{value}} \right\},
\end{equation*}
where \( p_{\text{value}} \) represents the value at the \( p \)th percentile for \( \HMhat \) over the entire input \( I \).

In Figure~\ref{Fig:Fisher}, we present two distinct regions form an input image. Initially, these regions underwent segmentation via the Fisher-Jenks natural breaks algorithm. Subsequently, a segmentation based on the \( p_{\text{value}} \), corresponding to the 90th percentile of the input values, was executed. For the first region, the input content of high relevance is limited. As a result, even though a significant portion of the input is classified into the top segment post the Fisher-Jenks application, the non-significant values are eliminated when filtered by the \( p_{\text{value}} \). This outcome essentially mirrors the result of directly selecting the top 90 percentile. Contrarily, in the second region, a substantial number of pixels surpass the top 90 percentile threshold. Hence, applying the Fisher-Jenks natural breaks to this segment results in more dispersed regions.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/Fisher.pdf}
	\end{center}
	\caption{This figure illustrates the effect of selecting a \( p \) value in combination with Fisher-Jenks natural breaks.}
	\label{Fig:Fisher}
\end{figure}

Next, we consider the space of points \( S_{i\HM} \subseteq \mathbb{R}^4 \), under the Euclidean metric, given by
\begin{equation*}
    S_{i\HM} = \left\{ (x, y, c, a \cdot \HMhat(x, y)) \mid (x, y) \in U_{i_p} \right\},
\end{equation*}
where \( a > 0 \) is a hyper-parameter that can be tuned to linearly scale the relative contributions of spatial distance and relevance; lower values of \( a \) will prioritise spatial similarity, while higher values of \( a \) will prioritise similar relevance scores. We define the space of points \( S_{\HM} \subseteq \mathbb{R}^4 \), under the Euclidean metric, to be the union of all \( S_{i\HM} \), defined as:
\begin{equation}
    S_{\HM} = \bigcup_{i=1}^{n} S_{i\HM}
    \label{eq:set_of_points}
\end{equation}

It represents all the points that the combined method takes into account. This set of points contains the highest or lowest relevance points within each of the regions that also conform to being in the top \( p \)th percentile.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Figures/NB.pdf}
	\end{center}
	\caption{This image showcases four contrasting outputs derived from different methods: the low \( p \)th percentile values, the high \( p \)th percentile values, the Fisher-Jenks natural breaks, and their combined technique.}
	\label{Fig:NB}
\end{figure}

In Figure~\ref{Fig:NB}, we show the results of using the \( p \)th percentile values, Fisher-Jenks natural breaks, and their combined approach. The combined technique shows clear improvements over using only the \( p \) value. While Fisher-Jenks avoids overly concentrated or sparse outputs, it sometimes selects features that are not globally relevant, as seen in Figure~\ref{Fig:NB} (A) and (D), where less important areas still have selected pixels. The combined approach ensures that only pixels from the 90th percentile are retained while de-noising feature-dense regions.


\subsection{Filtering Algorithm Pseudo-code}

% The pseudo-code for the filtering process can be seen here. This function reduces the dataset to a manageable size by retaining only pixels with highly positive or negative relevance values, ensuring that clustering focuses on truly meaningful data points.


\begin{algorithm}[ht]
\caption{Filtering Relevances}
\begin{algorithmic}[1]
\Require Relevance heatmap \( \HM \), percentile threshold \( p \), number of regions \( n \)
\Ensure Filtered set of points \( S_{\HM} \)
\Procedure{FilterRelevances}{}
    \State Compute the maximum absolute relevance per pixel:
    \ForAll{\( (x, y) \in \dom{I} \)}
        \State \( \HMhat(x, y) \gets \max_{c \in C} |\HM(x, y, c)| \)
    \EndFor
    \State Calculate the \( p \)th percentile value \( p_{\text{value}} \) of \( \HMhat \) over the entire input.
    \State Divide the image into \( n \) regions \( \{ U_1, U_2, \dots, U_n \} \).
    \State Initialize \( S_{\HM} \gets \emptyset \)
    \Comment{Prepare to collect the filtered points}
    \For{\( i \gets 1 \) to \( n \)}
        \Comment{Loop through each region \( U_i \)}
        \State Apply Fisher-Jenks natural breaks to \( \HMhat \) in region \( U_i \)
        \Comment{Partition the region based on relevance values}
        
        \State Obtain relevance subsets \( (U_{i1}, \dots, U_{i\ell}) \) using Fisher-Jenks.
        
        \State Select the subset with the highest relevance \( U_{i1} \).
        \Comment{Choose the top segment with the highest relevance scores}
        
        \State Define \( U_{i_p} \gets \{ (x, y) \in U_{i1} \mid \HMhat(x, y) \geq p_{\text{value}} \} \).
        \Comment{Keep only pixels above the relevance threshold \( p_{\text{value}} \)}
        
        \ForAll{\( (x, y) \in U_{i_p} \)}
            \Comment{Convert each selected pixel into a 4D point}
            \State \( S_{i\HM} \gets (x, y, c, a \cdot \HMhat(x, y)) \)
        \EndFor
        
        \State Add all 4D points from region \( U_i \) to the final set \( S_{\HM} \).
        \State \( S_{\HM} \gets S_{\HM} \cup S_{i\HM} \)
    \EndFor
    \State \Return \( S_{\HM} \)
\EndProcedure
\end{algorithmic}
\end{algorithm}
% \begin{algorithm}[H]
% \caption{Filtering Relevances (Part 1)}
% \begin{algorithmic}[1]
% \Require Relevance heatmap \( \HM \), percentile threshold \( p \), number of regions \( n \)
% \Ensure Filtered set of points \( S_{\HM} \)
% \Procedure{FilterRelevances}{}
%     \State Compute the maximum absolute relevance per pixel:
%     \ForAll{\( (x, y) \in \dom{I} \)}
%         \State \( \HMhat(x, y) \gets \max_{c \in C} |\HM(x, y, c)| \)
%     \EndFor
%     \State Calculate the \( p \)th percentile value \( p_{\text{value}} \) of \( \HMhat \) over the entire input.
%     \State Divide the image into \( n \) regions \( \{ U_1, U_2, \dots, U_n \} \).
%     \State Initialize \( S_{\HM} \gets \emptyset \)
%     \Comment{Prepare to collect the filtered points}
%     \EndProcedure {(Continued on next page)}
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[H]
% \caption{Filtering Relevances (Part 2)}
% \begin{algorithmic}[1]
% \For{\( i \gets 1 \) to \( n \)}
%     \Comment{Loop through each region \( U_i \)}
%     \State Apply Fisher-Jenks natural breaks to \( \HMhat \) in region \( U_i \)
%     \Comment{Partition the region based on relevance values}
    
%     \State Obtain relevance subsets \( (U_{i1}, \dots, U_{i\ell}) \) using Fisher-Jenks.
    
%     \State Select the subset with the highest relevance \( U_{i1} \).
%     \Comment{Choose the top segment with the highest relevance scores}
    
%     \State Define \( U_{i_p} \gets \{ (x, y) \in U_{i1} \mid \HMhat(x, y) \geq p_{\text{value}} \} \).
%     \Comment{Keep only pixels above the relevance threshold \( p_{\text{value}} \)}
    
%     \ForAll{\( (x, y) \in U_{i_p} \)}
%         \Comment{Convert each selected pixel into a 4D point}
%         \State \( S_{i\HM} \gets (x, y, c, a \cdot \HMhat(x, y)) \)
%     \EndFor
    
%     \State Add all 4D points from region \( U_i \) to the final set \( S_{\HM} \).
%     \State \( S_{\HM} \gets S_{\HM} \cup S_{i\HM} \)
% \EndFor
% \State \Return \( S_{\HM} \)
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

\section{Clustering}
\label{sec:clustering}

After filtering, only the most relevant input features remain, and a clustering algorithm is needed to identify complex input features as defined in Definition~\ref{def:cifv}. This thesis proposes partitioning the set \( S_{\HM} \) into \( k \) clusters, denoted \( (C_1, \dots, C_k) \), where \( k \geq 1 \). The clustering algorithm used in the examples is \DBSCAN\ (Density-Based Spatial Clustering of Applications with Noise)~\cite{EsterKSX96}.

\DBSCAN\ has several advantages over traditional clustering methods. First, it does not require the number of clusters to be specified in advance, which is ideal since the number of features varies depending on the input. Second, \DBSCAN\ can identify clusters of arbitrary shapes, offering flexibility, unlike many algorithms that assume clusters are centered around a centroid, which can be limiting for image features that do not follow simple shapes. Additionally, \DBSCAN\ has a built-in mechanism to handle noise, allowing it to differentiate between core points, border points, and noise.

The primary drawback of \DBSCAN\ is its poor scalability. However, this limitation is mitigated by applying it only to the most relevant pixels, which typically account for less than 5\% of the total input. This ensures the algorithm remains efficient in practice. In Figure~\ref{Fig:DBSCAN}, the performance of \DBSCAN\ is shown on top of the pixels selected by the combined approach of taking the top 90th percentile and Fisher-Jenks on the input.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.85\linewidth]{Figures/DBSCAN.pdf}
	\end{center}
	\caption{Clusters found by applying DBSCAN on top of the selected parts of the input from the combined approach of the 90th percentile and Fisher-Jenks natural breaks.}
	\label{Fig:DBSCAN}
\end{figure}
\subsection{Clustering Pseudo-code}

After the mathematical definitions, we provide the pseudo-code for the clustering process.

\begin{algorithm}[H]
\caption{Clustering Filtered Relevances}
\begin{algorithmic}[1]
\Require Filtered set of points \( S_{\HM} \), \DBSCAN\ parameters \( \epsilon \), \textit{min\_samples}
\Ensure Clusters \( \mathcal{C} = \{ C_1, C_2, \dots, C_k \} \)
\Procedure{ClusterRelevances}{}
    \State Apply \DBSCAN\ to \( S_{\HM} \) with parameters \( \epsilon \), \textit{min\_samples}.
    \State Obtain clusters \( \mathcal{C} = \{ C_1, C_2, \dots, C_k \} \).
    \State \Return \( \mathcal{C} \)
\EndProcedure
\end{algorithmic}
\end{algorithm}


To conclude, we present the complete algorithm that combines the filtering and clustering processes described above.

\begin{algorithm}[H]
\caption{Heatmap-Based Clustering Approach}
\begin{algorithmic}[1]
\Require Image \( \mathbf{I} \in \mathbb{R}^{H \times W \times C} \), percentile threshold \( p \), number of regions \( n \), scaling factor \( a \), \DBSCAN\ parameters \( \epsilon \), \textit{min\_samples}
\Ensure Clusters \( \mathcal{C} = \{ C_1, C_2, \dots, C_k \} \)
\Procedure{HeatmapBasedClustering}{}
    \State Compute the relevance heatmap \( \HM \) for the image \( \mathbf{I} \).
    \State Apply \textsc{FilterRelevances} to obtain the filtered set of points \( S_{\HM} \).
    \State Apply \textsc{ClusterRelevances} to \( S_{\HM} \) to obtain clusters \( \mathcal{C} \).
    \State \Return \( \mathcal{C} \)
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Comparison based on the neural network}

When evaluating feature relevance in a given input, it is important to recognise that different deep neural networks (DNNs) can interpret and classify data in distinct ways due to variations in their architecture and depth. This section compares heatmap generation across three prominent DNN architectures: ResNet50, Inception V3, and VGG16, highlighting both the differences and similarities observed.

\begin{figure}[ht!]
	\begin{center}
\includegraphics[width=\textwidth]{Figures/CLUSTERING_NETWORKS.pdf}
\end{center}
\caption{Heatmap comparisons for a chimpanzee image across DNN architectures: ResNet50, Inception V3, and VGG16. While all networks correctly identify the chimpanzee, the areas deemed relevant vary across architectures, highlighting the impact of network design on feature interpretation.}
\label{Fig:CLUSTERING_NETWORKS}
\end{figure}

ResNet50's residual connections enable it to train deeper networks without vanishing gradients, while Inception V3's wider architecture offers greater capacity for memorising inputs. VGG16, known for its simplicity, provides a different perspective in comparison. Despite all networks correctly classifying a chimpanzee image, Figure~\ref{Fig:CLUSTERING_NETWORKS} reveals notable differences in which parts of the image each network considers relevant. 

Interestingly, while architectural differences affect feature relevance to some extent, the choice of heatmap generation method has a far greater impact on the interpretation of relevance. This suggests that the method used for generating heatmaps plays a more critical role than the differences between network architectures.

The method’s effectiveness in identifying key features depends on both the DNN used and the heatmap generation technique. While the results may vary across configurations, the approach provides valuable insights into the network’s decision-making. 

\section{Object Detection and Heatmap Clustering Approach}

Heatmap clustering approach allows for the identification of regions that are important for the network given a relevance propagation approach. However, challenges arise when extracted features are significant to the network but not easily interpretable, indicating the need for more advanced interpretability techniques beyond basic feature extraction.

The Object Detection and Heatmap Clustering Approach emerges as a response to the challenge of interpretability in feature extraction. This section delves into how this approach not only identifies key features in images but also represents them in a manner that enhances one's understanding of what the network considers important. By integrating object detection principles with heatmap clustering, the methodology provides a clearer, more comprehensive view of how networks process and prioritise information. 

\subsection{Segment Anything Model (SAM)}
To bridge this gap between significant feature extraction and human interpretability, we propose the use of object detection. The method used in this chapter is "Segment Anything Method" (\SAM)~\cite{Kirillov2023SegmentA}. \SAM\ focuses on breaking down the input into segments or regions in a manner that aligns with human understanding. By doing so, \SAM\ translates the intricate and sometimes abstract features highlighted by the network into more understandable visual components that are recognisable to human observers. 

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=\textwidth]{Figures/Clusters_SAM.pdf}
\end{center}
\caption{Demonstration of the advanced object identification technique \SAM\/, showcasing the segmentation of images with 18 to over 170 distinct objects in six different inputs.}
\label{Fig:Sam_many_masks}
\end{figure} 

A challenge that emerges by using SAM is the volume of objects that can be present in a single image. This problem is vividly illustrated in Figure~\ref{Fig:Sam_many_masks}, where six inputs are shown, varying from 18 to over 170 distinct objects in a single input. In these instances, the presentation of such a vast number of objects becomes problematic, particularly in terms of human interpretability and visual clarity. The high number of objects leads to indistinguishable colours, making it difficult to differentiate between objects and correlate them with the legend, especially when assisted contribution values are considered (See  Section~\ref{chapter:revLRP} and Section~\ref{chapter:REVEAL} for how relevance values are assigned to complex input features).

This challenge highlights a disconnect between advanced image processing algorithms and human cognitive limits. While computer vision systems can segment many objects, presenting this data in a human-readable way remains difficult.

A practical solution is to simplify by displaying only the top 5 largest objects, reducing cognitive load and focusing on the most prominent elements. However, this approach does not always align size with object relevance, as shown in Fig~\ref{Fig:SAM_nor_right}, where important objects identified by relevance detection differ from the largest.

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=\textwidth]{Figures/SAM_SELECTION.pdf}
\end{center}
\caption{Only the largest 5 object selected in each input image. This can leads to not selecting the top most relevant objects in the image.}
\label{Fig:SAM_nor_right}
\end{figure} 

The Segment Anything Method (\SAM) excels in segmenting objects, while heatmap clustering highlights key features for neural network decision-making. This chapter proposes integrating \SAM\ with heatmap clustering to link interpretable object masks with deep network features. Some refinements are needed to ensure compatibility and effectiveness.

\subsection{Preprocessing the output of SAM for Integration with Heatmap Clustering}

Before fully examining the integration between SAM and Heatmap Clustering (discussed in Section~\ref{sec:integration}), it is essential to undertake a series of preprocessing steps. These steps are aimed at overcoming certain inherent limitations of \SAM\ and ensures that the object masks $\mathcal{M}_1$ generated by \SAM\ are adequately prepared and formatted to complement and enhance the insights provided by the set of heatmap clustering masks $\mathcal{M}_2$.

\subsubsection{Generating Masks for Undetected regions}

A key limitation of SAM is its occasional failure to detect certain parts of an image, potentially missing critical areas. To address this, the first preprocessing step is to scan the entire image to identify self-contained regions that SAM might have overlooked.

Let $\mathcal{M}_1$ represent the set of binary masks produced by SAM, where $\mathcal{M}_1 = \{M_1^1, M_1^2, \ldots, M_1^k\}$, with each $M_1^i$ being a binary matrix (size $n \times m$), where $1$ indicates detected objects and $0$ represents undetected regions. The sum of all masks is calculated as:
\begin{equation*}
S_1 = \sum_{i=1}^{k} M_1^i
\end{equation*}
where \(S_1\) shows the total coverage of detected objects, with zero entries marking areas missed by SAM.

Next, a complementary mask \( \overline{M} \) is created to highlight these undetected regions:
\begin{equation*}
\overline{M} = 1 - \min(S_1, 1)
\end{equation*}
This operation ensures \( \overline{M} \) marks undetected areas, using connected component analysis to identify distinct regions. Each component \(C_j\) forms a new mask $M'_j$ defined as:
\begin{equation} 
M'_{j,ij} = \begin{cases} 
   1 & \text{if } (i, j) \text{ belongs to } C_j \\
   0 & \text{otherwise} 
   \end{cases}
\end{equation}

The process yields new masks \( \{M'_1, M'_2, \ldots, M'_p\} \) that cover previously undetected regions. This complements SAM’s initial segmentation, ensuring comprehensive coverage of the image.

To manage the number of masks, a size threshold \( \tau \) (set at $0.1\%$ of the input image size $I$) is introduced:
\begin{equation} 
\tau = 0.001I 
\end{equation}
Only masks corresponding to connected components larger than this threshold are retained, resulting in a curated set of significant masks. The final set \( \mathcal{M}_1 = \{M_1^1, M_1^2, \ldots, M_1^n\} \) includes both SAM's original masks and the newly generated ones, ensuring comprehensive detection.

\begin{algorithm}[ht]
\caption{Generate Masks for Undetected Regions}
\begin{algorithmic}[1]
\Require Set of SAM masks $\mathcal{M}_1 = \{M_1^1, M_1^2, \ldots, M_1^k\}$, Image size $I$
\Ensure Updated set of masks $\mathcal{M}_1$ with new masks for undetected regions
\State Initialize $S_1 \gets 0$ matrix of size of the image
\For{each mask $M_1^i$ in $\mathcal{M}_1$}
    \State $S_1 \gets S_1 + M_1^i$
\EndFor
\State Compute the complementary mask: $\overline{M} \gets 1 - \min(S_1, 1)$
\State Identify connected components in $\overline{M}$: $CC \gets \text{ConnectedComponents}(\overline{M})$
\State Initialize $\text{new\_masks} \gets \emptyset$
\For{each connected component $C_j$ in $CC$}
    \If{$\text{Size}(C_j) \geq \tau$}
        \State Create new mask $M'_j$ where $M'_j(p, q) = 1$ if $(p, q) \in C_j$, else $0$
        \State Add $M'_j$ to $\text{new\_masks}$
    \EndIf
\EndFor
\State Update $\mathcal{M}_1 \gets \mathcal{M}_1 \cup \text{new\_masks}$
\State \Return $\mathcal{M}_1$
\end{algorithmic}
\end{algorithm}

\subsubsection{Enhancing Object Detection with Edge Consideration}

CNNs often emphasize edges as critical features during image analysis, whereas SAM typically segments objects without considering edge details. To better align SAM's segmentation with the neural network's focus, the boundaries of each object mask are expanded by approximately $0.001\%$ in all directions. This expansion incorporates edge features, making the segmented objects more consistent with CNN-detected patterns.

The dilation process expands the regions of interest (the '1' values in $M$) by convolving the mask with a square matrix $K$ of ones, where $K$ has dimensions $p \times p$ and $p$ is $0.001\%$ of the input image size. This operation effectively broadens the area around the original points of interest, capturing edge details (see Section~\ref{sec:conv} for convolution details).

\begin{algorithm}[H]
\caption{Enhance Masks with Edge Consideration}
\begin{algorithmic}[1]
\Require Set of masks $\mathcal{M}_1 = \{M_1^1, M_1^2, \ldots, M_1^n\}$, Expansion size $p$
\Ensure Expanded set of masks $\mathcal{M}_1$
\State Create a structuring element $K$ of ones with size $p \times p$
\For{each mask $M_1^i$ in $\mathcal{M}_1$}
    \State $M_1^i \gets \text{Dilate}(M_1^i, K)$
\EndFor
\State \Return $\mathcal{M}_1$
\end{algorithmic}
\end{algorithm}


\subsubsection{Resolving Overlapping Clusters}

Overlapping segments in the segmentation process can create redundant information and increase complexity. SAM may detect the same object multiple times with slight variations. 

An iterative algorithm is applied to refine the set of masks, ensuring each represents a distinct image segment. The refinement starts with the largest mask and progressively removes overlapping regions. Given a set of masks \(\mathcal{M}_1 = \{M_1^1, M_1^2, \ldots, M_1^n\}\), where each is a binary matrix, the masks are ordered by size (the number of '1's). The largest mask, \(M_1^1\), is used as the primary mask, and in each iteration, \(M_1^i\) is refined by removing overlaps with all larger masks \(M_1^j\) (where \(j > i\)) using bitwise operations:
\begin{equation}
M_1^i \leftarrow M_1^i \oplus (M_1^i \land M_1^j),
\end{equation}
where \( \oplus \) and \( \land \) are the XOR and AND operations, respectively.


\begin{algorithm}
\caption{Resolve Overlapping Clusters}
\begin{algorithmic}[1]
\Require Set of masks $\mathcal{M}_1 = \{M_1^1, M_1^2, \ldots, M_1^n\}$
\Ensure Refined set of masks $\mathcal{M}_1$ without overlaps
\State Sort $\mathcal{M}_1$ in descending order based on the size of masks (number of ones)
\For{$i = 1$ to $n$}
    \For{$j = i+1$ to $n$}
        \State $\text{Overlap} \gets M_1^i \land M_1^j$ \Comment{Bitwise AND}
        \If{any overlap exists}
            \State $M_1^j \gets M_1^j \oplus \text{Overlap}$ \Comment{Remove overlapping pixels using XOR}
        \EndIf
    \EndFor
\EndFor
\State \Return $\mathcal{M}_1$
\end{algorithmic}
\end{algorithm}


This hierarchical process ensures each mask is refined to eliminate overlaps with smaller masks.

\subsection{Integration between SAM and Heatmap Clustering}
\label{sec:integration}
We propose a methodological framework to analyse the intersections between two distinct sets of masks, \(\mathcal{M}_1\) (from post-processed SAM) and \(\mathcal{M}_2\) (from Heatmap Clustering). This approach identifies the most relevant objects in an image, prioritising significance over size.
 
Large objects in an image may not necessarily be the most informative or relevant for a given analysis. For instance, in medical imaging, a small anomaly might be of far greater significance than larger but normal anatomical structures. In Figure~\ref{Fig:SAM_nor_right} the top 5 objects by size post-processing are shown, one can notice that the objects selected by size do not coincide with the areas found by the heatmap clustering. The main objective of the combination between the heatmap clustering and SAM is to select the top objects that the network finds relevant rather than selecting them by size.

% \begin{figure}[ht!]
% \begin{center}
% \includegraphics[width=\textwidth]{Figures/by_size.pdf}
% \end{center}
% \caption{Illustration of the top 10 objects identified by size in a sample image. This highlights the need for more nuanced selection criteria, as larger objects may not correspond to the most relevant or informative elements for analysis.}
% \label{Fig:large}
% \end{figure} 


\subsubsection{Sum of Overlaps} 

The approach measures the total intersection area between each mask in \(\mathcal{M}_1\) and all masks in \(\mathcal{M}_2\), evaluating how much each mask in \(\mathcal{M}_1\) overlaps with \(\mathcal{M}_2\). This method focuses on total overlap rather than the size of individual masks, making it suitable when overall coverage is more important than specific pairings.

For each mask \( M_1^i \) in \(\mathcal{M}_1\), the overlap is calculated by summing pixel-wise logical AND operations with each mask \( M_2^j \) in \(\mathcal{M}_2\):
\begin{equation*}
    O_1^i = \sum_{j} \sum_{k, l} \left( M_1^i(k, l) \land M_2^j(k, l) \right)
\end{equation*}
Masks in \(\mathcal{M}_1\) are then ranked by their overlap sum \( O_1^i \).


Figure~\ref{Fig:most_relevant} shows the primary regions identified by this method, which tends to select larger regions as they cover more points highlighted by the heatmap clustering. While larger regions are often chosen, the method also effectively identifies those most relevant to the heatmap's indications.
\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.9\textwidth]{Figures/summs_of_overlaps.pdf}
\end{center}
\caption{Depicted here are the key regions identified using the sum of overlapping areas method. This approach inherently gravitates towards larger regions, as they generally encompass more points from the heatmap clustering maps.}
\label{Fig:most_relevant}
\end{figure} 

\subsubsection{Maximum Percentage Overlap with $\mathcal{M}_1$} 
To overcome the limitations of focusing on total overlap, this method evaluates the proportion of each mask in \(\mathcal{M}_1\) that achieves maximal overlap with any single mask in \(\mathcal{M}_2\). This helps determine how well each object mask in \(\mathcal{M}_1\) is covered by a feature mask in \(\mathcal{M}_2\), making it ideal for assessing the completeness of coverage.

For each mask \( M_1^i \), calculate the percentage of its area that overlaps with each mask \( M_2^j \), and take the maximum:
\begin{equation*}
    P_1^i = \max_j \left( \frac{\sum_{k, l} \left( M_1^i(k, l) \land M_2^j(k, l) \right)}{\sum_{k, l} M_1^i(k, l)} \right)
\end{equation*}
The masks in \(\mathcal{M}_1\) are then ranked by their maximum percentage overlap \( P_1^i \).

Figure~\ref{Fig:covarage_mask1} shows the regions identified using this method, which tends to prioritise smaller areas. Smaller regions are more likely to align closely with heatmap clustering, leading to a strong correlation with the clustering outputs. The precision of this method is evident in the selected regions, which accurately reflect the spatial patterns highlighted by the heatmap clustering.

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=\textwidth]{Figures/covarage_mask1.pdf}
\end{center}
\caption{This figure displays the selected regions identified by the method prioritising maximum percentage overlap of the masks from the heatmap-based clustering with those in $\mathcal{M}_1$. Notably, it reveals a tendency to highlight smaller regions, which is attributable to the proportionally higher coverage these regions achieve in alignment with the heatmap clustering.}
\label{Fig:covarage_mask1}
\end{figure} 

\subsubsection{Maximum Percentage Overlap with $\mathcal{M}_2$} 

This method reverses the perspective of the "Maximum Percentage Overlap with \(\mathcal{M}_1\)". It focuses on how well each heatmap-based cluster from \(\mathcal{M}_2\) is encapsulated by a single mask in \(\mathcal{M}_1\).

The maximum percentage overlap for each \( M_1^i \) is defined as:
\begin{equation*}
P_2^i = \max_j \left( \frac{\sum_{k, l} \left( M_1^i(k, l) \land M_2^j(k, l) \right)}{\sum_{k, l} M_2^j(k, l)} \right)
\end{equation*}
After calculating \( P_2^i \) for all \( i \), masks in \(\mathcal{M}_1\) are ranked by their coverage of \(\mathcal{M}_2\).

Figure~\ref{Fig:covarage_mask2} illustrates the regions identified using this method. It can detect objects that closely match heatmap clusters or larger objects that fully encapsulate the clusters, even if they partially overlap. This duality sometimes leads to selecting larger regions that align with the clusters but may introduce a discrepancy between the scale and specificity of the identified areas

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=\textwidth]{Figures/covarage_mask2.pdf}
\end{center}
\caption{This figure illustrates the regions identified by emphasising the complete encapsulation of heatmap-based clusters by individual object masks in $\mathcal{M}_1$. It highlights the method's ability to accurately align with certain heatmap clusters while also indicating instances where larger $\mathcal{M}_1$ objects are selected for completely encompassing smaller heatmap clusters.}
\label{Fig:covarage_mask2}
\end{figure} 

\subsubsection{Combined Average Overlap Percentage}

This method offers a balanced metric that considers the overlap percentages between both \(\mathcal{M}_1\) and \(\mathcal{M}_2\), providing a bidirectional analysis. It accounts for how much each \(\mathcal{M}_1\) mask is covered by \(\mathcal{M}_2\) and vice versa, ensuring neither set dominates the outcome. This enables a more comprehensive interpretation of the overlap between heatmap clusters in \(\mathcal{M}_2\) and SAM object masks in \(\mathcal{M}_1\). For each mask \( M_1^i \), the average of the overlap percentages relative to both \( M_1^i \) and \( M_2^j \) is calculated:
\begin{equation*}
C^i = \max_j \left( \frac{1}{2} \left[ \frac{\sum_{k, l} \left( M_1^i(k, l) \land M_2^j(k, l) \right)}{\sum_{k, l} M_1^i(k, l)} + \frac{\sum_{k, l} \left( M_1^i(k, l) \land M_2^j(k, l) \right)}{\sum_{k, l} M_2^j(k, l)} \right] \right)
\end{equation*}

Figure~\ref{Fig:covarage_masks} shows the regions identified by this method, balancing precision and comprehensiveness. It highlights highly precise areas with strong alignment to heatmap clusters, as well as larger regions that provide broader coverage, encapsulating key areas. This approach captures both detailed and extensive features relevant to the analysis.
\begin{figure}[ht!]
\begin{center}
\includegraphics[width=\textwidth]{Figures/covarage_masks.pdf}
\end{center}
\caption{This figure illustrates regions selected by the combined average overlap percentage method, showcasing a balanced blend of precision and comprehensiveness.}
\label{Fig:covarage_masks}
\end{figure} 



\section{Conclusion}

This chapter has rigorously explored the challenge of isolating complex input features through clustering techniques on top of heatmaps. 

The heatmap clustering methodology extracts features that are deemed crucial by the network for its decision-making process. This direct link to the network’s internal representations provides insights into its classification mechanisms. However, a significant challenge arises when the extracted features, although vital for the network, lack immediate interpretability. This necessitates a more sophisticated approach that extracts comprehensible data points.

The latter part of the chapter combines object detection with the heatmap clustering approach. This not only pinpoints key features within images but also presents them in a way that significantly enhances our understanding of the aspects deemed important by the network.

The examples provided throughout this chapter serve primarily as illustrations to demonstrate the capabilities of the proposed approach. While they showcase the effectiveness of integrating object detection with heatmap clustering for feature isolation and interpretability, they do not constitute a systematic validation of the method. Comprehensive validation requires further empirical studies and quantitative analyses across diverse datasets and architectures, which remain as future work to establish the robustness and generalizability of this approach.

The methodologies and insights outlined in this chapter feed into the broader objective of assigning singular importance values to complex input features, a central theme in forthcoming discussions (see Chapters \ref{chapter:REVEAL} and \ref{chapter:revLRP}).
