\chapter{A Framework for Faithful and Interpretable Explanations in Deep Neural Networks}
\chaptermark{Framework}
\label{chap:framework}

\section{Introduction}

In the preceding chapters, the diverse landscape of neural networks and their interpretability was explored, examining various methods that strive to make complex models more transparent and understandable. A clear need was observed for approaches that provide deep insights into the inner workings of models while delivering these insights in an easily graspable manner. Addressing this need, this chapter introduces a novel unified framework.

Human cognitive load is a significant concern in model interpretability. The number of variables or features that a person can simultaneously consider is limited -- three to five meaningful items at once~\cite{cowan2001magical, starkey1995development, morris2018human}. As this number increases, our capacity to draw meaningful insights diminishes. DNNs often deal with a large number of features, making it a daunting task for people to interpret them when given individual importance scores for each feature. People excel at abstraction, often making sense of complex data by organising it into higher-level concepts or groups~\cite{JANVRIN201431, olsen2023social}. This kind of abstraction is notably absent in the current feature-by-feature interpretability methods. 

Methods that reduce the complexity of model explanations to `important' or `not important' and show only small amount of selected features offer the advantage of easy interpretability. These simplified explanations can be highly accessible and immediately informative. However, while the interpretability may be high, the fidelity of such explanations to the actual workings of the model is often compromised, which could lead to misguided or even harmful decisions. This is particularly import for applications where a wrong explanation to what the model has learned can have dire consequences, such as medical diagnoses, financial predictions, or autonomous driving systems.

Given the risk associated with oversimplified interpretations, there is an evident need for approaches that not only convey the underlying decision-making process of a DNN in an interpretable manner for human users, but are also grounded in the principles of fidelity, input invariance, and effective management of saturation and sensitivity issues. 

\section{Bridging the Gap: A Unified Framework}

In the framework proposed here, the aim is to to harmonise faithfulness and interpretability, where instead of offering importance values for each feature or distilling outputs into binary judgements, we propose a way to cluster related features together and provide an importance score for each cluster. This abstraction is closer to how humans naturally process information~\cite{fiantika2018internal}, thereby making the interpretation more intuitive. The granular information about each feature's contribution offers higher fidelity to the model's intricate decision-making processes. Despite this increased granularity, interpretability is not compromised because the volume of information presented is still manageable: each cluster of features is represented by a single, easily understandable metric. Moreover, these singular importance values enable a ranking or ordering of feature clusters, displaying which most influence the model's decision. Such an ordered representation not only simplifies the explanation for human understanding but also retains the subtle distinctions among features that are crucial for a faithful representation of the model's behaviour.

This framework sets the foundation and outlines the essential consideration one needs to take into account when creating specific techniques and methodologies for both grouping input features and assigning a single importance score to a set of input features. 

\subsection{Isolating Complex Input Features}

Feature clustering is performed based on criteria such as spatial proximity. The objective is to group features that are both alike and are spatially close in some feature space. Once clustered, each cluster receives an importance score that captures the combined importance of all features within that group to the output of the network. We define a complex input feature vector to be a cluster of features from the input feature map.

\begin{Definition}{Complex Input Feature Vector}{}
Given an input feature map $F \in \mathbb{R}^{d_1 \times d_2 \times \ldots \times d_n}$, where $d_1, d_2, \ldots, d_n \in \mathbb{N}$ are the dimensions along each axis of the tensor, we define a \emph{complex input feature vector} of $F$ to be a \emph{connected} subset $CF \subseteq \bbR^n$ enclosing some subset of elements $CF\cap (d_1 \times d_2 \times \ldots \times d_n)$ from the input feature map.
\end{Definition}

Isolating a complex input feature vector can be a beneficial strategy for handling different types of data. For instance, in image data, pixels and edges alone don't mean much until they form shapes or objects. Clustering can help identify these higher-level patterns. A cluster may correspond to edges while another might represent textures or it could be based on objects, making it easier to understand why an image was classified a certain way. Images are often high-dimensional  and feature clustering can reduce this complexity into a set of meta-features. In natural language processing, clusters can be used to group words or phrases that often appear together and likely belong to the same topic, aiding in the understanding of the thematic structure of the text. Another way to cluster words can be based on semantic roles, helping to explain how a sentence or document's meaning is constructed. Features could also be grouped based on their contribution to sentiments like positive, negative, or neutral. In audio data, different frequency bands contribute to different aspects like pitch, tone, and rhythm. Clustering can be used to group features related to these elements. When speech recognition is performed on top of audio data, features can be clustered to separate voice from background noise, aiding in the understanding of how well the model can distinguish speech in speech recognition tasks~\cite{malik2021automatic}. Features can also be clustered based on their time-related properties, helping in tasks like event detection or speech segmentation.

\begin{Desideratum}{Clustering Desideratum}

The essential considerations for the clustering approach (regardless of the input type) are as follows:
\begin{enumerate}
    \item \textbf{Relevance:} The clustering method should  highlight the inherent and semantically meaningful structures in the data.
    \item \textbf{Scalability:} The method should be able to scale with the dimensionality and volume of the input data.
    \item \textbf{Robustness:} The clustering should be resistant to noise and perturbations. Small changes in the input should not result in drastically different clustering results.
    \item \textbf{Interpretability:} Results from the clustering should be easily understandable and interpretable.
    \item \textbf{Compactness and Separation:} Compactness requires cohesive clusters, where all items in the cluster should be close to each other in the chosen representation space. Whereas, separation ensures inter-cluster distinctions, where different clusters should be as far apart from each other as possible.
    \item \textbf{Efficiency:} The cluster derivation method should be timely and efficient, as real-time analysis is often desired.
    \item \textbf{Consistency:} The method should allow for reproducibility in results. Meaning, the same input, when clustered multiple times under the same conditions, should yield the same results.
    
\end{enumerate}  
\end{Desideratum}

Given the desiderata for isolating a complex input feature, we further explore specific techniques for clustering in Chapter~\ref{chap:clustering}

\subsubsection{Complex Input Features Advantages}

Isolating complex input features alleviate issues in neural networks interpretability in several notable ways in addition to reducing the dimensionality of the features that need to be interpreted. First, rather than overwhelming the user with intricate pixel-level information, complex input features offer consolidated visual representations. These representations highlight broader patterns or regions of importance, making it easier for users to focus on and discern the main points of interest without getting overwhelmed by minutiae. By reducing the potential areas of focus, complex input features tend to also promote more standardised interpretations. Users are less likely to draw wildly different conclusions from a more concise representation, thereby fostering more consistent understanding across different users.

Second, when presented with a dense heatmap, users might ``anthropormophise'' while trying to find ‘what the network is trying to see’, finding patterns or correlations that may not exist. Complex input features diminish this risk by providing a more distilled view, preventing users from drawing unnecessary or spurious conclusions. By presenting a more intelligible and streamlined representation, users are likely to feel more confident about their understanding of the model's decisions. This confidence can translate into more informed and deliberate actions based on the model's output.

Finally, if different models are being used for the same task, feature clustering can provide a more straightforward way to compare their behaviour. Instead of examining numerous individual features within each model, it may be sufficient to compare a few carefully chosen complex features in order to obtain a meaningful understanding of how various models perform and make decisions, as well as to identify differences among them.

\subsection{Complex Input Feature Importance Value}

The assignment of a singular value of importance to each complex input feature is a crucial step in achieving a balance between the interpretability and faithfulness of the explanation. The naive way to assign a single value is through aggregating the relevance scores given by an interpretability method to the individual features within the cluster, thereby providing a summarised measure of the cluster's significance. Various aggregation methods, such as mean, median, or weighted sum, could be considered based on the application. 

Aggregating relevance scores into a single value effectively compresses the diversity and granularity of the information contained within the cluster. This can be a problem when features have vastly different relevances that end up being lost through the aggregation. Not only can it lead to a loss of information, but it can also introduce biases or distortions. For example, taking a simple mean might be sensitive to outliers, while a median might not capture the influence of exceptionally important features well. The single value of relevance should be equally representative of all input features in the cluster, which could be problematic if the cluster contains a diverse range of features with different relevance scores, as no aggregation metric can capture this diversity with a single value. In Figure~\ref{Fig:aggregating} (overleaf), the aggregated score of various clusters is presented. The method used to aggregate the scores involves both the median and the mean. A notable observation from the figure is the distinction in scores among different clusters, based on their size and composition.

For instance, smaller clusters, exemplified by those representing the pegs of the violin, showcase both a high mean and a high median. This suggests that the majority of input features within these clusters have high relevance, leading to their elevated aggregated scores.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/Mean_median.pdf}
	\end{center}
	\caption{The figure is partitioned into two sections -- the top one being "Mean of relevances" and  the bottom one "Median of relevances." The left column displays the input and provides the baseline image for the subsequent visualisations and interpretations. The centre column is a heatmap generated using Layer-wise Relevance Propagation (LRP) and it emphasises the relevance of different regions within the image. The right column is an aggregated colour-coded relevance representation. The input image is overlaid with a colour gradient derived from the LRP heatmap, delineating regions of interest. Each colour corresponds to a level of relevance. Adjacent to this visualisation, a colour scale legend provides the numeric values corresponding to each hue, enabling a quantitative interpretation of the visual data.}
	\label{Fig:aggregating}
\end{figure} 

Conversely, the larger cluster representing the body of the violin exhibits a different trend. The violin's cluster consists of areas with significant variations in relevance—some areas possess exceptionally high relevance while others do not. This heterogeneity within the cluster results in both a mean and median that are relatively low. This is particularly interesting as the mean and median values for the violin's cluster might not give a comprehensive view or representation of all the input features that constitute it. It underscores the importance of understanding the individual contributions within a cluster rather than just relying on aggregated metrics, as they might mask the nuances and variations present within larger, more diverse clusters.


Each feature in the input feature map goes through various transformations as it passes through the layers of a network. These transformations are influenced by the surrounding features, affecting the ultimate relevance that a feature has. For example, pooling layers aggregate information from kernels of features, inherently making the output a function of multiple input features (see Section~\ref{section:avglayer}). Methods like batch normalisation standardise features based on the distribution formed by all features, further intertwining their relevances(see Section~\ref{section:bn}). Current explainability methods that focus on properties such as fidelity, input invariance, handling saturation and sensitivity, assign the value of relevance of each feature in the input space given the output and the connection it has to other features from the input~\cite{SimonyanVZ13, SimonyanVZ13, SpringenbergDBR14, bach2015pixel, SelvarajuCDVPB20, SelvarajuCDVPB20, ChattopadhyaySH18, abs-1908-01224, SmilkovTKVW17, bach2015pixel}. Therefore each feature's assigned value is relative to every other feature in the input feature map, so when aggregating the relevance from the same cluster, the relationship between the complex input feature relevance and other complex input feature relevances will not be the same as the relationships between the features that comprise it. A further problem with aggregation methods is that the relevance of distinct features in the input space can come from the \textit{same} more complex feature that is detected in the deeper layers of the network. Hence, in situations where a cluster contains highly correlated features, the aggregation might amplify this correlation, leading to overemphasis or underrepresentation of certain aspects in the data. Conversely, in cases where clusters contain uncorrelated or weakly related features, the aggregated score might dilute the individual feature's significance.

The inherent interdependence between features makes the process of untangling individual contributions challenging. For instance, when a convolutional layer identifies a specific pattern in an image, it doesn't just recognise a single pixel but rather a combination of neighbouring pixels forming a pattern. So, if one were to assign importance to a single pixel in this scenario, it would not accurately reflect the true importance. Instead, the combined pattern of several pixels holds the true importance. When features are aggregated into clusters, this interdependency causes the problems described so far. This is particularly problematic in cases where the network has many layers and complex interactions between features, as some features may seem relevant but might not have a direct or substantial contribution to the final output.

\begin{Desideratum}{Importance Value Assignment Desideratum}

The essential considerations for the value assigned to a complex input feature are as follows:
\begin{enumerate}
    \item \textbf{Faithfulness:} The value should be indicating the combined importance of the input features that comprise the complex feature with respect to the network and the output.
    \item \textbf{Consistency:} The method should allow for reproducibility of results. Meaning, the same complex input feature, when evaluated on the same input and the same neural network, should yield the same results.
    \item \textbf{Granularity:} It's essential to maintain an appropriate level of detail that acknowledges the diversity and significance of complex features. 
    \item \textbf{Interdependence Awareness:} The methodology should recognise and address the relationships between features. It should account for both highly correlated and uncorrelated features and prevent overemphasis, under-representation, or dilution of individual feature's importance.
    \item \textbf{Robustness to Network Architecture:} As networks can vary significantly in terms of their depth, type of layers, and operations (e.g., pooling, batch normalisation, convolution, attention), the method should be robust enough to provide accurate importance values across different architectures.
    \item \textbf{Bias and Distortion Mitigation:} The method should minimise the introduction of biases or distortions. This requires careful consideration of outliers, extreme values, and the size of the complex input feature (\ie the number of elements that comprise it)
    \item \textbf{Transparency:} The process of assigning a singular importance value to complex input features should be transparent and easily interpretable as to how the value was derived.
    \item \textbf{Scalability:} The method should be scalable in order to handle vast numbers of complex features or large number of layers comprising the neural network, without sacrificing accuracy or interpretability.
\end{enumerate}  
\end{Desideratum}

In light of these considerations this framework introduces a general approach for assigning an importance value to a complex input feature $CF$. Instead of determining the relevance by reversing the functions at each layer of the neural network~\cite{ZeilerKTF10, SpringenbergDBR14, SimonyanVZ13, bach2015pixel, LapuschkinBMMS16, ShrikumarGK17, SundararajanTY17}, we propose a \textit{forward-pass approach} that determines the influence only the complex input feature had during the inference (forward-pass) through the network. 

We refer to the forward-pass method as a \emph{contribution to classification} (\CTC\ method) and it calculates the contribution the complex input feature had to each layer during inference. This finally results in a measure of how much the complex feature contributed to the output. 


\begin{Definition}{Contribution}

For a given input vector $\vec{x}\in \bbR^d$ and a given complex input feature $CF\subseteq \bbR^d$, we define the \emph{contribution} at layer $k\in \Lambda$ inductively, by taking:
\begin{equation*}
       \vec{c}_k = \lambda^\prime_i\big(\vec{a}_k,[\vec{a}_j]_{j\passto k},[\vec{c}_j]_{j\passto k}\big)
\end{equation*}
where $\lambda^\prime_i$ denotes a rule depending on the type of layer $k=0,\dots, N$ allows for calculating the contribution the complex input feature had on the overall output during the forward pass. The value of the contribution $\vec{c}_k$ depends on the contributions from all preceding layers, as well as activation $\vec{a}_k$ computed during the forward pass.
\end{Definition}

The contribution of a complex feature during the forward pass differs from merely classifying the feature. It operates on the premise that each feature can have varying degrees of influence at different layers of the neural network. The \CTC\ method evaluates the feature's contribution at each layer during the forward pass. This enables a more granular understanding of how the feature influences the neural network's computations at various stages, but it is not the same as classifying the feature in isolation. 

An example of where the \CTC\ and the inference output differ is when calculating the contribution of the complex input feature to a max pooling layer. If the neuron that was most active in a kernel during the forward pass has received the most contribution from the complex input feature then the output of both \CTC\ and inference is the same. However in the case where the neuron with the biggest contribution is not the same, \CTC\ will select the same neuron that was selected during the forward pass and propagate the contribution of \textit{that} neuron forward, not the most active one. In this case, if the unchanged inference function is applied taking the complex input feature values as input it can result in neurons being selected that were not pooled during the forward pass, thereby violating the faithfulness on the explanation. In Figure~\ref{Fig:max_pooling} we illustrate the difference between the forward-pass max pooling layer and the contribution max pooling layer. In the forward-pass max pooling layer, the neuron with the maximum value is selected, and that value is propagated forward. In the provided figure, the neuron with a value of 0.5 is selected because it has the highest value among all inputs. However, when we look at the contribution of the max pooling layer, the input with a value of 0.8 has the highest contribution, but this input was not the one selected during the forward pass. The forward-pass selected the neuron with a value of 0.5. Therefore the same neuron is selected during the contribution selection, despite the value of 0.2 not being the highest value among all the contributions. This highlights the core difference between how max pooling works in the forward pass and how contributions are evaluated.


\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/max_pooling.pdf}
	\end{center}
	\caption{A comparative illustration of the forward-pass max pooling layer (left) and the contribution max pooling layer (right). While the forward-pass selects the neuron with the highest value (0.5), the contribution layer evaluates the contribution of the feature to each neuron and selects the neuron with the highest activation during the forward pass, despite that not being the same as the highest contribution. This highlights the potential differences between the calculation of activation values and their contributions.}
	\label{Fig:max_pooling}
\end{figure} 

Another example where \CTC\ and inference differ is in the case of a ReLU layer. For the activation of a neuron to pass through a ReLu layer it has to be more than 0. When applying the \CTC\ method if the partial activation (\ie relevance) given to a neuron is negative and that neuron during the forward pass had an activation greater than zero, this negative contribution is still allowed to propagate through a ReLu layer as is signifies the influence the complex input feature has on the output. In Figure~\ref{Fig:Relu} we illustrate the distinction between the forward-pass ReLU layer and the contribution ReLU layer. In the forward-pass ReLU layer (on the left), any activation less than 0 is clipped to 0, ensuring non-negative activations are propagated. This behavior is consistent with the typical operation of a ReLU function, which sets all negative values to 0 and lets positive values pass unchanged. On the other hand, the contribution ReLU layer (on the right) considers the contribution of each neuron with respect to the forward-pass value. Here, even if a neuron had a positive activation during the forward pass, its contribution could be negative. This negative contribution indicates that the complex input feature reduces or negatively impacts the output. The contribution ReLU layer allows these negative contributions to propagate, providing a deeper understanding of the influence of each feature on the model's output. The contribution can also be positive to a negatively active neuron during the forward pass. This contribution is not propagated as the neuron was made not active post the ReLU layer. 


\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=1\linewidth]{Figures/ReLU.pdf}
	\end{center}
	\caption{Comparison between the forward-pass ReLU layer (left) and the contribution ReLU layer (right). The forward-pass ReLU layer clips all negative activations to 0, while the contribution ReLU layer evaluates and propagates both positive and negative contributions, providing insights into the influence of complex input features on the neural network's output.}
	\label{Fig:Relu}
\end{figure} 

By comparing these two layers, we can observe the nuanced differences between a neuron's activation and its contribution. While activations are purely feed-forward and determined by the input values and weights, contributions reflect the broader context of how a specific feature influences the overall decision-making process of the neural network.

\subsection{Singular Importance (Contribution) Value Advantages}

The contribution of complex input features provides a more \textbf{robust}~(Desiderata 4.2~(5)) measure of feature importance given any neural network. Such complex features, often comprising multiple individual attributes, vary in relevance or are intertwined within the intricate function the network learns. This method's strength lies in accounting for these interdependencies(Desiderata 4.2~(4)), thus avoiding overemphasis, under-representation, or dilution of any single feature's importance. Notably, it can rank clusters of features by their importance, simplifying interpretation by spotlighting pivotal features and shedding light on the hierarchical significance of features for specific decisions or outcomes.


The contribution value of a complex input feature is dependant on the rules that are applied at each layer. Given that these rules are predetermined and don't rely on an interpreting secondary network, the method exhibits full \textbf{transparency}~(Desiderata 4.2~(7)) as to how the method arrives a the value. Assuming these rules mirror activation flows and isolate complex input feature contributions accurately, the method remains entirely \textbf{faithful}~(Desiderata 4.2~(1)) to the network and the final output, free from \textbf{biases or distortions}~(Desiderata 4.2~(6)). A good single importance value that represent the entire set of input features comprising the complex input feature. It also showcases consistency (Desiderata 4.2(2)), producing repeatable results for the same input and network, provided deterministic rules. Additionally, its~\textbf{granularity}~(Desiderata 4.2~(3)) enables feature comparison, and its layer-by-layer propagation offers insights into each neuron's contributions. This is invaluable for understanding contributions to specific filters or, in classification tasks, for discerning contributions to all classes, not just the primary classification. Finally, the method is \textbf{scalable}~(Desiderata 4.2(8)). Although it is computationally more intensive than simple backward-pass methods, it requires $n$-many modified forward passes, where $n$ is the number of complex features. This is not very computationally intensive and it can be further optimised by putting all complex features in a batch and therefore processing their importance at the speed of a single forward pass.


The specific rules and the modifications to the forward pass are major contributions in this PhD thesis and are discussed in Chapters~\ref{chapter:revLRP} \&~\ref{chapter:REVEAL}

\section{Convolutional Neural Network: Need for Faithful and Interpretable Explanations}

This framework introduces feature clustering and the assignment of a single value of relevance as a universal strategy to enhance the interpretability of neural networks. While the technique can be applied across data types — from text in natural language processing to the nuanced frequencies in audio signals — its most pressing need is in the applications of convolutional neural networks.

Textual representations, especially when tokenised, already have a more granular and interpretable structure since words, phrases, and sentences inherently carry meaning. While feature clustering can offer insights into topic associations or semantic relationships, the inherent structure of language offers a good foundational comprehensibility. Audio data, especially in controlled environments, can be relatively less complex compared to high-resolution images too. Arguably, key features such as pitch, tone, and rhythm can be more directly interpretable without requiring extensive clustering. Structured data tends to be the most interpretable type, in traditional tabular data, the features are often predefined and have specific meanings (e.g., age, income, temperature). The direct relationship between such features and outcomes can sometimes be deduced without the need for advanced clustering techniques to lower the dimentionality of the data.

While the advantages of feature clustering and the single value of importance are apparent across all types of data, the need varies. Image data, with its high dimensionality and the inherent complexity of visual tasks, presents unique challenges. A single image encompasses thousands to millions of pixels, each contributing a piece to the overall picture, but in ways that are often hard to discern. For instance, individual pixels might not provide any intuitive insight into what makes a cat different from a dog in an image classification task. Clustering can condense this high-dimensional space into manageable complex features like edges, textures, or objects, allowing for a more intuitive grasp of what the model is seeing. Additionally, images are central to numerous critical applications, from medical diagnostics to autonomous driving, where misinterpretations can have profound consequences. For example, in medical imaging, a misinterpretation can lead to incorrect diagnoses and treatment plans. In such high-stakes applications, the more understandable the model, the better the chances of it being effectively and responsibly used. Faithful and interpretable explanations can bring us a step closer by simplifying the features into higher-level concepts that users of the machine learning models (\eg medical professionals) can more readily understand and act upon.


Images used in machine learning models often come from varied and diverse sources. These can range from medical imaging to satellite photos to snapshots from social media. Each domain has its own specific sets of relevant features and requires specialised understanding. Feature clustering can serve as a unifying method of interpretation that can be adapted to different domains. Images also often contain multiple objects and a variety of backgrounds, making it hard to isolate what specific features are important for a given task. Understanding if the model is focusing on the object of interest, or is it getting distracted by the background is crucial. Feature clustering can provide a way to group features related to different objects or contextual elements, thereby allowing for a more nuanced interpretation.


In some real-world applications like autonomous driving or real-time surveillance, decisions need to be made quickly~\cite{Tian}. The high dimensionality of image data can be a bottleneck in understanding what the model is doing~\cite{Fong}. This framework can speed up the interpretability process, thereby making it more applicable in time-sensitive situations.

In short, while textual, tabular, and audio data too require nuanced methods for interpretability, the largest and most pressing need for \textit{better} explanations lie in the field of computer vision. Therefore, this presents the major the direction of investigation for the following chapters. 

\section{Conclusion}

This chapter serves as an introduction to the core concepts, motivations, and primary challenges surrounding a method for faithful and interpretable explanations of neural networks. It particularly focuses on the role and significance of grouping things into complex input features and the notion of assigning singular importance values to complex input features, which is critical to bridging the gap between high-dimensional data and human interpretability. With the overwhelming data granularity presented by deep learning models, the challenge is not merely about extracting feature importance but presenting it in a manner that is consistent, faithful, and intuitive to human users.

The Contribution to Classification (CTC) method introduced in this chapter offers a promising approach towards this end. By focusing on a forward-pass methodology, it aligns closely with the inherent workings of a neural network, ensuring that the derived importance values are both accurate and meaningful. Subsequent chapters will provide ways of identifying complex features (see Chapter~\ref{chap:clustering}) and specific ways of propagating the contribution of complex features through the network (see Chapter~\ref{chapter:revLRP} \& \ref{chapter:REVEAL}). Rules and results will mostly focusing on the interpretability of convolutional neural networks as the domain that has a high dimentionality of input data and therefore presents a more pressing need for this type of explanations. Both qualitative and quantitative metrics will be used to assess the faithfulness and interpretability, thereby making a case for the utility and novelty of the approach.


