\chapter{Forward Pass Retracing: Contribution to Classification}
\chaptermark{Forward Pass Retracing: CTC}
\label{chapter:REVEAL}
\newcommand{\lo}[1][j]{l_{#1}}
\newcommand{\clo}[1][j]{\text{$c$-}\lo[#1]}
\newcommand{\djo}[1][j]{d_{#1}}
\newcommand{\cdjo}[1][j]{\text{$c$-}\djo[#1]}
\section{Introduction}


The previous chapter delved into the intricacies of relevance distribution tracing as a technique for assigning significance to complex input features in neural networks. Despite its effectiveness, this approach concluded with an acknowledgement of the challenges posed by high-dimensional Jacobians, particularly their substantial computational and memory requirements. Forward pass retracing is designed to address these limitations, offering a more feasible and efficient method for network interpretation.

At its core, forward pass retracing simplifies the process of determining the contribution of each input feature to the network's output. Unlike the methods discussed in Chapter~\ref{chapter:revLRP} which require three passes through the network, the forward pass retracing method operates by conducting only two forward passes similar to all other state-of the-art interpretability methods. The first pass is the inference step and it computes the activations from the input at each layer until the classification is reached. The second pass is carried out with modified functions that replicate the network's behaviour during the inference step on a complex input feature. By doing so, the method directly analyses how the complex input features influence the output in a more straightforward and computationally efficient way.

This chapter comprehensively explores the mechanisms and advantages of forward pass retracing. We discuss how this method mirrors the network's inference behaviour for different types of layers, which is achieved without compromising the depth and fidelity of the interpretative insights gleaned from the network. 


\section{Motivation}

Forward pass retracing is a novel type of explanation of CNN classifications. This method emphasises the role of complex features in contributing to the final classification, a concept termed as \emph{Contribution To Classification (\CTC)}.

\begin{definition}[Contribution]
Given a trained neural network $\NN=\big(\Lambda,\passto,(f_k)_{k\in \Lambda}\big)$ with a set of layers $\Lambda=\{0,\dots, N\}$, with a single source $0\in \Lambda$, referred to as the input layer and single sink $N\in \Lambda$, referred to as the \emph{output layer}, each $f_k:\bbR^{n_j}\to \bbR^{m_k}$ describes a vector transformation associated with each layer, with the dimensionality conditions that $m_k=\sum_{j\passto k} n_j$, for all $k=1,\dots N$. For a given input vector $\vec{x}\in \bbR^d$ and a given feature $F\subseteq \bbR^2$, let the \emph{contribution} at layer $k\in \Lambda$ be inductively found, by taking:
\begin{equation*}
       \vec{c}_k = f_i^\prime\big(\vec{a}_k,[\vec{a}_j]_{j\passto k},[\vec{c}_j]_{j\passto k}\big)
\end{equation*}
where $f_i^\prime$ denotes the function that modifies the behaviour of the network at layer $\Lambda_k$, depending on the type of layer $k=0,\dots, N$, so that it calculates the contribution a feature had during the forward pass. The value of the contribution $\vec{c}_k$ depends on the contributions from all preceding layers, as well as activation $\vec{a}_k$ computed during the forward pass.
\end{definition}

This chapter outlines a set of rules for propagating activation from specific input regions through the hidden layers of the network, culminating in a singular \CTC\ value. The computation of a feature's \CTC\ value is facilitated by introducing the \emph{Contribution To a Neuron (\CTN)} value. This value represents the activation a neuron receives as a result of the feature during the forward pass, with the \CTC\ value of a feature being equivalent to the CTN value of the classification output neuron. This propagation of activation, which accounts for the feature's contribution during the forward pass, is distinct from a mere classification of the feature. This distinction is crucial due to the non-linear layers such as MaxPooling, which could otherwise pool the contribution of neurons not selected during the forward pass, compromising the explanation's faithfulness.


There is an important differentiation between the \CTC\ value of a feature and its relevance. While current interpretability methods focus on the importance of a feature as part of the entire input considering the specific classification output (\ie feature relevance), the approach described in this chapter isolates the importance of the \textit{feature alone} during the forward pass (\ie feature's \CTC\/). Figure~\ref{fig:contribution} illustrates the distinction between a feature's relevance and its \CTC\ value using a specific input. When considering each pixel as a distinct feature, techniques like \LRP\ assign significant importance values within the context of the entire input. However, for an individual pixel, the \CTC\ value is generally insignificant, as no single pixel notably impacts the overall classification. In contrast, when assessing an amalgamation of simple features (pixels), or a complex feature, the \CTC\ value effectively demonstrates the contribution of the entire region to the classification through a heatmap. To enhance interpretability, complex features are presented in varying colours.


% The chapter posits that the \CTC\ value provides a more useful explanation of a feature's importance to the classification than its relevance. In convolutional neural networks, while edges detected from the input are relevant for decision-making, their contribution to the classification can vary significantly. Relevance-based approaches may overestimate the importance of certain input parts, as they may not actively contribute much to the classification. This is particularly evident in deep convolutional neural networks with extensive convolutional layers, where almost all edges are likely to be deemed important. Figure~\ref{fig:guitar_dog} contrasts relevance-propagation methods with this contribution approach, highlighting the difference in the importance assigned to various input parts.


\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{Figures/tiger_contribution.pdf}
	\caption{ Illustration of the difference between contribution to classification and relevance on a pixel level. The explanations were computed on InceptionV3~\ref{szegedy2015rethinking} and the relevance heatmap is of LRP~\cite{bach2015pixel}}
	\label{fig:contribution}
\end{figure}


% \begin{figure}[h]
% \centering
% \includegraphics[width=\linewidth]{Figures/tiger_contribution.pdf}
% 	\caption{ \LRP\ identifies most of the edges in the image as relevant when explaining the ``Electric Guitar'' class including the head of the dog. In contrast, \REVEAL\ attributes only 0.73\% relevance to the head of the dog and 53\% relevance to the actual guitar.}
% 	\label{fig:guitar_dog}
% \end{figure}

\section{Propagation Rules}

This section defines a set of rules which allow for the contribution to be distributed layer by layer starting from the input layer until the output layer is reached. These rules differ from just performing a forward-pass on a complex input feature as they compute the activation that is a direct result of the presence of the complex input feature during the forward pass, as opposed to computing the activations that this complex input feature has by itself. Given the presence of different layer types, different contribution propagation rules must be defined which respect and reflect the underlying structure of the network and functions that comprise it.


\subsection{Contribution of First Layer}
The contribution of a region at the input layer 0 $\in \Lambda$ is the same as the activation's of that region (i.e the pixel values of the three input channels), so the contribution $\vec{c}_0$ is defined as:
\begin{equation*}
    \vec{c}_0 = \vec{a}_0 \odot \vec{m},
\end{equation*}
where $\vec{m}\in \{0,1\}^{n_0}$ is a mask over the region propagated as found by the method described in Section~\ref{chap:clustering}, and $\odot$ denotes element-wise multiplication operation. Performing the multiplication with the mask sets to zero all the activations of neurons that are not in the region of the feature that is being propagated. This allows only for the activations that are part of the complex input region to be propagated. 

\subsection{Masking After Each Layer}
Only neurons that were active during the inference step should have contributions distributed to them during the feature contribution propagation. Thereby after each layer's function is performed, a mask $\vec{m}_k\in \{0,1\}^{n_k}$ is extracted over the activations values in $\vec{a}_k$, as follows 
\begin{equation*}
    \label{eq:masking}
    m_k(i) = \begin{cases}
    1 & \mbox{if $a_k(i)\not=0$}\\
    0 & \mbox{otherwise}
    \end{cases} 
\end{equation*}
for all $i \in \{1,\dots, n_k\}$, so that every position that had a non-zero values during the forward pass after the function of the layer was computed has a value of one and otherwise has a value of zero. The contributions are then masked, by:
    \begin{equation*}
    \label{masking}
        \vec{c}_k = \vec{c}_k\odot \vec{m}_k,
    \end{equation*}
where $\vec{m_k}\in \{0,1\}^{n_k}$ is a mask over the activations values $\vec{a}_k$ in layer $\Lambda_k$. Note that neurons that were active during the classification but did not receive any contribution from the feature are still zero. The practice of distributing contributions only to neurons that were active during the inference step is not just a procedural detail, but a fundamental aspect of the feature contribution propagation process. Omitting this step could lead to inaccurate or misleading interpretations of a neuron's contribution to the final output. For instance, if contributions were assigned to inactive neurons, it would distort the understanding of how each feature influences the model's decision-making process. This could lead to erroneous conclusions, especially when analysing complex neural networks where the interplay of features and neurons is intricate.


\subsection{Dense Layer Rule}
The dense layer represented by a vector valued function $f_k: \bbR^{n_j}\to \bbR^{m_k}$ of the form $f_k(\vec{x})= W_k \vec{x} + \vec{b}_k$, for some matrix of weights $W_k \in \bbR^{n_j\times m_k}$ and some vector of biases $\vec{b}_k \in \bbR^{m_k}$. This function is often followed by an activation function, where the output of the the dense layer is often refereed to as the net value $\net$. The explanation of the function $f_k$ is broken down in the section below into its individual components. This approach aims to clearly illustrate the impact of the dense layer on the classification of a part of the input during the forward pass of the network.

\subsubsection{Weighting the contribution}
In the dense layer function, the first step involves the weighting of the activations that have reached the layer. Contribution refers to the amount of activation that is distributed from a specific region in the input space during the forward pass. Since weighting is a multiplicative operation, it proportionally scales each unit of activation relative to the weight. Thus, it is deemed sufficient for the contributions to be weighted in the same manner as the activations were during the forward pass, as detailed below:
\begin{equation*}
    \net^- = W_k [\vec{a}_j]_{j\passto k}\quad\mbox{and}\quad
   \cnet^- = W_k[\vec{c}_j]_{j\passto k},
\end{equation*}
where the left hand-side equation shows the weighting of the activation and the right hand-side one shows the weighting of the contribution.
\subsubsection{Adding the bias}
\label{section:adding_the_bias}
To calculate the net value $\net$ during the forward pass a learned bias term $\vec{b}_k$ of the layer is added to the weighted sum $\net^-$. 
The bias term holds a critical role in shaping the function learned by a neural network. During the backpropagation process not only does the bias influence the update of the weights, but the weight adjustments also affect the bias. This makes the bias an integral component of the neural network's learned function. Neglecting the bias during inference can lead to markedly different outputs. A striking example of this is illustrated in Figure~\ref{fig:bias_no_bias}, where the classification outcome for a guinea pig dramatically shifts from the most positive to the second most negative class simply due to the exclusion of biases. Moreover, the bias contributes to the model's ability to recognise patterns in data that don't centre around zero. It does this by inducing a learned shift in the activation function, either towards the positive or negative side. Omitting this shift can substantially alter the function of the neural network, underscoring the importance of the bias in its overall operation.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=\linewidth]{Figures/guini_without_bias.pdf}
	\end{center}
	\caption{The figure shows how the classification of an image can change completely when the learned bias in the network is removed. In this example it changes from the correctly classified guinea pig to a upright piano, which isn't present in the image. Guinea pig also becomes the second most negative classification, so the bias' effect on such image is difficult to be overstated.}
	\label{fig:bias_no_bias}
\end{figure} 
\noindent

When advancing the contribution forward, a straightforward approach is to add the bias as it is done in the forward pass. However, this method needs refinement, as the bias is calibrated for the contribution from the entire input space, not just a single feature. This mismatch can lead to two issues:
\begin{enumerate}
    \item If the contribution from a specific input region to a neuron is just a small fraction of the total input's contribution, adding the original bias can disproportionately amplify its effect on the output.
    \item Conversely, if the contribution from a single neuron is much larger than it was during the forward pass (perhaps due to the absence of input regions that are irrelevant or negatively contributing), the original bias might not adequately adjust the mean as it did during the forward pass.
\end{enumerate}


The bias is originally tuned to work within a specific range of values. In the first scenario, adding the same bias term can excessively influence the output, as demonstrated in Figure~\ref{fig:Neurons_with_bias}~A, where the relevance is a fraction of the activation. In the second scenario, illustrated in Figure~\ref{fig:Neurons_with_bias}~B, the relevance exceeds the activation. Here, adding the bias of 0.5 results in an output minimally affected by the bias, contrasting with its more significant role during the forward pass. These examples highlight the need for a nuanced approach to incorporating the bias when propagating contributions.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=0.6\linewidth]{Figures/AandB.png}
	\end{center}
	\caption{(A) illustration of the effect of the bias when the contribution of an input region with respect to a particular neuron is a very small fraction of the the entire input's contribution. (B) illustration of the converse effect, where the contribution of a single neuron might be far greater than the entire input' contribution.}
	\label{fig:Neurons_with_bias}
\end{figure} 
To address these issues, it's essential to apply a proportional amount of the bias, ensuring it neither overwhelms nor under-represents the relevance being propagated. The proportion of the bias to be added to the scaled contributions is defined by a matrix $P_{k}$ that when multiplied with the original bias $\vec{b}_k$ yields an adjusted bias matrix. The exact way $P_{k}$ is defined can be seen in Section~\ref{sec:scale}, which presents a discussion of how the relevance signal can be preserved when dealing with learned network parameters. Given that the method proposed in this chapter involves propagating only a portion of the input through the network, maintaining the integrity and the influence of this small subset of activations, without it getting diluted or lost presents a significant challenge. This general way of adjustment can be formulated as:
\begin{equation}
   \vec{c}_k  = W_k \vec{c_j} +  P_{k}{b}_k.
\end{equation} 
\subsection{Convolutional Layer Rules}
Convolutional layers comprise of multiple operations stacked after one another. Suppose that $k\in \Lambda$ is an convolutional layer with $f_k$ of the form $f_k(\vec{x})=\act(W_k\ast\vec{x} + \vec{b}_k)$, for some matrix kernel $W_k:\bbR^{d_k\times d_k'}$, some vector of biases $\vec{b}_k\in \bbR^{m_k}$, and some (component-wise) activation function $\act:\bbR^{m_k}\to \bbR^{m_k}$. 

\subsubsection{Weighting the contribution}
First the convolutional layer divides the input data into smaller regions called \emph{local receptive fields}. Each receptive field corresponds to one element or a small neighbourhood of elements in the input data. Next the convolution operation is performed by sliding the filters over the input data, applying element-wise multiplication between the filter and the receptive field, and summing up the results. This produces a single value, which represents the activation of a neuron in this resulting layer. This process is repeated for each location from the input to the layer, which helps in capturing the same features regardless of their location (see Section~\ref{sec:conv}).

The sum of each locally receptive field multiplication with a filter is a smaller (local for each locally receptive field) version of taking the weighted sum in a dense layer. Similarly as in the dense layer, weighting is a multiplicative operation, it scales each unit of activation in a locally receptive field proportionally to the weight in the filter. Therefore it is sufficient for the contributions to be
weighted in the same way as activations were during the forward pass. As detailed
\begin{equation}
    \net^- = W_k\ast[\vec{a}_j]_{j\passto k}\quad\mbox{and}\quad
   \cnet^- = W_k\ast[\vec{c}_j]_{j\passto k},
\end{equation}
where the left hand-side equation shows the weighting of the activation and the right hand-side one shows the weighting of the contribution.
\subsubsection{Adding the Bias}

The bias vector \(\vec{b}_k \in \bbR^{m_k}\) is added to the output of the convolution operation. The process enhances the layer’s ability to learn patterns in data and adjust its activation function appropriately. Adding bias in a convolutional layer is crucial as it allows the network to adjust the output of the layer independently of its inputs. This is particularly useful in cases where the data should not centre around zero, as it helps in shifting the activation function. This general way of adjusting the biases in convolutional layers can be mathematically formulated as:
\begin{equation}
   \vec{c}_k =\cnet^- +  P_{k}{b}_k.
\end{equation} 
\subsubsection{Activation}
The convolutional layer often has an activation function as its last operation. If the function is ReLU it is bypassed by the contribution rules. This allows for negative relevances to pass through and accurately show that a complex input feature had a negative relevance on a neuron that had an overall positive activation. Note that the masking after each layers ensures that relevances assigned to non-active neurons during the forward pass are assigned zero.  


\subsection{Max Pooling Layer Rule}

The process of pooling in neural networks can be described as a transformation via a vector-valued function, denoted as $f_k:\bbR^{n_j}\to\bbR^{m_k}$, where the dimensionality $n_j$ of the input layer is reduced to $m_k$ in the subsequent layer, with $n_j \geq m_k$ (see Section~\ref{section:max}). The core objective of the contribution propagation during the new pooling function is to ascertain the significance of each complex input feature in relation to the neurons that exhibit maximum values during the network's forward pass. This evaluation of importance is crucial in understanding how each feature contributes to the model's decision-making process. A key aspect to consider is that the maximum value within a given patch identified during the classification phase might not align with the value deemed most significant during the contribution propagation phase. Hence, it is not feasible to simply apply max pooling on the relevance vector, as this would overlook the dynamic nature of the most relevant features across different phases of the network's operation.

\subsubsection{Finding the Maximum Elements During Inference}
To refine the pooling process for contributions, a method is introduced that more precisely mirrors the actions taken during the forward pass. This method involves the creation of a binary mask, $\vec{m} \in \{0,1\}^{n_j}$, which is applied over the inputs to layer \(k\). In this mask, elements corresponding to inputs that were not pooled during the forward pass are set to zero, while those that were pooled are set to one. This approach allows for an exact replication of the pooling behaviour observed in the forward pass. The mask \(\vec{m}\) is created by taking the gradient of the function \(\vec{f}_k\) with respect to the activations from the preceding layer(s), denoted as \([\vec{a}_j]_{j\passto k}\). This is represented as:
\begin{equation}
    \vec{m_j} = \nabla f_k\big([\vec{a}_j]_{j\passto k}\big).
\end{equation}
where, the subscript \(j\) refers to the input layers that feed into layer \(k\), with \(j \passto k\) indicating the connection from layer \(j\) to \(k\). The mask \(\vec{m_j}\) filters out the non-maximum elements, as all neurons that was not pooled during the inference step will have a gradient of zero. When this mask is applied to the contributions from layer \(j\), it results in a new vector, \(\vec{c_j}^\prime\), which retains only the contributions of the neurons that were actually pooled. This is expressed as:
\begin{equation}
    \vec{c_j}^\prime = [\vec{c}_{j}]_{j\passto k} \odot \vec{m_j}.
\end{equation}
The resulting vector, \(\vec{c_j}^\prime\), contains contribution values for the neurons that were pooled, with zeros in all other positions. 

\subsubsection{Pooling the Relevance Values at the Maximally Activated Neurons During Inference}

When assessing the new contribution vector \(\vec{c_j}^\prime\), if all contributions are positive, determining the most significant contributions becomes straightforward. This involves performing a max pooling operation on \(\vec{c_j}^\prime\), wherein the largest values indicate the most significant contributions of the pooled neurons. However, it's important to note that a region's contribution to a neuron can be negative, even if that neuron was maximally active when considering the entire input. To pool the relevances no matter whether they are positive or negative one can perform the max pooling operation on the absolute values of the $\vec{c_j}^\prime$ vector 
\begin{equation}
    \label{pool}
    \vec{c_{k}}^- = f_{k}(||\vec{c_j}^\prime||)
\end{equation}
and then add the sign of the contribution to the resulting vector by:
\begin{equation}
    \label{reasign}
    \vec{c_{k}} = \begin{cases}
    c_{k}^-(i) & \mbox{if $f_{k}(\vec{c_j}^\prime)\not=0$}\\
    -1\times c_{k}^-(i)& \mbox{otherwise}.
    \end{cases}
\end{equation}

The sign re-assignment in Equation~\ref{reasign} is attained by pooling the non-absolute contributions after masking $\vec{c_j}^\prime$ (\ie taking $f_{k}(\vec{c_j}^\prime)$). The output of $f_{k}(\vec{c_j}^\prime)$ will be non-zero in the cases where the contribution at the position pooled is positive and zero in all other positions. This allows to establish the positions where the contribution is negative and the sign needs to be reassigned, as those positions will have a non-zero value after preforming $f_{k}(||\vec{c_j}^\prime||)$, but a zero value when performing $f_{k}(\vec{c_j}^\prime)$. Therefore a minus sign is assigned for $c_{k}(i)$ whenever the pooled value is zero.


\subsection{Batch Normalisation Layer Rule}
A batch normalisation layer $k\in \Lambda$ during the forward pass can be expressed as a vector-valued function $f_k:\bbR^{n_k}\to \bbR^{m_k}$ ($n_k=m_k$) given~by:
\begin{equation*}
    f_k(\vec{x}) = \gamma \frac{(\vec{x}-\mu)}{\sqrt{\sigma_x^2 +\epsilon}} + \beta ,
\end{equation*}
where $\gamma,\beta \in \mathbb{R}$ are learned parameters that scale and shift the normalised input respectively. This layer adjusts each feature dimension of the input \(\vec{x}\) based on the learned mean \(\mu\) and standard deviation \(\sigma\) from the training process, incorporating a small constant \(\epsilon > 0\) to maintain numerical stability.

As batch normalisation does not alter the contribution of features but just shifts and scales them, this layer can safely be bypassed. The contribution of layer $\Lambda_k$ is therefore defined as:
\begin{equation*}
    \vec{c}_k = \vec{c}_j
\end{equation*}

% Focusing on first normalising the activations to have an approximate mean of zero and a standard deviation of one, the step can be trivially rewritten as:
% \begin{equation*}
%   \vec{a_k}^- = \frac{1}{\sqrt{\sigma_j^2 +\epsilon}}(\vec{a_j}-\mu)
% \end{equation*}
% making it easy to notice that the normalisation is equivalent to shifting the activations by the learned mean $\mu$ and scaling the resulting value by the learned standard deviation $\sigma$. The shifting of the activation with respect to the learned mean is the same as adding the bias in a dense layer, where the activation in each feature dimension are also shifted by a constant learned value (see Section~\ref{section:dense}). Given that the mean $\mu$ is learned as an approximate mean of the training examples seen, it is tuned for values within a certain distribution and magnitude. To preserve the signal of the complex input feature distributed (which may be just a small portion of the entire input) it is important for this learned value to be scaled. If it is not it can cause problems, as it can have either a disproportionately high or low effect on the output. Therefore when shifting the activations to be centred roughly around zero (note that the shift is not precise as the value is learned and is not the exact mean of this input) one needs to weight the mean with respect to the how much contribution of the feature has been propagated in comparison to the activation, as follows:
% \begin{equation*}
%   \vec{c_j}^\prime= \vec{c_j}- P_j\mu
% \end{equation*}
% The scaling matrix $P_j$ is defined in a similar way to the matrix that scales the bias in dense layers. Methods for defining the matrix $P_j$ for weighting the added constant are described in Section~\ref{sec:scale}. Note that if one takes the new mean of the relevances and normalises them this mean value, the signal of the relevance will be lost. This is the case as no matter how big or small the feature is, if a separate mean for each is calculated, it will result in all features being centred around zero and therefore losing all the signal they carry. Scaling the mean allows for activations that were positive and now have a smaller relevance than the original activation values to have a positive relevance distributed forward. 


% The resulting vector must be divided by the standard deviation. Normally division scales each unit of activation proportionally and it should be sufficient for the contributions to be weighted in the same way as activations were during the forward pass. However, the standard deviation is also a learned parameter during training and is a result of the typical deviation from the mean. As the mean and the standard deviation are learned together, the standard deviation is a direct result from it. Therefore, for the purpose of preserving the contribution signal through the network a new standard deviation needs to be used that is equivalent to the deviation from the new mean. The new standard deviation uses the following new variance
% \begin{equation*}
% \sigma_j^{\prime2} = \sigma_j^{2} \frac{\frac{1}{M}\sum_{i=1}^{M}(c_i - P_j\mu)^2}{\frac{1}{N}\sum_{i=1}^{N}(x_i - \mu)^2},
% \end{equation*}
% where the learned variance is scaled by the ratio between the deviation of the activations to their mean and the deviation of the relevances to the new mean. 

% This new variance is used to re-scale the already centred contribution vector $\vec{c_j}^-$, as follows:
% \begin{equation*}
%   \vec{c_k}^-= \frac{\vec{c_j}^\prime}{\sqrt{\sigma^{\prime2} +\epsilon}}
% \end{equation*}

% The resulting vector $\vec{c_k}^-$ is scaled and shifted. Now it needs to be scaled by $\gamma$ and get the new mean $\beta$:
% \begin{equation}
%     \vec{c_k} = \gamma \vec{c_k}^- + \beta.
% \end{equation}


\subsection{Concatenation \& Average Pooling Layers}

There are two types of layer that we have yet to consider: concatenation layers and average pooling layers. Both the concatenation layer and average pooling layer perform functions that are not influence by the input. Therefore the functions for distributing the contribution forward is exactly the same as during the forward pass, where the activation is distributed. Each concatenations layer $k\in \Lambda$ is given by a function, where $a_k = f_k\big([\vec{a}_j]_{j\passto k}\big) = [\vec{a}_j]_{j\passto k}$. 

Therefore, it is sufficient to take the contribution of layer $k$ to be the concatenation of the contributions of the same collection of layers $j\in \Lambda$ such that $j\passto k$: 
\begin{equation*}
    \vec{c}_k = f_k\big([\vec{c}_j]_{j\passto k}\big) = [\vec{c}_j]_{j\passto k}.
\end{equation*}

The average pooling operator involves calculating the average for each patch of the feature map. This operation is pivotal in reducing the spatial dimensions of the input feature map, effectively summarising the information contained in each patch. Given an average pooling layer $l$, the operation can be defined as follows:

\begin{equation*}
    \vec{c}_k = \frac{1}{|\mathcal{P}_l|} \sum_{a \in \mathcal{P}_l} a(i),
\end{equation*}

where $\mathcal{P}_l$ represents the set of pixels in each patch of the feature map that the pooling layer $\Lambda_j$ processes, and $a(i)$ is the activation of a neuron $i$ in the patch $\mathcal{P}_l$.


\section{Preserving The Contribution Signal Through The Network by Scaling Learned Parameters}
\label{sec:scale}

The focus of traditional neural network interpretability methods is on evaluating the entire input data, given the complete classification output to determine the importance of different input components. These methods use the entire activation signal that was present during the forward pass. This means that values that were learned by the network are still receiving data in the same distribution. However, the approach described in this chapter diverges from this norm. It involves propagating only a portion of the input through the network. This selective propagation presents a unique challenge: maintaining the integrity and the influence of a small subset of activations as they pass through the various layers of the network, without the relevance getting diluted or lost, thereby accurately reflecting the significance in the overall decision-making process of the model. When the operation is multiplicative, the signal is preserved, as the relevances get scaled proportionately to how activations were scaled during the forward pass. The big challenge is introduced by learned parameters that are added or subtracted to the activation during the forward pass. Adding such parameters directly to the relevances leads to a degraded relevance signal. This can be seen in dense layers and convolutional layers when the bias is added.


What all of these learned parameters have in common is that they represent a shift in either the positive or the negative direction. This means that the scaling matrix $P_{k}$ has to be positive. Consequently, when adding or subtracting the learned parameter to the contribution, the scaled parameter should maintain its directional influence --- a positive bias or mean should continue to exert a positive shift, and similarly, a negative bias or mean should remain negative. In the following subsections, several methods for determining the values of the adjustment matrix are examined. The process might appear straightforward, but even minor modifications in this calculation can lead to significant changes in the network's output, especially in larger and more complex architectures due to repeated application of the same function in sequential layers (see Section~\ref{sec:parallels}).

\subsection{Naïve Weighting of The Learned Parameters}
\label{naive}

A naïve approach to define $P_{k}$ is as an element-wise absolute ratio between the relevances and the activations. In the case of dense layers this is defined as a ratio between the elements of the weighted sum of the contribution matrix and the weighted sum of the activation matrix.
\begin{equation*}
    P_{k} = \dfrac{\vec{W_k(j)}\,[c_j(i)]_{j\passto k}}{\vec{W_k(j)}\,[a_j(i)]_{j\passto k}}
\end{equation*}
In the case of convolutional layers it represents the ratio between the contribution element after the convolution operation and the activation elements after the convolution operation. Defined as:
\begin{equation*}
    P_{k} = \dfrac{\vec{W_k(j)}\ast\,[c_j(i)]_{j\passto k}}{\vec{W_k(j)} \ast \,[a_j(i)]_{j\passto k}}
\end{equation*}
Using this definition of the ratio has the same effect as calculating the bias per unit of activation and then multiplying that by the amount of contributions in the region.

\begin{equation*}
   c_k = W_k \vec{c_j} + \left| \dfrac{\vec{b}_k}{W_k\,[\vec{a}_j]_{j\passto k}}\right| W_k\,[\vec{c}_j]_{j\passto k}, \quad \mbox{and}\quad c_k = W_k \ast \vec{c_j} + \left| \dfrac{\vec{b}_k}{W_k \ast \,[\vec{a}_j]_{j\passto k}}\right| W_k\ast \,[\vec{c}_j]_{j\passto k},
\end{equation*}
\newline
\newline
This method of weighting the bias works for a single layer in isolation, but whenever the activation and the relevance are of considerably different magnitudes, the bias will become very big or very small. Here big and small refer to the orders of magnitude of the value. This may be the case even when the relevances are in the same distribution as the activations. For example, a neuron that has an activation of 0.05 and a relevance of 0.5, will lead to a ratio of 10, which after multiplied with the bias and added to the contribution, will lead to a very big contribution that is out of distribution. This contribution will then be used in following layers and for calculating a ratio that will push the next relevances to become even further out of distribution. As layers that have learned parameters will be repeated numerous times within a single neural network, the order of magnitude between the activation and the relevance amplifies and grows exponentially. This pushes the bias to be either extremely small or extremely large, particularly in deeper networks with multiple sequential layers (see Figure~\ref{fig:exponential}). Given that the contribution of a subsequent layer is a function of the bias in the previous layer, an exponentially growing (or shrinking) bias term results in an exponential change in the value of the relevances too. 

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=\linewidth]{Figures/Logarithms_of_the_ratios.pdf}
	\end{center}
	\caption{The logarithmic scale comparison of maximum (left) and minimum (right) ratio values in VGG16 neural network layers. The left graph illustrates how the logarithm of the ratio can become exceedingly large, indicating a very high bias when the activation is significantly smaller than the relevance. Conversely, the right graph shows instances where the logarithm of the ratio is highly negative, reflecting an extremely low bias when the activation is much larger than the relevance. These extremes demonstrate the instability of the ratio in the network.}
	\label{fig:exponential}
\end{figure} 

\subsection{Drawing Parallels to Exploding and Vanishing Gradients in Backpropagation}
\label{sec:parallels}

The challenges in preserving the contribution signal through a neural network, especially when managing learned parameters, bear striking parallels to the phenomena of exploding and vanishing gradients in backpropagation~\cite{DBLP:journals/tnn/BengioSF94}. These similarities are crucial for understanding and addressing the stability and effectiveness in deep neural networks.

Similar to the exploding and vanishing gradients problem, the relevance signal in the network can either exponentially increase or decrease as it is processed through multiple layers. This is a direct parallel to how gradients can grow or shrink during backpropagation, leading to instability in the network's learning dynamics. Just as deeper networks are more susceptible to the compounding effects of gradients, they are equally prone to the distortion of relevance signals due to the repeated application of biases and scaling parameters across layers. This necessitates a careful approach to manage these effects in larger architectures. The issues of exploding gradients can cause numerical instability and hinder convergence, while vanishing gradients can impede learning. Analogously, an uncontrolled relevance signal can distort the interpretability and functionality of the network, misrepresenting the importance of inputs or activations. Both scenarios underscore the importance of considered parameter management. Techniques like normalisation are vital in backpropagation, just as sophisticated methods for calculating and adjusting the scaling matrix \( P_{k} \) are critical for maintaining a stable and accurate relevance signal. The next subsections demonstrate how a version of normalisation can be applied in the context of calculating the scaling matrix \(P_{k}\).

\subsection{Weighting of the bias within distribution}

A more sophisticated approach would be to scale the orders of magnitude of the ratio into an acceptable standard deviation. However as the learned parameters do not have a distribution (\eg there is one bias per channel of activations), it is difficult to find the acceptable range.  For ease of comprehension, the term magnitude henceforth refers to the order of magnitude of a value with respect to its mean. Scaling the magnitudes of the ratios to an acceptable standard deviation limits the exponential magnification or shrinking of the contribution when functions that have learned parameters are called repeatedly throughout sequential layers of the neural network. 

To answer the question of what is acceptable standard deviation of the distribution of the magnitude of the ratio, it helps to think of the magnitude of the output values. The magnitude of the scaled learned parameter can deviate from its mean to the same extend that the output's magnitude can deviate from its mean. The deviation of the bias by this limited amount will not lead to any noticeable magnitude change after the scaled learned parameter is added to the relevances. The activation output magnitude can therefore be used as the maximum standard deviation allowed for the learned parameter's magnitude standard deviation. 

The learned parameter is scaled through the scaling matrix \(P_{k}\), so one needs to find an equivalent relationship between the scaling matrix \(P_{k}\) and another ratio $Q_{k}$ as the relationship between the learned parameter's magnitude is to the activation output magnitude. The ratio $Q_{k}$ is therefore defined as the ratio between the activation output and its mean:
\begin{equation*}
    Q_{k} = \dfrac{\vec{a_k}}{\mu_{a_k}},
\end{equation*}
where $\vec{a_k}$ is the activation output vector (\ie after the learned parameter is added or subtracted). This ratio captures the variability of the activation's output. When \( P_{k} \) is multiplied by the learned parameters, the resulting values should have a magnitude deviation that is no greater than when the ratio $Q_{k}$ is multiplied with the mean of the activation output. Therefore, \( Q_{k} \) serves as a benchmark for determining how much the learned parameter, once scaled by \( P_{k} \), can deviate in magnitude. This constraint ensures that the scaling process does not introduce disproportionate amplification or reduction of the learned parameters, which could otherwise lead to an unfaithful explanation. The key idea here is to ensure that the magnitude deviation of the learned parameters, after being scaled, does not exceed the magnitude deviation of the layer's outputs. This is crucial because it aligns the influence of the learned parameters with the natural scale of the network's activations, thus maintaining a balanced and stable operation of the network.


It is important to note that scaling the standard deviation of the magnitudes of the \(P_{k}\) matrix to match the standard deviation of the magnitudes of the $Q_{k}$ matrix is not the same as transforming the \(P_{k}\) distribution to match the $Q_{k}$, as that would limit the magnitude of \(P_{k}\) from becoming exponentially large, but would not limit the magnitude of \(P_{k}\) from becoming exponentially small. 


To keep the orders of magnitude of the \(P_{k}\) matrix and $Q_{k}$ within the same distribution, one first needs to get the magnitudes of both matrices. This is achieved by taking the natural logarithm of the activation and the contribution values, with all zero values being discarded and set to zero (given that the magnitude of zero is undefined). 
\begin{equation}
\label{eq:log}
    \lo (i) = \begin{cases}
    \ln |Q_k(i)| & \mbox{if $Q_k(i)\not=0$}\\
    0 & \mbox{otherwise}
    \end{cases}
    \quad\mbox{and}\quad 
    \clo(i) =  \begin{cases}
    \ln |P_k(i)| & \mbox{if $P_k(i)\not=0$}\\
    0 & \mbox{otherwise}
    \end{cases} 
\end{equation}
\newline
The logarithm vectors will have a value higher than 0 if the number is greater than 1, and it will have a value less than 0 if the number is between 0 and 1. The more positive the logarithm of ratio is, the larger the ratio, similarly the more negative the logarithm is the smaller the ratio.

Next, the standard deviation of both the magnitude of the scaling matrix \(P_{k}\) and the standard deviation of the magnitude of \(Q_{k}\) are calculated. These standard deviations provide a quantitative measure of how spread out the values are in each matrix. The standard deviation of the magnitudes of \(Q_{k}\), denoted as \(\sigma_{Q_k}\), reflects the variability in the activation outputs relative to their mean. Similarly, the standard deviation of the magnitudes of \(P_{k}\), denoted as \(\sigma_{P_k}\), reflects the possible variability in the scaled learned parameter relative to its mean.

\begin{equation}
\sigma_{Q_k} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (\lo(i) - \mu_{\lo})^2}
\quad \text{and} \quad
\sigma_{P_k} = \sqrt{\frac{1}{M} \sum_{i=1}^{N} (\clo(i) - \mu_{\clo})^2},
\end{equation}

where \(\mu_{\lo}\) and \(\mu_{\clo}\) represent the mean of the logarithmic magnitudes of \(Q_{k}\) and \(P_{k}\) respectively, and \(N\) and \(M\) denote the number of non-zero elements in each matrix $Q_k$ and $P_k$ respectively. These standard deviations are crucial for the next step: ensuring that the scaled learned parameters maintain a magnitude deviation within an acceptable range.

The goal is to scale \(P_{k}\) in such a way that its standard deviation of magnitudes, \(\sigma_{P_k}\), does not exceed the standard deviation of magnitudes of \(Q_{k}\), \(\sigma_{Q_k}\). This constraint is vital because it ensures that the contribution of the scaled learned parameters remains within the natural variability of the layer’s outputs, as represented by \(Q_{k}\). To achieve this, if  \(\sigma_{P_k}\) is bigger than  \(\sigma_{Q_k}\) an equivalent to batch normalisation function is performed. This involves normalising the magnitudes \( \clo(i) \) of the scaling matrix \( P_{k} \), then scaling them to match the magnitudes \( \lo(i) \) of \( Q_{k} \), followed by a shift back to the original mean \( \mu_{\clo} \) of \( \clo(i) \). This can be broken down into the following steps:
\begin{enumerate}
    \item First, \( \clo(i) \) is normalised to zero mean and a unit variance:
\begin{equation*}
    \clo'(i) = \frac{\clo(i) - \mu_{\clo}}{\sigma_{P_k}}.
\end{equation*}
    \item Then, the normalised \( \clo'(i) \) is scaled to have the same standard deviation as \( \lo(i) \), after which it is shifted back to the original mean \( \mu_{\clo} \), by:
\begin{equation*}
    \clo''(i) = (\clo'(i) \cdot \sigma_{Q_k}) + \mu_{\clo}.
\end{equation*}
In this way, the adjusted \(\clo''(i)\) now has the same standard deviation as \(\lo(i)\) and is centred around its original mean $\mu_{\clo}$. Centring around the original mean of $\mu_{\clo}$ is very important as it preserves the difference between different complex input features.
    \item Finally, the adjusted magnitude matrix \(\clo''(i)\) can now be scaled back to the new scaling matrix \(P_{k}^\prime\), as such:
\begin{equation*}
    P_{k}^\prime(i) = e^{\clo''(i)}
\end{equation*}
This exponential transformation converts the logarithmic magnitudes back to their original scale. The scaling matrix \(P_{k}^\prime\) now effectively embodies the adjusted scaling factors, ensuring that the influence of the learned parameters is in line with the network's natural output variability.
\end{enumerate}

This methodology achieves two primary goals. Firstly, it adjusts the scaling matrix \(P_{k}^\prime\) to preserve the natural range of the activation outputs. This balance ensures that the learned parameters significantly influence the explanation without overwhelming it. Secondly, by centring around the original mean \(\mu_{\clo}\), the approach retains the distinctions among complex input features, which is crucial for the network's interpretive accuracy, particularly in cases where minor differences in inputs are significant.

Illustrated in Figure~\ref{fig:scaled}, the logarithmic scale of ratios after adjusting \(P_{k}^\prime\) shows that the natural layer output variability of the network is maintained. This result is consistent across the same mask, network, and example. Notably, the signal's general shape remains similar to that in Figure~\ref{fig:exponential} where ratios are not adjusted, yet the scale stays within the layer’s output variability. In this scenario, the maximum outlier has a logarithm below 12 with the adjusted \(P_{k}^\prime\), as opposed to over 25,000 when unbounded. Similarly, the minimum log is -22 with adjustment, in contrast to less than -1600 when unbounded.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=\linewidth]{Figures/Logarithms_of_the_ratios_scaled.pdf}
	\end{center}
	\caption{The logarithmic scale comparison of maximum and minimum ratio values in VGG16 neural network layers after \(P_{k}^\prime\) adjustment. While maintaining the signal, this Figure mirrors the shape of the unbounded signal of Figure~\ref{fig:exponential}  but on a significantly smaller scale.}
	\label{fig:scaled}
\end{figure}

Thus, this approach enables a more measured and balanced integration of learned parameters into the network. It ensures these parameters are scaled effectively to keep the network interpretable. The incorporation of \(P_{k}^\prime\) is vital for ensuring that these scaled parameters contribute meaningfully and precisely to the model's decision-making process, avoiding distortions from disproportionate scaling. This method is especially beneficial in deep neural networks, where layer-wise parameter interactions can greatly affect both performance and interpretability.

The rule described in this section for scaling the learned parameters is a result of taking a ratio between the relevances and the activations. In the cases where either the relevances or the activations are very small, round-off error can occur due to the inherent limitations of computers in representing decimal numbers. This rounding can lead to small inaccuracies in calculations. These errors accumulate over successive computational steps (similar to how the the ratio can become exponentially small or big as discussed in Chapter~\ref{sec:parallels}), sometimes significantly impacting the final result. To remedy this in the implementation of forward pass retracing the activation and relevance are quantised before the ratio is taken so that such errors are minimised. The quantised activation and relevance are only used for the computation of the ratio and therefore this does not effect the overall contribution of features and the methods accuracy.

\section{Conclusion}

The method proposed in this chapter assigns a single value of relevance to a complex input feature by propagating its contribution forward. The contribution is calculated by modified versions of the layers of the network. These modified layers allow for the behaviour of inference step of the neural network to be replicated, but with only part of the input present. 

Chapter~\ref{chapter:results} delves into the practical implementation of forward pass retracing, as well as the interpretability and fidelity of the contribution to classification (\CTC\/) values. Through the comprehensive evaluation on various types of neural network architectures and datasets, the next chapter shows the versatility and effectiveness of the \CTC\ method described in this chapter by comparing it to state-of-the-art methods. The chapter further discusses how different architectures and parameter choices affect the contribution values. Through these investigations, the aim is to uncover patterns, correlations, and potential areas for improvement of the \CTC\ method.