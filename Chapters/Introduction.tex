\chapter{Introduction}
\chaptermark{Introduction}
\justifying
\setlength{\parindent}{0em}
% \textbf{The primary goal of this PhD is to create a method for generating explanations of a deep neural network’s (DNN's) classification that are both human-understandable (interpretable) and accurate (faithful) with respect to how  the  network reached  the classification}

\section{Machine Learning}
In the early days of \textit{artificial intelligence} (AI), the field was concerned with solving problems that are well-defined, formal and can be expressed with mathematical rules~\cite{bernstein1958computer, minsky1956heuristic, minsky1962problems}. These types of problems are computationally and intellectually difficult for humans to perform, which is why the field of artificial intelligence was interested in solving them. However, despite these types of problems posing difficulties for humans, they were easy to encode, so were relatively uncomplicated for a computer to reason about.

The challenge of current AI systems is to perform tasks that come intuitively to humans (\eg recognising objects and faces, interpreting speech, text generation \textit{etc.}). Such tasks are difficult to perform as they require a huge amount of knowledge about the world. This makes it difficult to formulate the problem in a well-defined mathematical way with enough complexity that allows the system to reason about the information and infer knowledge from the input. Finding ways of encoding the environment in an accurate and detailed way is still an active area of research. 

A way to avoid this problem of \textit{knowledge engineering} is to let the AI algorithm obtain its own knowledge through learning from experience. These types of AI systems are referred to as \textit{machine learning} (ML) systems. Machine learning algorithms aim to obtain their own knowledge by \textit{extracting information} from data, \textit{recognising useful patterns} in data or \textit{making decisions} based on data. The ease of application of a number of ML techniques comes from each system's ability to perform specific decision-making tasks without requiring explicit human instruction. This makes them applicable to a wide range of areas~\cite{forthcoming} from content filtering and recommendations on social networks and e-commerce websites~\cite{de2010combining} to protein folding~\cite{varadi2022alphafold}, autonomous vehicle control~\cite{kuutti2020survey}, financial markets~\cite{cavalcante2016computational} and cancer diagnostics~\cite{huang2020artificial}. ML techniques have existed since the 1960s~\cite{minsky1969introduction, samuel1959some, nilsson1965learning}, but it was not until recently (circa 2010) that the integration into ML systems into wider society became noteworthy. This is mainly due to the the increase of available information, as well as other developments, including hardware improvements and new optimisation algorithms.  
\section{Feature Engineering Problem}
\label{featureeng}
Powerful ML models trained on large amounts of data can achieve great performance, and on some cognitive tasks record results on a par with humans~\cite{brown2017libratus, zhai2023can, silver2017mastering}. Many tasks, previously thought to be so computationally demanding as to be unattainable, such as image detection and recognition~\cite{HeZRS16}, strategic game planning~\cite{SilverHMGSDSAPL16} and natural language processing~\cite{DengHK13}, have seen rapid development~\cite{LeCunBH15}. 

To perform such tasks well, machine learning systems need a large amount of data to learn how to perform a specific task. The performance of all machine learning algorithms is strongly influenced by the representation and structure of this data. Hence, after the necessary data is collected some aspects, qualities, or characteristics of the data are often chosen that make the task easier to learn. These are often referred to as \textit{features} and finding the right set which allow the algorithm to learn a task well is a difficult and intricate process. Moreover, finding such features can be a waste of human time and effort, as features for more complicated tasks may take decades and a community of researchers to design~\cite{goodfellow2016deep}. 

To overcome this, some machine learning algorithm not only learn a mapping from input to output, but also the features they should focus on from the input. This is called \textit{representation learning} and the type of machine learning algorithms that are capable of performing representation learning are various types of \textit{artificial neural networks (ANN)}.  

\section{Black-box Nature of Artificial Neural Networks}

Algorithms that learn their own feature representation (\ie ANN) often perform much better than algorithms that use human-deigned features~\cite{goodfellow2016deep}. However, the problem with leaving ANNs to learn their own features is that the knowledge learned and encoded in the network is not comprehensible and that makes the decisions taken by neural networks hard to trust. 

Currently most ANNs are deployed and expected to behave as intended if they show a high degree of predictive accuracy on test data. However, in some cases, the high predictive accuracy of  neural networks can be the result of erroneous exploration of artefacts in the data (\ie the presence of systematic bias in data that the system bases its decision upon) rather than the result of correctly identified parts of the input that \textit{should} lead to the decision~\cite{leek2010tackling, SzegedyZSBEGF13, corr, taylor2006methods}. AI systems that exploit such spurious correlations from the input to make their decisions are referred to as ``Clever Hans'' Predictors~\footnote{Clever Hans was a horse that was believed to be able to count and was declared a scientific marvel in the years around the 1900s. Later it turned out that the horse did not learn how to count, but was rather deriving the answer from the questioner's reactions~\cite{lapuschkin2019unmasking}}. Moreover, since accuracy is calculated on test data, there is no guarantee that once deployed on real-world data it will remain high. Hence the lack of transparent decision making in deep learning systems not only limits their trustworthiness, but also poses problems to their \emph{acceptance} and \emph{applicability}, due to concerns over poor performance in safety critical applications, potentially leading to complex enquiries regarding \emph{accountability} (\ie who or what is at fault) and industrial \emph{liability} (\ie who or what is liable for costly mistakes made by an algorithm)~\cite{NguyenHHTK14, abs, kucharski2016study, WolfMG17}. The problem of \emph{non-transparency} of neural networks is even more pressing in life-critical situations where it is essential to robustly verify a system's decision-making process (\eg in areas including medicine and the criminal justice system, etc.)~\cite{CaruanaLGKSE15, BojarskiYCCFJM17}.

In recent years there has been growing recognition beyond academia of the need to develop explainable machine learning methods. In an attempt to prevent the problems that could occur from the integration and the rising importance of uninterpretable algorithms in a wide range of areas, the European Union (EU) introduced a right to explanation in the General Data Protection Regulation Act (GDPR)~\cite{GoodmanF17} in May 2018. The EU further introduced an EU’s Artificial Intelligence Act, which enforces that high-risk AI systems shall be designed and developed in such a way to ensure that their operation are sufficiently transparent to enable users to interpret the system’s output and use it appropriately. While the legal requirement for explanations is recent and only in specific geographic areas (the EU), the term explainable artificial intelligence was coined in 2004~\cite{LentFM04} and the problem of creating explanations in AI can be traced back to the mid-1970s~\cite{moore1988explanation}. 

\section{ Artificial Neural Networks Explainability Methods}

For one to develop methods which will make neural networks deemed \emph{trusted} and accepted by the user, one must have clear criteria of when one has succeeded in creating a \emph{trustworthy} method. However, trust and acceptance are concepts that are hard to quantify and formalise~\cite{doshi2017towards, Lipton18} and as such, it is hard for them to be used to guide the development of \emph{safe} ML algorithms. Hence, achieving human-understandable (\textit{interpretable}) explanations that are reliable (\textit{faithful}) with respect to how the classifier makes its decisions has been primary focus of the field. Achieving these properties is a crucial stepping stone for achieving trust~\cite{Lipton18}. The research area that addresses the problem of neural networks explainability is part of the broader area of explainable artificial intelligence (XAI).

The techniques developed for explaining neural networks fall within two broad categories of intrinsic explainability and post-hoc explainability (see Chapter~\ref{chap:lit}). The intrinsically explainable models have a primary focus on either acquiring an interpretable variant of the current model~\cite{VandewieleJOTH16, BastaniKB17a}, refining the existing model by adding constrains to boost its interpretability~\cite{ZhangWZ18a, NguyenYC16, SabourFH17}, or adapting the learning process to yield a model that is inherently more interpretable~\cite{LinsleySES19, ShiXXCLLG21}. These strategies present a compromise, enabling the employment of precise and sophisticated models, while also achieving a certain degree of interpretability. However, placing excessive focus on one aspect (\ie accuracy or interpretability) can greatly reduce the effectiveness of the other. Current research is directed towards resolving this problem by finding ways to maintain high accuracy while also offering transparent understanding of the model's decision-making process. Due to the difficulties inherent in creating models that are intrinsically interpretable, recent studies are more and more focused on interpreting models after they have been trained. Initial approaches to post-hoc explanations were low in interpretability and had significant fidelity issues. They produced explanations that were difficult to understand and did not accurately reflect the model's reasoning~\cite{SimonyanVZ13, SpringenbergDBR14}. Subsequent methods improved human comprehension but may not truly represent the model's internal workings~\cite{Ribeiro0G16, LundbergL17, ElenbergDFK17, Ribeiro0G18}. These methods emphasise parts of the input already understandable to humans and show if they are significant to the classification. By clustering smaller units (like image pixels) into larger features (like superpixels), these methods present a limited set of information to the user. It has been shown that a human can not comprehend more than three to five meaningful item at once~\cite{cowan2001magical, starkey1995development, morris2018human}, so presenting these higher-level explanations enhances interpretability. However, these explanation methods, which assign importance based on how feature modifications affect the classification, can lead to unfaithful explanations. These methods may overlook important features or wrongly attribute importance to irrelevant ones, especially in cases of low classification certainty~\cite{Kottke}. Recognising these fidelity issues has become a distinct research area~\cite{KindermansHAASDEK19, DombrowskiAAAMK19, GhorbaniAZ19}. Methods following the fidelity critiques of interpretability methods, have aimed to resolve fidelity problems in explanations~\cite{ShrikumarGK17, SundararajanTY17, SmilkovTKVW17, ChattopadhyaySH18, bach2015pixel}. However, they assess each part of the input making their interpretability not as high as methods grouping smaller units into larger features. The future direction in machine learning interpretability is arguably moving towards a blend of approaches, where explanations are both highly interpretable as the big feature methods, and faithful to the actual workings of the model (\ie encompass all key fidelity properties), similar to recent methods.


\section{Thesis Aims \& Structure}

This thesis aims to investigate the gap between faithfulness and interpretability of the explanation. In Chapter~\ref{chap:framework} a unified framework is proposed for creating methods that could generate both faithful and comprehensible explanations. The first challenge in creating explanations that are more easily comprehensible is isolating key complex input features in deep neural networks. The second challenge is finding the importance of such regions that is faithful to the model and doesn't lead to oversimplification. 

Chapter~\ref{chap:clustering} focuses on heatmap-based clustering techniques. The approach of using heatmap clustering to highlight crucial features for the network's decision-making process, links the selected complex input features directly to the network's internal representations. However, the challenge lies in the interpretability of these extracted features. To address this, the thesis integrates the object detection model Segment Anything Model (SAM)~\cite{kirillov2023segment} with the heatmap clustering method, enhancing the understanding of the network's prioritised areas of the input. 

This technique crucially feeds into the broader objective of assigning singular importance values to the complex input features. This thesis outlines two primary methods to address the challenge of assigning a single value of relevance to complex input features. The first is reverse relevance distribution tracing (see Chapter~\ref{chapter:revLRP}), which inspects part of the input with respect to the distribution of relevance it had by an explainability method. The second is forward pass retracing (see Chapter~\ref{chapter:REVEAL}), which inspects the contribution of complex input feature to the output with respect to the inference step. 

The reverse relevance distribution tracing is based on top of LRP~\cite{bach2015pixel}, but any method that distributes relevance layer by layer can be utilised. The method finds the amount of relevance distributed during the relevance propagation method for each layer. Next the relevance is scaled by the new amount of relevance that is propagated only from the complex input feature. This new amount is then propagated forward to the neurons in the next layer until the classification is reached. A critical drawback of  this type of forward relevance propagation is the computational complexity and memory intensity of the computation of the relevance values. 

Forward pass retracing is designed to address the computational complexity and memory intensity of reverse relevance distribution tracing, offering a more feasible and efficient method for network interpretation. This pass is carried out with modified functions that replicate the network's behaviour during the inference step only on the complex input feature. By doing so, the method directly analyses how the input features influence each layer and most importantly the output. A particular focus of those rules is to preserve the contribution signal through the network. This is particularly challenging as unlike other methods, only a subset of the activations are distributed during the forward pass. This method is evaluated on VGG16~\cite{SimonyanZ14a}, VGG19~\cite{SimonyanZ14a}, ResNet50~\cite{he2015deep}, InceptionV3~\cite{szegedy2015rethinking} and  DenseNet121~\cite{huang2018densely} based on the widely recognised ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC 2012) dataset~\cite{ILSVRC15}. The results in Chapter~\ref{chapter:results} compare the method proposed both qualitatively and quantitatively against established attribution methods.


Finally, Chapter~\ref{chapter:conclusion} presents an in-depth discussion of the fidelity and interpretability of the methods proposed in this thesis. Additionally, the final chapter presents directions for future research, highlighting potential avenues to enhance the understanding and application of these methods.

The next chapter~\ref{chap:background} lays the groundwork by introducing the fundamental concepts, structure, learning mechanisms and performance evaluation inherent in artificial neural networks, setting the stage for a comprehensive exploration of interpretability of these complex machine learning algorithms.


